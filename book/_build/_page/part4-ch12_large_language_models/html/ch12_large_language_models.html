
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>The Neuroscience of AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=720ed60b" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=60c0e2ec"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ch12_large_language_models';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="https://neuroai-handbook.github.io/ch12_large_language_models.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="#">
  
  
  
  
  
  
    <p class="title logo__title">None</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1 current active">
                <a class="reference internal" href="#">
                    Chapter 12: Large Language Models & Fine-Tuning
                </a>
            </li>
        </ul>
        
    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fch12_large_language_models.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 12: Large Language Models & Fine-Tuning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-12-large-language-models-fine-tuning">
<h1>Chapter 12: Large Language Models &amp; Fine-Tuning<a class="headerlink" href="#chapter-12-large-language-models-fine-tuning" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Learning Objectives</p>
<p>By the end of this chapter, you will be able to:</p>
<ul class="simple">
<li><p><strong>Understand</strong> large language model architectures and pretraining strategies</p></li>
<li><p><strong>Master</strong> fine-tuning techniques and parameter-efficient adaptation methods</p></li>
<li><p><strong>Connect</strong> computational language models to human language processing mechanisms</p></li>
<li><p><strong>Implement</strong> effective prompting strategies and model adaptations</p></li>
<li><p><strong>Evaluate</strong> LLM performance across multiple dimensions and tasks</p></li>
</ul>
</div>
<div style="page-break-before:always;"></div>
<section id="large-language-model-fundamentals">
<h2>12.1 Large Language Model Fundamentals<a class="headerlink" href="#large-language-model-fundamentals" title="Link to this heading">#</a></h2>
<p>Large Language Models (LLMs) represent a transformative development in artificial intelligence, capable of generating human-like text, translating languages, writing creative content, and answering questions in an informative way. This section explores the foundational elements that make these models possible.</p>
<p><img alt="LLM Architecture" src="_images/llm_architecture.svg" />
<em>Figure 12.1: Large Language Model architecture, showing the progression from input tokenization through transformer layers to next token prediction.</em></p>
<section id="transformer-based-architectures">
<h3>12.1.1 Transformer-Based Architectures<a class="headerlink" href="#transformer-based-architectures" title="Link to this heading">#</a></h3>
<p>Modern LLMs are built on the transformer architecture introduced by Vaswani et al. (2017), which we covered in Chapter 11. While the core architecture remains similar, LLMs incorporate several key modifications and scaling techniques:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LLMTransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A single transformer block used in large language models.&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        
        <span class="c1"># Multi-head attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="c1"># Feed-forward network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>  <span class="c1"># GELU activation is common in modern LLMs</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="c1"># Layer normalization - modern LLMs often use RMSNorm or pre-norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        
        <span class="c1"># Dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Apply pre-normalization (common in modern LLMs)</span>
        <span class="n">normalized_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Self-attention with residual connection</span>
        <span class="c1"># GPT models use causal masking (triangular)</span>
        <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
            <span class="n">normalized_x</span><span class="p">,</span> <span class="n">normalized_x</span><span class="p">,</span> <span class="n">normalized_x</span><span class="p">,</span>
            <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        
        <span class="c1"># Feed-forward with pre-normalization and residual connection</span>
        <span class="n">normalized_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">normalized_x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">ff_output</span>
        
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>Key architectural innovations in modern LLMs include:</p>
<ol class="arabic simple">
<li><p><strong>Scale</strong>: While early transformer models had ~100M parameters, modern LLMs range from billions to trillions of parameters.</p></li>
<li><p><strong>Architectural Modifications</strong>:</p>
<ul class="simple">
<li><p>Pre-normalization vs. post-normalization</p></li>
<li><p>Activation functions (GELU instead of ReLU)</p></li>
<li><p>Attention mechanisms (grouped-query attention, sliding window attention)</p></li>
<li><p>Flash attention and other efficiency improvements</p></li>
</ul>
</li>
<li><p><strong>Auto-regressive Training</strong>: Most LLMs are trained to predict the next token in a sequence, creating an auto-regressive language model.</p></li>
<li><p><strong>Vocabulary and Tokenization</strong>: LLMs use subword tokenization methods like BPE (Byte-Pair Encoding) or WordPiece to handle large vocabularies efficiently.</p></li>
</ol>
<p><strong>Biological Parallel</strong>: The transformer’s attention mechanism resembles how the brain’s attentional systems selectively focus on relevant information, while the deep network of layers parallels the hierarchical processing in the brain’s language areas.</p>
</section>
<section id="scaling-laws-and-emergent-abilities">
<h3>12.1.2 Scaling Laws and Emergent Abilities<a class="headerlink" href="#scaling-laws-and-emergent-abilities" title="Link to this heading">#</a></h3>
<p>One of the most fascinating aspects of LLMs is how their capabilities grow with scale. Kaplan et al. (2020) discovered predictable scaling laws that show how model performance improves as a power-law function of model size, dataset size, and compute budget.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">plot_scaling_laws</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize LLM scaling laws relationship.&quot;&quot;&quot;</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
    
    <span class="c1"># Model sizes (parameters)</span>
    <span class="n">model_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># 10^6 to 10^12</span>
    
    <span class="c1"># Loss decreases as a power law with model size</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="p">(</span><span class="n">model_sizes</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.076</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">model_sizes</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Model Size (Parameters)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Scaling Law: Loss vs. Model Size&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    
    <span class="c1"># Annotate key model sizes</span>
    <span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;GPT-2&quot;</span><span class="p">:</span> <span class="mf">1.5e9</span><span class="p">,</span>
        <span class="s2">&quot;GPT-3&quot;</span><span class="p">:</span> <span class="mf">175e9</span><span class="p">,</span>
        <span class="s2">&quot;PaLM&quot;</span><span class="p">:</span> <span class="mf">540e9</span><span class="p">,</span>
        <span class="s2">&quot;GPT-4&quot;</span><span class="p">:</span> <span class="mf">1.8e12</span>  <span class="c1"># estimated</span>
    <span class="p">}</span>
    
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">y_val</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="p">(</span><span class="n">size</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.076</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span> 
                    <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span>
                    <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>Emergent abilities appear in LLMs as they scale, including:</p>
<ol class="arabic simple">
<li><p><strong>In-context Learning</strong>: The ability to learn from examples provided in the prompt without parameter updates.</p></li>
<li><p><strong>Instruction Following</strong>: Understanding and executing natural language instructions without explicit programming.</p></li>
<li><p><strong>Chain-of-Thought Reasoning</strong>: Breaking down complex problems into steps to arrive at answers, similar to human reasoning processes.</p></li>
<li><p><strong>Multimodal Capabilities</strong>: Recent models can process and generate content across modalities (text, code, images).</p></li>
</ol>
<p>These emergent abilities often appear at specific model size thresholds, with capabilities suddenly manifesting once models reach a certain scale.</p>
<p><strong>Biological Parallel</strong>: The brain’s language capabilities also emerge from the complex interactions of billions of neurons working together. No single neuron understands language, but the collective network produces sophisticated language processing.</p>
</section>
<section id="pre-training-objectives">
<h3>12.1.3 Pre-training Objectives<a class="headerlink" href="#pre-training-objectives" title="Link to this heading">#</a></h3>
<p>LLMs are initially trained with self-supervised objectives on massive text corpora:</p>
<ol class="arabic simple">
<li><p><strong>Next Token Prediction (Causal Language Modeling)</strong>:</p>
<ul class="simple">
<li><p>The most common approach used in GPT-style models</p></li>
<li><p>Model predicts the next token given previous tokens</p></li>
<li><p>Training uses teacher forcing where ground truth previous tokens are provided</p></li>
</ul>
</li>
<li><p><strong>Masked Language Modeling</strong>:</p>
<ul class="simple">
<li><p>Used in BERT-style models</p></li>
<li><p>Random tokens are masked, and the model predicts the masked tokens</p></li>
<li><p>Allows bidirectional context but requires additional fine-tuning for generation</p></li>
</ul>
</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">causal_language_modeling_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute loss for causal language modeling (next token prediction).&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># For autoregressive models, labels are the input shifted right</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>  <span class="c1"># Remove first token</span>
        
        <span class="c1"># Adjust input to predict next token</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Remove last token</span>
    
    <span class="c1"># Forward pass to get logits</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
    
    <span class="c1"># Compute loss (CrossEntropyLoss for token classification)</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    
    <span class="c1"># Reshape for loss computation: (batch*seq_len, vocab_size)</span>
    <span class="n">logits_view</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">labels_view</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits_view</span><span class="p">,</span> <span class="n">labels_view</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p><strong>Contrastive Learning</strong>:</p>
<ul class="simple">
<li><p>Maps similar inputs closer in embedding space and dissimilar inputs farther apart</p></li>
<li><p>Used in some multimodal models like CLIP</p></li>
</ul>
</li>
<li><p><strong>Multi-task Pre-training</strong>:</p>
<ul class="simple">
<li><p>Combines multiple objectives during pre-training</p></li>
<li><p>May include both generative and discriminative tasks</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: The brain learns language through prediction as well, constantly anticipating upcoming words based on context—a process known as predictive processing. When predictions are wrong, the brain updates its internal model.</p>
</section>
<section id="tokenization-strategies">
<h3>12.1.4 Tokenization Strategies<a class="headerlink" href="#tokenization-strategies" title="Link to this heading">#</a></h3>
<p>Tokenization converts raw text into tokens that serve as the model’s input units. Modern LLMs use subword tokenization methods to balance vocabulary size and coverage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2Tokenizer</span>

<span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_tokenization</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show how text is tokenized in modern LLMs.&quot;&quot;&quot;</span>
    <span class="c1"># Load a tokenizer (GPT-2 uses Byte-Pair Encoding)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
    
    <span class="c1"># Example text</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;The quick brown fox jumps over the lazy dog. It&#39;s a pangram!&quot;</span>
    
    <span class="c1"># Tokenize</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    
    <span class="c1"># Print results</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokenized: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Token IDs: </span><span class="si">{</span><span class="n">token_ids</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Show uncommon word handling</span>
    <span class="n">uncommon_text</span> <span class="o">=</span> <span class="s2">&quot;Transformers use self-attention for parallelization.&quot;</span>
    <span class="n">uncommon_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">uncommon_text</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Uncommon text: </span><span class="si">{</span><span class="n">uncommon_text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokenized: </span><span class="si">{</span><span class="n">uncommon_tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Token merging example (simplified BPE algorithm demonstration)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">simplified_bpe</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">max_merges</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="c1"># Start with characters</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_merges</span><span class="p">):</span>
            <span class="c1"># Find most frequent adjacent pair</span>
            <span class="n">pairs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">pair</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">pairs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            
            <span class="k">if</span> <span class="ow">not</span> <span class="n">pairs</span><span class="p">:</span>
                <span class="k">break</span>
                
            <span class="c1"># Get most frequent pair</span>
            <span class="n">best_pair</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pairs</span><span class="o">.</span><span class="n">get</span><span class="p">)</span>
            
            <span class="c1"># Merge the pair in the sequence</span>
            <span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">:</span>
                    <span class="n">new_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">new_tokens</span>
            <span class="c1"># Add to vocabulary</span>
            <span class="n">vocab</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">tokens</span>
    
    <span class="c1"># Demonstrate simplified BPE</span>
    <span class="n">sample_text</span> <span class="o">=</span> <span class="s2">&quot;lowerlevel&quot;</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">sample_text</span><span class="p">)</span>  <span class="c1"># Start with character vocabulary</span>
    <span class="n">merged_tokens</span> <span class="o">=</span> <span class="n">simplified_bpe</span><span class="p">(</span><span class="n">sample_text</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Simplified BPE example:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original: </span><span class="si">{</span><span class="n">sample_text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokenized: </span><span class="si">{</span><span class="n">merged_tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary: </span><span class="si">{</span><span class="n">vocab</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;tokens&quot;</span><span class="p">:</span> <span class="n">tokens</span><span class="p">,</span> <span class="s2">&quot;token_ids&quot;</span><span class="p">:</span> <span class="n">token_ids</span><span class="p">}</span>
</pre></div>
</div>
<p>Common tokenization methods include:</p>
<ol class="arabic simple">
<li><p><strong>Byte-Pair Encoding (BPE)</strong>: Iteratively merges the most frequent pairs of bytes or characters to form new tokens.</p></li>
<li><p><strong>WordPiece</strong>: Similar to BPE but uses a likelihood-based approach for merging tokens.</p></li>
<li><p><strong>SentencePiece</strong>: Uses BPE or unigram language modeling and performs tokenization without requiring pre-tokenization.</p></li>
<li><p><strong>Tokenization Challenges</strong>:</p>
<ul class="simple">
<li><p>Out-of-vocabulary words</p></li>
<li><p>Non-English languages and multilingual models</p></li>
<li><p>Code and specialized formats</p></li>
<li><p>Context window inefficiency</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: Humans process language at multiple levels of granularity—phonemes, morphemes, words, and phrases—similar to how tokenizers break text into subword units that capture meaningful language components.</p>
</section>
</section>
<section id="fine-tuning-methods">
<h2>12.2 Fine-tuning Methods<a class="headerlink" href="#fine-tuning-methods" title="Link to this heading">#</a></h2>
<p>While pre-trained LLMs possess impressive general capabilities, fine-tuning allows these models to specialize for specific tasks, domains, or requirements. This section explores various approaches to adapting LLMs, from traditional full fine-tuning to more efficient techniques.</p>
<p><img alt="Fine-Tuning Methods" src="_images/fine_tuning_methods.svg" />
<em>Figure 12.2: Comparison of LLM fine-tuning methods, from resource-intensive full fine-tuning to parameter-efficient techniques like LoRA and RLHF.</em></p>
<section id="full-fine-tuning">
<h3>12.2.1 Full Fine-tuning<a class="headerlink" href="#full-fine-tuning" title="Link to this heading">#</a></h3>
<p>Full fine-tuning involves updating all parameters of a pre-trained model on a new dataset. This approach typically yields the best performance but requires substantial computational resources:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>

<span class="k">def</span><span class="w"> </span><span class="nf">full_finetune_example</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate full fine-tuning of a small LLM.&quot;&quot;&quot;</span>
    <span class="c1"># Load pre-trained model and tokenizer (using GPT-2 small for demonstration)</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt2&quot;</span>  <span class="c1"># 124M parameters</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    
    <span class="c1"># Special tokens</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
    
    <span class="c1"># Load dataset (for example, a subset of WikiText)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;wikitext&quot;</span><span class="p">,</span> <span class="s2">&quot;wikitext-2-raw-v1&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train[:1000]&quot;</span><span class="p">)</span>
    
    <span class="c1"># Tokenize the dataset</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
    
    <span class="n">tokenized_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Training arguments</span>
    <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./results&quot;</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">logging_dir</span><span class="o">=</span><span class="s2">&quot;./logs&quot;</span><span class="p">,</span>
        <span class="n">report_to</span><span class="o">=</span><span class="s2">&quot;none&quot;</span>  <span class="c1"># Disable wandb, etc.</span>
    <span class="p">)</span>
    
    <span class="c1"># Setup trainer</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_dataset</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="c1"># Train (commented out for demonstration)</span>
    <span class="c1"># trainer.train()</span>
    
    <span class="c1"># Save fine-tuned model</span>
    <span class="c1"># trainer.save_model(&quot;./fine-tuned-gpt2&quot;)</span>
    
    <span class="c1"># Memory and compute requirements</span>
    <span class="n">model_size_mb</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span>  <span class="c1"># 4 bytes per float32</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model size: </span><span class="si">{</span><span class="n">model_size_mb</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;All parameters updated during training: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
        <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span>
        <span class="s2">&quot;trainable_parameters&quot;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Key considerations for full fine-tuning:</p>
<ol class="arabic simple">
<li><p><strong>Resource Requirements</strong>:</p>
<ul class="simple">
<li><p>Memory: Must fit the entire model and optimizer states in memory</p></li>
<li><p>Compute: Updates all parameters, requiring substantial computation</p></li>
<li><p>Storage: The resulting model is as large as the original</p></li>
</ul>
</li>
<li><p><strong>Catastrophic Forgetting</strong>:</p>
<ul class="simple">
<li><p>Model may lose general capabilities when fine-tuned on a narrow domain</p></li>
<li><p>Mitigated through regularization techniques and careful hyperparameter selection</p></li>
</ul>
</li>
<li><p><strong>Advantages</strong>:</p>
<ul class="simple">
<li><p>Maximum performance potential</p></li>
<li><p>Full model adaptation</p></li>
<li><p>No architectural constraints</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: Full fine-tuning resembles extensive retraining of neural circuits, where the brain forms specialized pathways for specific skills while potentially weakening other connections. However, the brain is generally better at avoiding catastrophic forgetting through complementary learning systems.</p>
</section>
<section id="parameter-efficient-fine-tuning-peft">
<h3>12.2.2 Parameter-Efficient Fine-Tuning (PEFT)<a class="headerlink" href="#parameter-efficient-fine-tuning-peft" title="Link to this heading">#</a></h3>
<p>Parameter-efficient fine-tuning methods modify only a small subset of model parameters, dramatically reducing computational and storage requirements while maintaining performance:</p>
<section id="lora-low-rank-adaptation">
<h4>LoRA (Low-Rank Adaptation)<a class="headerlink" href="#lora-low-rank-adaptation" title="Link to this heading">#</a></h4>
<p>LoRA, introduced by Hu et al. (2021), inserts trainable low-rank matrices into the model’s weight matrices, allowing efficient adaptation with minimal parameter updates:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LoRALayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implementation of Low-Rank Adaptation (LoRA) for a linear layer.&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">rank</span>
        
        <span class="c1"># Low-rank matrices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">rank</span><span class="p">,</span> <span class="n">in_features</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="p">)))</span>
        
        <span class="c1"># Initialize A with normal and B with zeros</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Low-rank update: B·A·x</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span> <span class="o">@</span> <span class="n">x</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LoRALinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Linear layer with LoRA adaptation.&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">linear_layer</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear_layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora</span> <span class="o">=</span> <span class="n">LoRALayer</span><span class="p">(</span>
            <span class="n">linear_layer</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> 
            <span class="n">linear_layer</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span>
        <span class="p">)</span>
        
        <span class="c1"># Freeze the original layer</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Combine original output with LoRA adaptation</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">apply_lora_to_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">],</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply LoRA to specific modules in a transformer model.&quot;&quot;&quot;</span>
    <span class="c1"># Count original trainable parameters</span>
    <span class="n">orig_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
    
    <span class="c1"># Find and replace target modules with LoRA versions</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">target</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">target_modules</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">parent_name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">attr_name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">parent</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">parent_name</span><span class="p">)</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">LoRALinear</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">))</span>
    
    <span class="c1"># Only LoRA parameters should be trainable</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">LoRALayer</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    
    <span class="c1"># Count new trainable parameters</span>
    <span class="n">lora_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original trainable parameters: </span><span class="si">{</span><span class="n">orig_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LoRA trainable parameters: </span><span class="si">{</span><span class="n">lora_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameter reduction: </span><span class="si">{</span><span class="n">lora_params</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">orig_params</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<p>LoRA provides several advantages:</p>
<ol class="arabic simple">
<li><p><strong>Efficiency</strong>: Typically reduces trainable parameters to &lt;1% of the original model.</p></li>
<li><p><strong>Performance</strong>: Achieves comparable performance to full fine-tuning in many tasks.</p></li>
<li><p><strong>Modularity</strong>: Multiple LoRA adaptations can be created for different tasks and switched without changing the base model.</p></li>
</ol>
</section>
<section id="adapter-layers">
<h4>Adapter Layers<a class="headerlink" href="#adapter-layers" title="Link to this heading">#</a></h4>
<p>Adapters insert small trainable modules within the transformer architecture:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">AdapterLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adapter module with bottleneck architecture.&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">adapter_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Initialize weights</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="other-peft-techniques">
<h4>Other PEFT Techniques<a class="headerlink" href="#other-peft-techniques" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Prefix Tuning</strong>: Prepends trainable vectors to keys and values in attention layers.</p></li>
<li><p><strong>Prompt Tuning</strong>: Adds trainable “soft prompt” tokens to the input.</p></li>
<li><p><strong>BitFit</strong>: Updates only bias terms throughout the network.</p></li>
</ul>
<p><strong>Biological Parallel</strong>: PEFT approaches resemble how the brain can adapt to new tasks by modifying small subnetworks within larger neural circuits while preserving overall structure and general knowledge.</p>
</section>
</section>
<section id="instruction-fine-tuning">
<h3>12.2.3 Instruction Fine-tuning<a class="headerlink" href="#instruction-fine-tuning" title="Link to this heading">#</a></h3>
<p>Instruction fine-tuning adapts models to follow natural language instructions, enhancing their ability to understand and execute user requests:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">prepare_instruction_dataset</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Prepare a dataset for instruction fine-tuning.&quot;&quot;&quot;</span>
    
    <span class="c1"># Example instruction-response pairs</span>
    <span class="n">instruction_data</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;instruction&quot;</span><span class="p">:</span> <span class="s2">&quot;Summarize the following text about climate change.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;Global warming is the long-term heating of Earth&#39;s surface observed since the pre-industrial period due to human activities, primarily fossil fuel burning, which increases heat-trapping greenhouse gas levels in Earth&#39;s atmosphere. It is most commonly measured as the average increase in Earth&#39;s global surface temperature. Since the pre-industrial period, human activities are estimated to have increased Earth&#39;s global average temperature by about 1 degree Celsius (1.8 degrees Fahrenheit), a number that is currently increasing by 0.2 degrees Celsius (0.36 degrees Fahrenheit) per decade.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="s2">&quot;Climate change refers to the long-term warming of Earth caused by human activities, especially burning fossil fuels that release greenhouse gases. The planet has warmed about 1°C since pre-industrial times, with temperature currently rising 0.2°C per decade.&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;instruction&quot;</span><span class="p">:</span> <span class="s2">&quot;Translate this English text to French.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;The quick brown fox jumps over the lazy dog.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="s2">&quot;Le rapide renard brun saute par-dessus le chien paresseux.&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;instruction&quot;</span><span class="p">:</span> <span class="s2">&quot;Write a short poem about mountains.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="s2">&quot;Majestic peaks reach toward the sky,</span><span class="se">\n</span><span class="s2">Ancient stones that time defy.</span><span class="se">\n</span><span class="s2">Snow-capped sentinels standing tall,</span><span class="se">\n</span><span class="s2">Whispering winds, nature&#39;s call.&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># Format data for model training</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">format_instruction</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;### Instruction:</span><span class="se">\n</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;instruction&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">### Input:</span><span class="se">\n</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">### Response:</span><span class="se">\n</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;### Instruction:</span><span class="se">\n</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;instruction&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">### Response:</span><span class="se">\n</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="n">formatted_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">format_instruction</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">instruction_data</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;raw_data&quot;</span><span class="p">:</span> <span class="n">instruction_data</span><span class="p">,</span>
        <span class="s2">&quot;formatted_data&quot;</span><span class="p">:</span> <span class="n">formatted_data</span><span class="p">,</span>
        <span class="s2">&quot;samples&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">instruction_data</span><span class="p">)</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Key aspects of instruction fine-tuning:</p>
<ol class="arabic simple">
<li><p><strong>Dataset Structure</strong>:</p>
<ul class="simple">
<li><p>Instruction-output pairs (with optional input)</p></li>
<li><p>Diverse task coverage</p></li>
<li><p>Consistent formatting</p></li>
</ul>
</li>
<li><p><strong>Training Approach</strong>:</p>
<ul class="simple">
<li><p>Initially trained on human-written instruction-response pairs</p></li>
<li><p>May use a mix of real and synthetic data</p></li>
<li><p>Often combined with PEFT techniques for efficiency</p></li>
</ul>
</li>
<li><p><strong>Performance Considerations</strong>:</p>
<ul class="simple">
<li><p>Quality and diversity of instructions matter</p></li>
<li><p>Template consistency affects generalization</p></li>
<li><p>Task coverage determines capabilities</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: Instruction tuning resembles how humans learn to follow verbal instructions, a capability that develops through exposure to diverse command-response pairings.</p>
</section>
<section id="rlhf-and-alignment">
<h3>12.2.4 RLHF and Alignment<a class="headerlink" href="#rlhf-and-alignment" title="Link to this heading">#</a></h3>
<p>Reinforcement Learning from Human Feedback (RLHF) fine-tunes models to produce outputs that humans prefer, enhancing helpfulness, honesty, and harmlessness:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">rlhf_process</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate the RLHF process components.&quot;&quot;&quot;</span>
    <span class="c1"># 1. Start with an instruction-tuned model (SFT)</span>
    <span class="n">sft_model</span> <span class="o">=</span> <span class="s2">&quot;instruction_tuned_model&quot;</span>
    
    <span class="c1"># 2. Train a reward model from human preferences</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">train_reward_model</span><span class="p">():</span>
        <span class="c1"># Example preference data</span>
        <span class="n">preference_examples</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;How can I improve my programming skills?&quot;</span><span class="p">,</span>
                <span class="s2">&quot;chosen&quot;</span><span class="p">:</span> <span class="s2">&quot;To improve your programming skills, you should practice regularly, work on real projects, learn from code reviews, study different languages and paradigms, and collaborate with other developers. Consistent practice with increasingly challenging problems will help you grow.&quot;</span><span class="p">,</span>
                <span class="s2">&quot;rejected&quot;</span><span class="p">:</span> <span class="s2">&quot;Just code more. You&#39;ll get better eventually.&quot;</span>
            <span class="p">}</span>
        <span class="p">]</span>
        
        <span class="c1"># Train classifier to predict human preferences</span>
        <span class="n">reward_model</span> <span class="o">=</span> <span class="s2">&quot;trained_reward_model&quot;</span>
        <span class="k">return</span> <span class="n">reward_model</span>
    
    <span class="c1"># 3. Fine-tune with reinforcement learning</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">rl_fine_tuning</span><span class="p">(</span><span class="n">sft_model</span><span class="p">,</span> <span class="n">reward_model</span><span class="p">):</span>
        <span class="c1"># PPO algorithm components</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">get_model_outputs</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
            <span class="c1"># Generate multiple responses using the model</span>
            <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;Response 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Response 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Response 3&quot;</span><span class="p">]</span>
        
        <span class="k">def</span><span class="w"> </span><span class="nf">compute_rewards</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">reward_model</span><span class="p">):</span>
            <span class="c1"># Score responses using the reward model</span>
            <span class="k">return</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>
        
        <span class="k">def</span><span class="w"> </span><span class="nf">update_policy</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">responses</span><span class="p">,</span> <span class="n">rewards</span><span class="p">):</span>
            <span class="c1"># Update model to increase probability of high-reward responses</span>
            <span class="k">return</span> <span class="s2">&quot;updated_model&quot;</span>
        
        <span class="c1"># RL training loop (simplified)</span>
        <span class="n">rlhf_model</span> <span class="o">=</span> <span class="n">sft_model</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Example prompt 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Example prompt 2&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
                <span class="c1"># Generate responses from current policy</span>
                <span class="n">responses</span> <span class="o">=</span> <span class="n">get_model_outputs</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">rlhf_model</span><span class="p">)</span>
                
                <span class="c1"># Compute rewards</span>
                <span class="n">rewards</span> <span class="o">=</span> <span class="n">compute_rewards</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">reward_model</span><span class="p">)</span>
                
                <span class="c1"># Update policy to maximize rewards</span>
                <span class="n">rlhf_model</span> <span class="o">=</span> <span class="n">update_policy</span><span class="p">(</span><span class="n">rlhf_model</span><span class="p">,</span> <span class="n">responses</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">rlhf_model</span>
    
    <span class="c1"># Execute RLHF pipeline</span>
    <span class="n">reward_model</span> <span class="o">=</span> <span class="n">train_reward_model</span><span class="p">()</span>
    <span class="n">rlhf_model</span> <span class="o">=</span> <span class="n">rl_fine_tuning</span><span class="p">(</span><span class="n">sft_model</span><span class="p">,</span> <span class="n">reward_model</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;supervised_fine_tuned&quot;</span><span class="p">:</span> <span class="n">sft_model</span><span class="p">,</span>
        <span class="s2">&quot;reward_model&quot;</span><span class="p">:</span> <span class="n">reward_model</span><span class="p">,</span>
        <span class="s2">&quot;rlhf_model&quot;</span><span class="p">:</span> <span class="n">rlhf_model</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>The RLHF process involves:</p>
<ol class="arabic simple">
<li><p><strong>Supervised Fine-Tuning (SFT)</strong>:</p>
<ul class="simple">
<li><p>Train the model on high-quality examples</p></li>
<li><p>Establishes base capabilities for following instructions</p></li>
</ul>
</li>
<li><p><strong>Reward Model Training</strong>:</p>
<ul class="simple">
<li><p>Human evaluators rate or rank model outputs</p></li>
<li><p>Train a reward model to predict human preferences</p></li>
</ul>
</li>
<li><p><strong>RL Optimization</strong>:</p>
<ul class="simple">
<li><p>Fine-tune the SFT model using Proximal Policy Optimization (PPO)</p></li>
<li><p>Optimize for reward model scores while constraining divergence from original model (via KL penalty)</p></li>
</ul>
</li>
<li><p><strong>Challenges</strong>:</p>
<ul class="simple">
<li><p>Reward hacking (optimizing for reward signals rather than true intent)</p></li>
<li><p>Alignment tax (trade-off between capability and alignment)</p></li>
<li><p>Distribution shifts</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: RLHF parallels how humans learn social norms and behavioral guidelines through feedback from others. The brain’s dopaminergic systems provide reinforcement signals, strengthening neural pathways that lead to positive outcomes and weakening those that lead to negative consequences.</p>
</section>
</section>
<section id="prompting-techniques">
<h2>12.3 Prompting Techniques<a class="headerlink" href="#prompting-techniques" title="Link to this heading">#</a></h2>
<p>Prompting has emerged as a powerful way to control and direct LLM behavior without modifying model weights. Effective prompting techniques can dramatically improve model performance on specific tasks and enable capabilities that weren’t explicitly trained.</p>
<p><img alt="Prompting Techniques" src="_images/prompting_techniques.svg" />
<em>Figure 12.3: Various prompting techniques for LLMs, from zero-shot to chain-of-thought prompting and system prompt design.</em></p>
<section id="zero-shot-and-few-shot-learning">
<h3>12.3.1 Zero-Shot and Few-Shot Learning<a class="headerlink" href="#zero-shot-and-few-shot-learning" title="Link to this heading">#</a></h3>
<p>One of the most remarkable capabilities of large language models is their ability to perform tasks with minimal or no examples:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_prompting_techniques</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate zero-shot and few-shot prompting techniques.&quot;&quot;&quot;</span>
    <span class="c1"># Zero-shot prompting: no examples provided</span>
    <span class="n">zero_shot_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Classify the following text into one of these categories: </span>
<span class="s2">    Business, Politics, Technology, Sports, Entertainment.</span>
<span class="s2">    </span>
<span class="s2">    Text: Apple announced their new M3 chip, which they claim offers significant </span>
<span class="s2">    performance improvements over the previous generation.</span>
<span class="s2">    </span>
<span class="s2">    Category:</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Few-shot prompting: providing examples to establish a pattern</span>
    <span class="n">few_shot_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Classify the text into one of these categories: </span>
<span class="s2">    Business, Politics, Technology, Sports, Entertainment.</span>
<span class="s2">    </span>
<span class="s2">    Text: The Federal Reserve has decided to keep interest rates unchanged this quarter.</span>
<span class="s2">    Category: Business</span>
<span class="s2">    </span>
<span class="s2">    Text: The baseball team won their third consecutive championship last night.</span>
<span class="s2">    Category: Sports</span>
<span class="s2">    </span>
<span class="s2">    Text: The prime minister announced new climate initiatives yesterday.</span>
<span class="s2">    Category: Politics</span>
<span class="s2">    </span>
<span class="s2">    Text: Apple announced their new M3 chip, which they claim offers significant </span>
<span class="s2">    performance improvements over the previous generation.</span>
<span class="s2">    Category:</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Format comparison for demonstration purposes</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;zero_shot&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">zero_shot_prompt</span><span class="p">,</span>
            <span class="s2">&quot;tokens&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">zero_shot_prompt</span><span class="o">.</span><span class="n">split</span><span class="p">()),</span>
            <span class="s2">&quot;expected_answer&quot;</span><span class="p">:</span> <span class="s2">&quot;Technology&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;few_shot&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">few_shot_prompt</span><span class="p">,</span>
            <span class="s2">&quot;tokens&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">few_shot_prompt</span><span class="o">.</span><span class="n">split</span><span class="p">()),</span>
            <span class="s2">&quot;expected_answer&quot;</span><span class="p">:</span> <span class="s2">&quot;Technology&quot;</span><span class="p">,</span>
            <span class="s2">&quot;examples_provided&quot;</span><span class="p">:</span> <span class="mi">3</span>
        <span class="p">}</span>
    <span class="p">}</span>
</pre></div>
</div>
<section id="zero-shot-learning">
<h4>Zero-Shot Learning<a class="headerlink" href="#zero-shot-learning" title="Link to this heading">#</a></h4>
<p>Zero-shot prompting involves asking the model to perform a task without any demonstrations:</p>
<ol class="arabic simple">
<li><p><strong>When to Use</strong>:</p>
<ul class="simple">
<li><p>Simple, common tasks that the model has likely encountered in training</p></li>
<li><p>When context length is limited</p></li>
<li><p>For initial exploration of model capabilities</p></li>
</ul>
</li>
<li><p><strong>Limitations</strong>:</p>
<ul class="simple">
<li><p>Less predictable performance</p></li>
<li><p>Lower accuracy on complex or nuanced tasks</p></li>
<li><p>Sensitive to exact wording of the prompt</p></li>
</ul>
</li>
</ol>
</section>
<section id="few-shot-learning">
<h4>Few-Shot Learning<a class="headerlink" href="#few-shot-learning" title="Link to this heading">#</a></h4>
<p>Few-shot prompting provides examples of the desired input-output pattern:</p>
<ol class="arabic simple">
<li><p><strong>When to Use</strong>:</p>
<ul class="simple">
<li><p>Complex tasks requiring specific formats or reasoning</p></li>
<li><p>When consistency in outputs is important</p></li>
<li><p>To guide the model toward specific approaches</p></li>
</ul>
</li>
<li><p><strong>Best Practices</strong>:</p>
<ul class="simple">
<li><p>Use diverse, representative examples</p></li>
<li><p>Match example format exactly to your desired output</p></li>
<li><p>Order examples from simple to complex</p></li>
<li><p>Include both positive and negative examples when appropriate</p></li>
</ul>
</li>
<li><p><strong>Limitations</strong>:</p>
<ul class="simple">
<li><p>Consumes token context</p></li>
<li><p>May not generalize well beyond provided examples</p></li>
<li><p>Can suffer from recency bias (favoring later examples)</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: Few-shot learning resembles how humans rapidly adapt to new tasks after seeing just a few examples, a capability believed to rely on the brain’s meta-learning mechanisms in the prefrontal cortex.</p>
</section>
</section>
<section id="chain-of-thought-prompting">
<h3>12.3.2 Chain-of-Thought Prompting<a class="headerlink" href="#chain-of-thought-prompting" title="Link to this heading">#</a></h3>
<p>Chain-of-Thought (CoT) prompting, introduced by Wei et al. (2022), encourages models to break down complex problems into step-by-step reasoning:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_cot_prompting</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate chain-of-thought prompting for complex reasoning.&quot;&quot;&quot;</span>
    
    <span class="c1"># Standard prompting (direct question)</span>
    <span class="n">standard_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. </span>
<span class="s2">    Each can has 3 tennis balls. How many tennis balls does he have now?</span>
<span class="s2">    </span>
<span class="s2">    Answer:</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Chain-of-thought prompting (with reasoning steps)</span>
    <span class="n">cot_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. </span>
<span class="s2">    Each can has 3 tennis balls. How many tennis balls does he have now?</span>
<span class="s2">    </span>
<span class="s2">    Let&#39;s think through this step by step:</span>
<span class="s2">    1. Initially, Roger has 5 tennis balls.</span>
<span class="s2">    2. He buys 2 cans of tennis balls.</span>
<span class="s2">    3. Each can has 3 tennis balls.</span>
<span class="s2">    4. So from the cans, he gets 2 * 3 = 6 tennis balls.</span>
<span class="s2">    5. In total, he has 5 + 6 = 11 tennis balls.</span>
<span class="s2">    </span>
<span class="s2">    Answer: 11</span>
<span class="s2">    </span>
<span class="s2">    Question: Sarah has 3 boxes of books. Each box has 8 books. She gives away </span>
<span class="s2">    7 books to her friend. How many books does she have left?</span>
<span class="s2">    </span>
<span class="s2">    Let&#39;s think through this step by step:</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Few-shot CoT (providing CoT examples)</span>
    <span class="n">few_shot_cot</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. </span>
<span class="s2">    Each can has 3 tennis balls. How many tennis balls does he have now?</span>
<span class="s2">    </span>
<span class="s2">    Let&#39;s think through this step by step:</span>
<span class="s2">    1. Initially, Roger has 5 tennis balls.</span>
<span class="s2">    2. He buys 2 cans of tennis balls.</span>
<span class="s2">    3. Each can has 3 tennis balls.</span>
<span class="s2">    4. So from the cans, he gets 2 * 3 = 6 tennis balls.</span>
<span class="s2">    5. In total, he has 5 + 6 = 11 tennis balls.</span>
<span class="s2">    </span>
<span class="s2">    Answer: 11</span>
<span class="s2">    </span>
<span class="s2">    Question: Sarah has 3 boxes of books. Each box has 8 books. She gives away </span>
<span class="s2">    7 books to her friend. How many books does she have left?</span>
<span class="s2">    </span>
<span class="s2">    Let&#39;s think through this step by step:</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Zero-shot CoT with trigger phrase</span>
    <span class="n">zero_shot_cot</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Question: Sarah has 3 boxes of books. Each box has 8 books. She gives away </span>
<span class="s2">    7 books to her friend. How many books does she have left?</span>
<span class="s2">    </span>
<span class="s2">    Let&#39;s think through this step by step:</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;standard&quot;</span><span class="p">:</span> <span class="n">standard_prompt</span><span class="p">,</span>
        <span class="s2">&quot;cot_example&quot;</span><span class="p">:</span> <span class="n">cot_prompt</span><span class="p">,</span>
        <span class="s2">&quot;few_shot_cot&quot;</span><span class="p">:</span> <span class="n">few_shot_cot</span><span class="p">,</span>
        <span class="s2">&quot;zero_shot_cot&quot;</span><span class="p">:</span> <span class="n">zero_shot_cot</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Key aspects of Chain-of-Thought prompting:</p>
<ol class="arabic simple">
<li><p><strong>Implementation Approaches</strong>:</p>
<ul class="simple">
<li><p>Few-shot CoT: Provide examples with reasoning steps</p></li>
<li><p>Zero-shot CoT: Use trigger phrases like “Let’s think step by step”</p></li>
<li><p>Self-consistency: Generate multiple reasoning paths and take majority vote</p></li>
</ul>
</li>
<li><p><strong>Effectiveness</strong>:</p>
<ul class="simple">
<li><p>Dramatically improves performance on math, logic, and complex reasoning</p></li>
<li><p>Helps with tasks requiring multi-step reasoning or planning</p></li>
<li><p>Makes reasoning explicit and auditable</p></li>
</ul>
</li>
<li><p><strong>Variations</strong>:</p>
<ul class="simple">
<li><p>Tree of Thoughts: Explore multiple reasoning branches</p></li>
<li><p>Program-of-Thoughts: Use code-like structured reasoning</p></li>
<li><p>Verification of Thoughts: Self-verify each step</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: Chain-of-Thought resembles human explicit reasoning processes where complex problems are broken down into manageable steps, a process associated with the prefrontal cortex’s role in planning and problem-solving.</p>
</section>
<section id="prompt-engineering-best-practices">
<h3>12.3.3 Prompt Engineering Best Practices<a class="headerlink" href="#prompt-engineering-best-practices" title="Link to this heading">#</a></h3>
<p>Crafting effective prompts requires understanding how LLMs process and respond to text:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_prompt_optimization</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show how prompt structure affects model performance.&quot;&quot;&quot;</span>
    <span class="c1"># Basic prompt</span>
    <span class="n">basic_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Extract the companies mentioned in this text: </span>
<span class="s2">    Apple announced a partnership with Microsoft to improve cloud integration.</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Improved structured prompt</span>
    <span class="n">structured_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    TASK: Extract all company names mentioned in the text below.</span>
<span class="s2">    </span>
<span class="s2">    FORMAT: Return a JSON array with the company names: [&quot;Company1&quot;, &quot;Company2&quot;]</span>
<span class="s2">    </span>
<span class="s2">    TEXT: Apple announced a partnership with Microsoft to improve cloud integration.</span>
<span class="s2">    </span>
<span class="s2">    COMPANIES:</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Role-based prompt</span>
    <span class="n">role_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    You are an expert in named entity recognition specialized in identifying </span>
<span class="s2">    organization names. Extract all companies mentioned in the following text.</span>
<span class="s2">    </span>
<span class="s2">    TEXT: Apple announced a partnership with Microsoft to improve cloud integration.</span>
<span class="s2">    </span>
<span class="s2">    Return only the company names as a comma-separated list.</span>
<span class="s2">    COMPANIES:</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;basic&quot;</span><span class="p">:</span> <span class="n">basic_prompt</span><span class="p">,</span>
        <span class="s2">&quot;structured&quot;</span><span class="p">:</span> <span class="n">structured_prompt</span><span class="p">,</span>
        <span class="s2">&quot;role_based&quot;</span><span class="p">:</span> <span class="n">role_prompt</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Best practices for prompt engineering include:</p>
<ol class="arabic simple">
<li><p><strong>Clear Instructions</strong>:</p>
<ul class="simple">
<li><p>Be specific about task requirements</p></li>
<li><p>Define format expectations explicitly</p></li>
<li><p>Use numbered lists for multi-part instructions</p></li>
</ul>
</li>
<li><p><strong>Context and Constraints</strong>:</p>
<ul class="simple">
<li><p>Provide relevant context</p></li>
<li><p>Set boundaries and limitations</p></li>
<li><p>Specify word/character limits if needed</p></li>
</ul>
</li>
<li><p><strong>Examples and Demonstrations</strong>:</p>
<ul class="simple">
<li><p>Include diverse examples (avoid biasing toward specific patterns)</p></li>
<li><p>Match example format to desired output</p></li>
<li><p>Start with simpler examples before complex ones</p></li>
</ul>
</li>
<li><p><strong>Structural Elements</strong>:</p>
<ul class="simple">
<li><p>Use delimiters to separate sections: “```”, “###”, “TEXT:”, etc.</p></li>
<li><p>Label components explicitly: “INPUT:”, “OUTPUT:”, “REASONING:”</p></li>
<li><p>Utilize formatting like bullets, numbering, and indentation</p></li>
</ul>
</li>
<li><p><strong>Iterative Refinement</strong>:</p>
<ul class="simple">
<li><p>Test and revise prompts based on outputs</p></li>
<li><p>Identify and address weaknesses or biases</p></li>
<li><p>Build on successful patterns</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: Prompt engineering resembles how humans craft instructions for others, requiring an understanding of shared context, clear communication, and adaptive refinement based on feedback.</p>
</section>
<section id="system-prompts-and-persona-design">
<h3>12.3.4 System Prompts and Persona Design<a class="headerlink" href="#system-prompts-and-persona-design" title="Link to this heading">#</a></h3>
<p>System prompts set the overall behavior and personality of the model, establishing its role and response style:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_system_prompts</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show different system prompts and their effects on model behavior.&quot;&quot;&quot;</span>
    <span class="c1"># Standard helpful assistant</span>
    <span class="n">standard_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    You are a helpful, harmless, and honest assistant. You answer questions </span>
<span class="s2">    accurately and concisely based on the best available information.</span>
<span class="s2">    </span>
<span class="s2">    User: What are the planets in our solar system?</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Expert persona</span>
    <span class="n">expert_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    You are an expert astronomer with a PhD in planetary science and 15 years </span>
<span class="s2">    of experience at NASA. You communicate complex astronomical concepts clearly </span>
<span class="s2">    while maintaining scientific accuracy. Include relevant data and cite your </span>
<span class="s2">    sources when appropriate.</span>
<span class="s2">    </span>
<span class="s2">    User: What are the planets in our solar system?</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Tailored for children</span>
    <span class="n">child_friendly_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    You are a friendly science teacher for elementary school children. You explain </span>
<span class="s2">    scientific concepts in simple, engaging ways using analogies, fun facts, and </span>
<span class="s2">    age-appropriate language. Keep answers short, around 3-4 sentences, and add </span>
<span class="s2">    a touch of excitement to spark curiosity.</span>
<span class="s2">    </span>
<span class="s2">    User: What are the planets in our solar system?</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Constrained role</span>
    <span class="n">constrained_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    You are an assistant that only provides information about astronomy and space. </span>
<span class="s2">    If asked about any other topic, politely explain that you can only discuss </span>
<span class="s2">    astronomy-related questions. Never break character, regardless of how the user </span>
<span class="s2">    phrases their request.</span>
<span class="s2">    </span>
<span class="s2">    User: What are the planets in our solar system?</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;standard&quot;</span><span class="p">:</span> <span class="n">standard_prompt</span><span class="p">,</span>
        <span class="s2">&quot;expert&quot;</span><span class="p">:</span> <span class="n">expert_prompt</span><span class="p">,</span>
        <span class="s2">&quot;child_friendly&quot;</span><span class="p">:</span> <span class="n">child_friendly_prompt</span><span class="p">,</span>
        <span class="s2">&quot;constrained&quot;</span><span class="p">:</span> <span class="n">constrained_prompt</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Key aspects of system prompts and persona design:</p>
<ol class="arabic simple">
<li><p><strong>Role Definition</strong>:</p>
<ul class="simple">
<li><p>Establish expertise and background</p></li>
<li><p>Set tone, style, and format expectations</p></li>
<li><p>Define operational constraints</p></li>
</ul>
</li>
<li><p><strong>Behavioral Guidelines</strong>:</p>
<ul class="simple">
<li><p>Specify response length and depth</p></li>
<li><p>Set ethical boundaries</p></li>
<li><p>Configure helpfulness vs. conciseness balance</p></li>
</ul>
</li>
<li><p><strong>Purpose-Specific Personae</strong>:</p>
<ul class="simple">
<li><p>Technical expert (code, science, medical)</p></li>
<li><p>Educational assistant (simplified explanations)</p></li>
<li><p>Creative collaborator (brainstorming, writing)</p></li>
<li><p>Task-specific tools (data analysis, summarization)</p></li>
</ul>
</li>
<li><p><strong>Techniques for Testing and Refinement</strong>:</p>
<ul class="simple">
<li><p>Red-teaming: Test persona boundaries with challenging queries</p></li>
<li><p>Comparative evaluation: Test same queries with different personae</p></li>
<li><p>Iterative enhancement: Refine based on observed behaviors</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: System prompts tap into similar mechanisms as social role-playing in humans, where people adjust their language, vocabulary, and behavior based on social context and professional roles, a capability supported by the brain’s theory of mind and social cognition networks.</p>
</section>
</section>
<section id="neural-basis-of-language">
<h2>12.4 Neural Basis of Language<a class="headerlink" href="#neural-basis-of-language" title="Link to this heading">#</a></h2>
<p>Understanding how the human brain processes language can provide insights into both the capabilities and limitations of large language models. This section explores the parallels between neural language processing and computational language models.</p>
<p><img alt="Brain Language Areas" src="_images/brain_language_areas.svg" />
<em>Figure 12.4: Key language processing areas in the human brain and their functional parallels in large language models.</em></p>
<section id="language-areas-in-the-brain">
<h3>12.4.1 Language Areas in the Brain<a class="headerlink" href="#language-areas-in-the-brain" title="Link to this heading">#</a></h3>
<p>The brain’s language network involves several specialized regions that work in concert to process and produce language:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">brain_language_network</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Describe the main brain regions involved in language processing.&quot;&quot;&quot;</span>
    <span class="n">language_areas</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Broca&#39;s Area&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;location&quot;</span><span class="p">:</span> <span class="s2">&quot;Left inferior frontal gyrus&quot;</span><span class="p">,</span>
            <span class="s2">&quot;functions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Speech production&quot;</span><span class="p">,</span> <span class="s2">&quot;Syntactic processing&quot;</span><span class="p">,</span> <span class="s2">&quot;Grammatical operations&quot;</span><span class="p">],</span>
            <span class="s2">&quot;damage_effects&quot;</span><span class="p">:</span> <span class="s2">&quot;Broca&#39;s aphasia: slow, effortful speech with simplified grammar&quot;</span><span class="p">,</span>
            <span class="s2">&quot;llm_parallel&quot;</span><span class="p">:</span> <span class="s2">&quot;Feed-forward networks and output layers in language generation&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Wernicke&#39;s Area&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;location&quot;</span><span class="p">:</span> <span class="s2">&quot;Left superior temporal gyrus&quot;</span><span class="p">,</span>
            <span class="s2">&quot;functions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Language comprehension&quot;</span><span class="p">,</span> <span class="s2">&quot;Semantic processing&quot;</span><span class="p">,</span> <span class="s2">&quot;Word meaning&quot;</span><span class="p">],</span>
            <span class="s2">&quot;damage_effects&quot;</span><span class="p">:</span> <span class="s2">&quot;Wernicke&#39;s aphasia: fluent but meaningless speech, poor comprehension&quot;</span><span class="p">,</span>
            <span class="s2">&quot;llm_parallel&quot;</span><span class="p">:</span> <span class="s2">&quot;Self-attention mechanisms and contextual representations&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Angular Gyrus&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;location&quot;</span><span class="p">:</span> <span class="s2">&quot;Posterior part of the inferior parietal lobule&quot;</span><span class="p">,</span>
            <span class="s2">&quot;functions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Semantic integration&quot;</span><span class="p">,</span> <span class="s2">&quot;Cross-modal associations&quot;</span><span class="p">,</span> <span class="s2">&quot;Reading comprehension&quot;</span><span class="p">],</span>
            <span class="s2">&quot;damage_effects&quot;</span><span class="p">:</span> <span class="s2">&quot;Alexia, agraphia, anomia (naming difficulties)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;llm_parallel&quot;</span><span class="p">:</span> <span class="s2">&quot;Cross-attention between different embedding spaces&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Arcuate Fasciculus&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;location&quot;</span><span class="p">:</span> <span class="s2">&quot;White matter tract connecting Broca&#39;s and Wernicke&#39;s areas&quot;</span><span class="p">,</span>
            <span class="s2">&quot;functions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Connects language comprehension and production&quot;</span><span class="p">,</span> <span class="s2">&quot;Facilitates repetition&quot;</span><span class="p">],</span>
            <span class="s2">&quot;damage_effects&quot;</span><span class="p">:</span> <span class="s2">&quot;Conduction aphasia: difficulty repeating heard phrases&quot;</span><span class="p">,</span>
            <span class="s2">&quot;llm_parallel&quot;</span><span class="p">:</span> <span class="s2">&quot;Information flow from encoder to decoder in seq2seq models&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Temporal Lobe&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;location&quot;</span><span class="p">:</span> <span class="s2">&quot;Middle and inferior temporal regions&quot;</span><span class="p">,</span>
            <span class="s2">&quot;functions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Semantic memory&quot;</span><span class="p">,</span> <span class="s2">&quot;Word meaning storage&quot;</span><span class="p">,</span> <span class="s2">&quot;Concept relations&quot;</span><span class="p">],</span>
            <span class="s2">&quot;damage_effects&quot;</span><span class="p">:</span> <span class="s2">&quot;Semantic dementia, naming deficits&quot;</span><span class="p">,</span>
            <span class="s2">&quot;llm_parallel&quot;</span><span class="p">:</span> <span class="s2">&quot;Embedding representations in the model&quot;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">language_areas</span>
</pre></div>
</div>
<p>Key insights about brain language processing:</p>
<ol class="arabic simple">
<li><p><strong>Distributed Processing</strong>:</p>
<ul class="simple">
<li><p>Language processing involves multiple brain regions working in parallel</p></li>
<li><p>Different aspects of language (phonology, semantics, syntax) engage different neural circuits</p></li>
<li><p>Bilateral involvement with left-hemisphere dominance in most people</p></li>
</ul>
</li>
<li><p><strong>Hierarchical Organization</strong>:</p>
<ul class="simple">
<li><p>Primary auditory cortex processes basic speech sounds</p></li>
<li><p>Secondary areas recognize phonemes and word forms</p></li>
<li><p>Association areas integrate meaning and context</p></li>
<li><p>Frontal regions coordinate grammar and planning</p></li>
</ul>
</li>
<li><p><strong>Connection to LLMs</strong>:</p>
<ul class="simple">
<li><p>LLMs’ distributed representation system parallels the brain’s semantic networks</p></li>
<li><p>Layer-wise processing in deep networks resembles cortical hierarchies</p></li>
<li><p>Attention mechanisms parallel neural selective enhancement</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: The specialized but interconnected language regions in the brain mirror how LLMs develop specialized components within their network structure during training.</p>
</section>
<section id="predictive-processing-in-language">
<h3>12.4.2 Predictive Processing in Language<a class="headerlink" href="#predictive-processing-in-language" title="Link to this heading">#</a></h3>
<p>The brain actively predicts upcoming linguistic elements rather than simply reacting to input:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_predictive_processing</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate predictive processing in language comprehension.&quot;&quot;&quot;</span>
    <span class="c1"># Examples of sentences with varying predictability</span>
    <span class="n">high_predictability</span> <span class="o">=</span> <span class="s2">&quot;The chef cooked the meal in the ______.&quot;</span> <span class="c1"># kitchen</span>
    <span class="n">medium_predictability</span> <span class="o">=</span> <span class="s2">&quot;The student wrote notes with a ______.&quot;</span> <span class="c1"># pen/pencil</span>
    <span class="n">low_predictability</span> <span class="o">=</span> <span class="s2">&quot;The person found a ______.&quot;</span> <span class="c1"># many possibilities</span>
    
    <span class="c1"># Simplified N400 response simulation</span>
    <span class="c1"># (N400 is an ERP component that indexes semantic predictability)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">simulate_n400</span><span class="p">(</span><span class="n">sentence_completion_predictability</span><span class="p">):</span>
        <span class="c1"># Higher predictability = lower N400 amplitude</span>
        <span class="k">if</span> <span class="n">sentence_completion_predictability</span> <span class="o">==</span> <span class="s2">&quot;high&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.2</span>  <span class="c1"># Small N400 (expected word)</span>
        <span class="k">elif</span> <span class="n">sentence_completion_predictability</span> <span class="o">==</span> <span class="s2">&quot;medium&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.5</span>  <span class="c1"># Moderate N400</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.9</span>  <span class="c1"># Large N400 (unexpected word)</span>
    
    <span class="c1"># Predictive processing illustration</span>
    <span class="n">example_sentences</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;high_pred&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="n">high_predictability</span><span class="p">,</span>
            <span class="s2">&quot;completions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;kitchen&quot;</span><span class="p">,</span> <span class="s2">&quot;oven&quot;</span><span class="p">,</span> <span class="s2">&quot;microwave&quot;</span><span class="p">],</span>
            <span class="s2">&quot;n400_amplitudes&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">simulate_n400</span><span class="p">(</span><span class="s2">&quot;high&quot;</span><span class="p">),</span>
                <span class="n">simulate_n400</span><span class="p">(</span><span class="s2">&quot;medium&quot;</span><span class="p">),</span>
                <span class="n">simulate_n400</span><span class="p">(</span><span class="s2">&quot;medium&quot;</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">},</span>
        <span class="s2">&quot;medium_pred&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="n">medium_predictability</span><span class="p">,</span>
            <span class="s2">&quot;completions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;pen&quot;</span><span class="p">,</span> <span class="s2">&quot;pencil&quot;</span><span class="p">,</span> <span class="s2">&quot;typewriter&quot;</span><span class="p">,</span> <span class="s2">&quot;rock&quot;</span><span class="p">],</span>
            <span class="s2">&quot;n400_amplitudes&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">simulate_n400</span><span class="p">(</span><span class="s2">&quot;medium&quot;</span><span class="p">),</span>
                <span class="n">simulate_n400</span><span class="p">(</span><span class="s2">&quot;medium&quot;</span><span class="p">),</span>
                <span class="n">simulate_n400</span><span class="p">(</span><span class="s2">&quot;medium&quot;</span><span class="p">),</span>
                <span class="n">simulate_n400</span><span class="p">(</span><span class="s2">&quot;low&quot;</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">},</span>
        <span class="s2">&quot;low_pred&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="n">low_predictability</span><span class="p">,</span>
            <span class="s2">&quot;completions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;book&quot;</span><span class="p">,</span> <span class="s2">&quot;car&quot;</span><span class="p">,</span> <span class="s2">&quot;friend&quot;</span><span class="p">,</span> <span class="s2">&quot;solution&quot;</span><span class="p">],</span>
            <span class="s2">&quot;n400_amplitudes&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">simulate_n400</span><span class="p">(</span><span class="s2">&quot;low&quot;</span><span class="p">),</span>
                <span class="n">simulate_n400</span><span class="p">(</span><span class="s2">&quot;low&quot;</span><span class="p">),</span>
                <span class="n">simulate_n400</span><span class="p">(</span><span class="s2">&quot;low&quot;</span><span class="p">),</span>
                <span class="n">simulate_n400</span><span class="p">(</span><span class="s2">&quot;low&quot;</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="c1"># Parallel with LLM next-token prediction</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">llm_next_token_prediction</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
        <span class="c1"># Simplified example of how LLMs predict next tokens</span>
        <span class="c1"># In reality, they would output a probability distribution over the vocabulary</span>
        <span class="k">if</span> <span class="n">sentence</span> <span class="o">==</span> <span class="n">high_predictability</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;kitchen&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span> <span class="s2">&quot;oven&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s2">&quot;pot&quot;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">}</span>
        <span class="k">elif</span> <span class="n">sentence</span> <span class="o">==</span> <span class="n">medium_predictability</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;pen&quot;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span> <span class="s2">&quot;pencil&quot;</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span> <span class="s2">&quot;marker&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;book&quot;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span> <span class="s2">&quot;car&quot;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">,</span> <span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">}</span>
            <span class="c1"># More uniform distribution for low predictability</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;examples&quot;</span><span class="p">:</span> <span class="n">example_sentences</span><span class="p">,</span>
        <span class="s2">&quot;llm_predictions&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;high_pred&quot;</span><span class="p">:</span> <span class="n">llm_next_token_prediction</span><span class="p">(</span><span class="n">high_predictability</span><span class="p">),</span>
            <span class="s2">&quot;medium_pred&quot;</span><span class="p">:</span> <span class="n">llm_next_token_prediction</span><span class="p">(</span><span class="n">medium_predictability</span><span class="p">),</span>
            <span class="s2">&quot;low_pred&quot;</span><span class="p">:</span> <span class="n">llm_next_token_prediction</span><span class="p">(</span><span class="n">low_predictability</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Key aspects of predictive processing:</p>
<ol class="arabic simple">
<li><p><strong>Neural Evidence</strong>:</p>
<ul class="simple">
<li><p>N400 ERP component: Larger amplitude for unexpected words</p></li>
<li><p>Reduced neural activity for predictable language elements</p></li>
<li><p>Prediction error signals drive learning</p></li>
</ul>
</li>
<li><p><strong>Hierarchical Prediction</strong>:</p>
<ul class="simple">
<li><p>Higher-level predictions about meaning and intent</p></li>
<li><p>Mid-level predictions about syntax and structure</p></li>
<li><p>Low-level predictions about word forms and sounds</p></li>
</ul>
</li>
<li><p><strong>Connection to LLMs</strong>:</p>
<ul class="simple">
<li><p>Next-token prediction objective aligns with brain’s prediction mechanisms</p></li>
<li><p>Surprisal (negative log probability) correlates with human processing difficulty</p></li>
<li><p>Attention patterns resemble predictive focus in human reading</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: Both brains and LLMs generate expectations about upcoming linguistic content, with prediction errors driving learning and adaptation.</p>
</section>
<section id="compositional-representations">
<h3>12.4.3 Compositional Representations<a class="headerlink" href="#compositional-representations" title="Link to this heading">#</a></h3>
<p>Both the brain and LLMs must represent meaning by composing elements into structured wholes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_compositionality</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show compositional representations in language understanding.&quot;&quot;&quot;</span>
    <span class="c1"># Examples demonstrating compositional understanding</span>
    <span class="n">examples</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="s2">&quot;The black cat chased the small mouse.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;compositional_elements&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;mouse&quot;</span><span class="p">],</span>
                <span class="s2">&quot;attributes&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;cat&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;black&quot;</span><span class="p">],</span> <span class="s2">&quot;mouse&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;small&quot;</span><span class="p">]},</span>
                <span class="s2">&quot;relation&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;agent&quot;</span><span class="p">:</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;chase&quot;</span><span class="p">,</span> <span class="s2">&quot;patient&quot;</span><span class="p">:</span> <span class="s2">&quot;mouse&quot;</span><span class="p">}</span>
            <span class="p">}</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="s2">&quot;The small mouse was chased by the black cat.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;compositional_elements&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;mouse&quot;</span><span class="p">],</span>
                <span class="s2">&quot;attributes&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;cat&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;black&quot;</span><span class="p">],</span> <span class="s2">&quot;mouse&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;small&quot;</span><span class="p">]},</span>
                <span class="s2">&quot;relation&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;agent&quot;</span><span class="p">:</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;chase&quot;</span><span class="p">,</span> <span class="s2">&quot;patient&quot;</span><span class="p">:</span> <span class="s2">&quot;mouse&quot;</span><span class="p">}</span>
            <span class="p">},</span>
            <span class="s2">&quot;note&quot;</span><span class="p">:</span> <span class="s2">&quot;Same meaning as previous example despite different surface form&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="s2">&quot;The chef who had won the competition prepared the meal.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;compositional_elements&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;chef&quot;</span><span class="p">,</span> <span class="s2">&quot;competition&quot;</span><span class="p">,</span> <span class="s2">&quot;meal&quot;</span><span class="p">],</span>
                <span class="s2">&quot;relations&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;agent&quot;</span><span class="p">:</span> <span class="s2">&quot;chef&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;win&quot;</span><span class="p">,</span> <span class="s2">&quot;patient&quot;</span><span class="p">:</span> <span class="s2">&quot;competition&quot;</span><span class="p">},</span>
                    <span class="p">{</span><span class="s2">&quot;agent&quot;</span><span class="p">:</span> <span class="s2">&quot;chef&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">:</span> <span class="s2">&quot;prepare&quot;</span><span class="p">,</span> <span class="s2">&quot;patient&quot;</span><span class="p">:</span> <span class="s2">&quot;meal&quot;</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;nesting&quot;</span><span class="p">:</span> <span class="s2">&quot;First relation is embedded within subject noun phrase&quot;</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># Demonstration of how LLMs process compositional structures</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">llm_compositionality_handling</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simplified representation of how LLMs process compositional structures.&quot;&quot;&quot;</span>
        <span class="c1"># In reality, this is handled by the distributed representations in the network</span>
        <span class="c1"># and attention patterns across tokens</span>
        <span class="k">if</span> <span class="s2">&quot;cat chased&quot;</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span>
                <span class="s2">&quot;attention_pattern&quot;</span><span class="p">:</span> <span class="s2">&quot;Subject tokens attend to verb, verb attends to object&quot;</span><span class="p">,</span>
                <span class="s2">&quot;representation&quot;</span><span class="p">:</span> <span class="s2">&quot;Distributed activation pattern capturing agent-action-patient&quot;</span><span class="p">,</span>
                <span class="s2">&quot;inference_capability&quot;</span><span class="p">:</span> <span class="s2">&quot;Can answer &#39;Who did the chasing?&#39; correctly&quot;</span>
            <span class="p">}</span>
        <span class="k">elif</span> <span class="s2">&quot;was chased by&quot;</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span>
                <span class="s2">&quot;attention_pattern&quot;</span><span class="p">:</span> <span class="s2">&quot;Object tokens attend to passive verb, &#39;by&#39; attends to agent&quot;</span><span class="p">,</span>
                <span class="s2">&quot;representation&quot;</span><span class="p">:</span> <span class="s2">&quot;Different surface pattern but similar final semantic representation&quot;</span><span class="p">,</span>
                <span class="s2">&quot;inference_capability&quot;</span><span class="p">:</span> <span class="s2">&quot;Can still answer &#39;Who did the chasing?&#39; correctly&quot;</span>
            <span class="p">}</span>
        <span class="k">elif</span> <span class="s2">&quot;who had won&quot;</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span>
                <span class="s2">&quot;attention_pattern&quot;</span><span class="p">:</span> <span class="s2">&quot;Complex pattern with relative clause attended to by main subject&quot;</span><span class="p">,</span>
                <span class="s2">&quot;representation&quot;</span><span class="p">:</span> <span class="s2">&quot;Hierarchical structure with embedded relation&quot;</span><span class="p">,</span>
                <span class="s2">&quot;inference_capability&quot;</span><span class="p">:</span> <span class="s2">&quot;Can answer questions about both events&quot;</span>
            <span class="p">}</span>
    
    <span class="n">llm_handling</span> <span class="o">=</span> <span class="p">{</span><span class="n">ex</span><span class="p">[</span><span class="s2">&quot;sentence&quot;</span><span class="p">]:</span> <span class="n">llm_compositionality_handling</span><span class="p">(</span><span class="n">ex</span><span class="p">[</span><span class="s2">&quot;sentence&quot;</span><span class="p">])</span> <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;examples&quot;</span><span class="p">:</span> <span class="n">examples</span><span class="p">,</span>
        <span class="s2">&quot;llm_handling&quot;</span><span class="p">:</span> <span class="n">llm_handling</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Key aspects of compositional representation:</p>
<ol class="arabic simple">
<li><p><strong>Structural Binding</strong>:</p>
<ul class="simple">
<li><p>Brain combines concepts while preserving their relationships</p></li>
<li><p>Neural synchrony may help bind related elements</p></li>
<li><p>Working memory coordinates structural relationships</p></li>
</ul>
</li>
<li><p><strong>Recursive Processing</strong>:</p>
<ul class="simple">
<li><p>Brain processes nested structures (phrases within phrases)</p></li>
<li><p>Compositional hierarchy enables unlimited expressivity</p></li>
<li><p>Structure-sensitive operations follow grammatical rules</p></li>
</ul>
</li>
<li><p><strong>Connection to LLMs</strong>:</p>
<ul class="simple">
<li><p>Self-attention creates context-sensitive representations of words</p></li>
<li><p>Multi-head attention captures different types of dependencies</p></li>
<li><p>LLMs learn compositional patterns implicitly through training</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: Both brains and LLMs must solve the binding problem—representing which properties belong to which entities and how entities relate to each other in complex scenes.</p>
</section>
<section id="semantic-and-syntactic-processing">
<h3>12.4.4 Semantic and Syntactic Processing<a class="headerlink" href="#semantic-and-syntactic-processing" title="Link to this heading">#</a></h3>
<p>The brain processes both the meaning (semantics) and structure (syntax) of language:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">semantic_syntactic_processing</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate semantic and syntactic processing in the brain and LLMs.&quot;&quot;&quot;</span>
    <span class="c1"># Examples demonstrating semantics vs. syntax</span>
    <span class="n">examples</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Semantic violation&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="s2">&quot;The coffee drank the man.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Semantic anomaly: inanimate objects cannot drink&quot;</span><span class="p">,</span>
            <span class="s2">&quot;brain_response&quot;</span><span class="p">:</span> <span class="s2">&quot;N400 effect (semantic processing difficulty)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;brain_regions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Temporal lobe&quot;</span><span class="p">,</span> <span class="s2">&quot;Angular gyrus&quot;</span><span class="p">,</span> <span class="s2">&quot;Inferior frontal gyrus&quot;</span><span class="p">]</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Syntactic violation&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="s2">&quot;The man drinking coffee the.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Grammatical error: incorrect word order&quot;</span><span class="p">,</span>
            <span class="s2">&quot;brain_response&quot;</span><span class="p">:</span> <span class="s2">&quot;P600 effect (syntactic processing difficulty)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;brain_regions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Broca&#39;s area&quot;</span><span class="p">,</span> <span class="s2">&quot;Left anterior temporal lobe&quot;</span><span class="p">,</span> <span class="s2">&quot;Basal ganglia&quot;</span><span class="p">]</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Garden path sentence&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="s2">&quot;The horse raced past the barn fell.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Initially parsed as active verb (&#39;raced&#39;), must be reanalyzed as passive participle&quot;</span><span class="p">,</span>
            <span class="s2">&quot;brain_response&quot;</span><span class="p">:</span> <span class="s2">&quot;P600 effect upon encountering &#39;fell&#39;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;brain_regions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Broca&#39;s area&quot;</span><span class="p">,</span> <span class="s2">&quot;Left inferior frontal gyrus&quot;</span><span class="p">,</span> <span class="s2">&quot;Anterior cingulate&quot;</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># How LLMs handle semantic vs. syntactic anomalies</span>
    <span class="n">llm_responses</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;semantic_violation&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;perplexity&quot;</span><span class="p">:</span> <span class="s2">&quot;High token-level perplexity on &#39;drank&#39;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;handling&quot;</span><span class="p">:</span> <span class="s2">&quot;May correct to &#39;The man drank the coffee&#39; or flag semantic issue&quot;</span><span class="p">,</span>
            <span class="s2">&quot;attention&quot;</span><span class="p">:</span> <span class="s2">&quot;Strong cross-attention between subject and verb tokens&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;syntactic_violation&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;perplexity&quot;</span><span class="p">:</span> <span class="s2">&quot;High perplexity on final &#39;the&#39;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;handling&quot;</span><span class="p">:</span> <span class="s2">&quot;May attempt to complete or restructure the sentence&quot;</span><span class="p">,</span>
            <span class="s2">&quot;attention&quot;</span><span class="p">:</span> <span class="s2">&quot;Disrupted attention patterns reflecting grammatical expectations&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;garden_path&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;perplexity&quot;</span><span class="p">:</span> <span class="s2">&quot;Spike in perplexity at &#39;fell&#39;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;handling&quot;</span><span class="p">:</span> <span class="s2">&quot;May require sufficient context window to resolve correctly&quot;</span><span class="p">,</span>
            <span class="s2">&quot;attention&quot;</span><span class="p">:</span> <span class="s2">&quot;Complex attention pattern revision when encountering &#39;fell&#39;&quot;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="c1"># Double dissociation in language disorders</span>
    <span class="n">clinical_evidence</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;semantic_disorders&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;condition&quot;</span><span class="p">:</span> <span class="s2">&quot;Semantic dementia&quot;</span><span class="p">,</span>
            <span class="s2">&quot;symptoms&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Loss of word meanings&quot;</span><span class="p">,</span> <span class="s2">&quot;Preserved grammar&quot;</span><span class="p">,</span> <span class="s2">&quot;Fluent but empty speech&quot;</span><span class="p">],</span>
            <span class="s2">&quot;brain_areas&quot;</span><span class="p">:</span> <span class="s2">&quot;Anterior temporal lobes&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;syntactic_disorders&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;condition&quot;</span><span class="p">:</span> <span class="s2">&quot;Agrammatic aphasia&quot;</span><span class="p">,</span>
            <span class="s2">&quot;symptoms&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Impaired grammar&quot;</span><span class="p">,</span> <span class="s2">&quot;Preserved word meaning&quot;</span><span class="p">,</span> <span class="s2">&quot;Telegraphic speech&quot;</span><span class="p">],</span>
            <span class="s2">&quot;brain_areas&quot;</span><span class="p">:</span> <span class="s2">&quot;Left inferior frontal gyrus (Broca&#39;s area)&quot;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;examples&quot;</span><span class="p">:</span> <span class="n">examples</span><span class="p">,</span>
        <span class="s2">&quot;llm_responses&quot;</span><span class="p">:</span> <span class="n">llm_responses</span><span class="p">,</span>
        <span class="s2">&quot;clinical_evidence&quot;</span><span class="p">:</span> <span class="n">clinical_evidence</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Key insights about semantic and syntactic processing:</p>
<ol class="arabic simple">
<li><p><strong>Distinct Neural Substrates</strong>:</p>
<ul class="simple">
<li><p>Semantic processing primarily engages temporal and parietal regions</p></li>
<li><p>Syntactic processing relies on frontal regions and basal ganglia</p></li>
<li><p>Double dissociation in language disorders supports this distinction</p></li>
</ul>
</li>
<li><p><strong>Integration Mechanisms</strong>:</p>
<ul class="simple">
<li><p>White matter tracts connect semantic and syntactic processing areas</p></li>
<li><p>Working memory coordinates integration of meaning and structure</p></li>
<li><p>Context modulates the interplay between semantics and syntax</p></li>
</ul>
</li>
<li><p><strong>Connection to LLMs</strong>:</p>
<ul class="simple">
<li><p>Different attention heads specialize in semantic vs. syntactic relationships</p></li>
<li><p>Layer hierarchy processes increasingly abstract linguistic features</p></li>
<li><p>Token perplexity spikes for both semantic and syntactic violations</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: The distinction between semantic and syntactic processing in the brain is reflected in the specialization of different components within LLMs, though LLMs integrate these aspects more diffusely across their network.</p>
</section>
</section>
<section id="limitations-and-challenges">
<h2>12.5 Limitations and Challenges<a class="headerlink" href="#limitations-and-challenges" title="Link to this heading">#</a></h2>
<p>While LLMs have shown remarkable capabilities, they also face significant limitations and challenges. Understanding these constraints is essential for effective deployment and ongoing development of these models.</p>
<section id="hallucinations-and-factuality">
<h3>12.5.1 Hallucinations and Factuality<a class="headerlink" href="#hallucinations-and-factuality" title="Link to this heading">#</a></h3>
<p>LLMs can generate plausible-sounding but factually incorrect content, a phenomenon known as hallucination:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_hallucinations</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate LLM hallucinations and factuality issues.&quot;&quot;&quot;</span>
    <span class="c1"># Examples of hallucinations</span>
    <span class="n">hallucination_examples</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of Wakanda?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;response&quot;</span><span class="p">:</span> <span class="s2">&quot;The capital of Wakanda is Birnin Zana, also known as the Golden City. It is the technological and cultural center of the nation, home to the royal palace and the Wakandan Design Group.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Factual hallucination: Wakanda is a fictional country from Marvel comics, not a real nation&quot;</span><span class="p">,</span>
            <span class="s2">&quot;confidence&quot;</span><span class="p">:</span> <span class="s2">&quot;High confidence despite being fictional&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;Explain the Hendricks-Palmer theory of quantum gravity.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;response&quot;</span><span class="p">:</span> <span class="s2">&quot;The Hendricks-Palmer theory of quantum gravity, proposed by physicists Sarah Hendricks and Robert Palmer in 2008, attempts to reconcile general relativity with quantum mechanics by introducing &#39;quantum spacetime nodes&#39; that form a discrete structure at the Planck scale. The theory suggests that gravity emerges from entanglement entropy between these nodes, which fluctuate according to modified Heisenberg uncertainty principles.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Fabrication hallucination: This theory and these physicists do not exist&quot;</span><span class="p">,</span>
            <span class="s2">&quot;confidence&quot;</span><span class="p">:</span> <span class="s2">&quot;Elaborately detailed despite being completely fictional&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;What are the main points in the 2023 Supreme Court case Johnson v. Microsoft?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;response&quot;</span><span class="p">:</span> <span class="s2">&quot;In the 2023 Supreme Court case Johnson v. Microsoft, the Court ruled 6-3 that tech companies can be held liable for algorithmic discrimination even without proof of discriminatory intent. Key points included: 1) Establishing the &#39;disparate outcome&#39; test for algorithmic systems, 2) Extending Civil Rights Act protections to AI decision contexts, and 3) Creating a new standard for technological due process.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Temporal hallucination: Describes a court case that didn&#39;t happen as if it did&quot;</span><span class="p">,</span>
            <span class="s2">&quot;confidence&quot;</span><span class="p">:</span> <span class="s2">&quot;Specific details about a non-existent case&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># Strategies to mitigate hallucinations</span>
    <span class="n">mitigation_strategies</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Retrieval-Augmented Generation (RAG)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Ground model outputs in reliable external information&quot;</span><span class="p">,</span>
            <span class="s2">&quot;implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;Retrieve relevant documents from curated sources and condition generation on this information&quot;</span><span class="p">,</span>
            <span class="s2">&quot;trade_offs&quot;</span><span class="p">:</span> <span class="s2">&quot;Requires maintaining external knowledge base; retrieval quality affects output&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Self-Consistency Checking&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Have model verify its own outputs&quot;</span><span class="p">,</span>
            <span class="s2">&quot;implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;Generate multiple responses and cross-check, or explicitly ask model to verify factual claims&quot;</span><span class="p">,</span>
            <span class="s2">&quot;trade_offs&quot;</span><span class="p">:</span> <span class="s2">&quot;Can miss systematic errors; model may be confidently wrong&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Uncertainty Quantification&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Encourage model to express uncertainty about claims&quot;</span><span class="p">,</span>
            <span class="s2">&quot;implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;Fine-tune model to calibrate confidence or use sampling techniques to estimate uncertainty&quot;</span><span class="p">,</span>
            <span class="s2">&quot;trade_offs&quot;</span><span class="p">:</span> <span class="s2">&quot;May reduce usefulness for some applications by being overly cautious&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Human Feedback and Oversight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Keep humans in the loop for fact-checking&quot;</span><span class="p">,</span>
            <span class="s2">&quot;implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;Use RLHF to reward accurate responses, implement human review of critical outputs&quot;</span><span class="p">,</span>
            <span class="s2">&quot;trade_offs&quot;</span><span class="p">:</span> <span class="s2">&quot;Scales poorly; humans can also make errors or have biases&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;examples&quot;</span><span class="p">:</span> <span class="n">hallucination_examples</span><span class="p">,</span>
        <span class="s2">&quot;mitigation&quot;</span><span class="p">:</span> <span class="n">mitigation_strategies</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Key challenges with factuality include:</p>
<ol class="arabic simple">
<li><p><strong>Types of Hallucinations</strong>:</p>
<ul class="simple">
<li><p>Confabulation: Creating entirely fictitious information</p></li>
<li><p>Conflation: Mixing facts from different sources or contexts</p></li>
<li><p>Temporal confusion: Citing future events or outdated information</p></li>
<li><p>Over-precision: Providing specific details beyond what’s known</p></li>
</ul>
</li>
<li><p><strong>Causes of Hallucinations</strong>:</p>
<ul class="simple">
<li><p>Training methodology: Optimizing for plausibility rather than accuracy</p></li>
<li><p>Parametric knowledge: Relying on weights instead of external sources</p></li>
<li><p>Distribution shift: Encountering queries outside training distribution</p></li>
<li><p>Prompt misinterpretation: Misunderstanding user intent</p></li>
</ul>
</li>
<li><p><strong>Impact on Applications</strong>:</p>
<ul class="simple">
<li><p>Critical in domains requiring factual accuracy (medicine, law, education)</p></li>
<li><p>Deceptive content may propagate misinformation</p></li>
<li><p>Users may over-trust plausible-sounding but incorrect information</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: The human brain also produces confabulations, especially in certain neurological conditions like Korsakoff’s syndrome. However, healthy humans typically have better calibrated confidence and can distinguish between knowledge and speculation.</p>
</section>
<section id="bias-and-fairness">
<h3>12.5.2 Bias and Fairness<a class="headerlink" href="#bias-and-fairness" title="Link to this heading">#</a></h3>
<p>LLMs can reflect, amplify, or introduce various biases present in their training data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">bias_examples</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate biases in language models and mitigation approaches.&quot;&quot;&quot;</span>
    <span class="c1"># Examples of different types of bias</span>
    <span class="n">bias_examples</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Gender bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;I need to hire a babysitter and a programmer.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;biased_completion&quot;</span><span class="p">:</span> <span class="s2">&quot;She should be good with kids. He should know Python and JavaScript.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Gender stereotyping of occupations&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Cultural/Western bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;What does a traditional wedding look like?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;biased_completion&quot;</span><span class="p">:</span> <span class="s2">&quot;A traditional wedding typically takes place in a church with the bride in a white dress...&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Defaults to Western cultural norms without acknowledging diversity&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Representation bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;Show me a picture of a CEO.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;biased_completion&quot;</span><span class="p">:</span> <span class="s2">&quot;I can&#39;t generate images, but CEOs are typically portrayed as middle-aged white men in business attire...&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Reinforces underrepresentation patterns from training data&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Political bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the best approach to economic policy?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;biased_completion&quot;</span><span class="p">:</span> <span class="s2">&quot;The most effective economic policy focuses on market solutions with minimal government intervention...&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Presents one political perspective as objective fact&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># Bias evaluation frameworks</span>
    <span class="n">evaluation_methods</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;Bias benchmark datasets&quot;</span><span class="p">,</span>
            <span class="s2">&quot;examples&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;BOLD&quot;</span><span class="p">,</span> <span class="s2">&quot;WinoBias&quot;</span><span class="p">,</span> <span class="s2">&quot;StereoSet&quot;</span><span class="p">,</span> <span class="s2">&quot;CrowS-Pairs&quot;</span><span class="p">],</span>
            <span class="s2">&quot;measures&quot;</span><span class="p">:</span> <span class="s2">&quot;Stereotype associations, representation disparities, preference patterns&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;Counterfactual testing&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Change protected attributes in prompts and measure output differences&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;Compare responses for &#39;I am a Black person...&#39; vs &#39;I am a white person...&#39;&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;Red-teaming&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Adversarial testing to find and exploit biases&quot;</span><span class="p">,</span>
            <span class="s2">&quot;implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;Expert teams probe model boundaries and failure modes&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># Bias mitigation strategies</span>
    <span class="n">mitigation_strategies</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Training data curation&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Carefully select and balance training data&quot;</span><span class="p">,</span>
            <span class="s2">&quot;challenges&quot;</span><span class="p">:</span> <span class="s2">&quot;Hard to scale, difficult to address all bias dimensions&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;RLHF for fairness&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Use human feedback to reduce biased outputs&quot;</span><span class="p">,</span>
            <span class="s2">&quot;implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;Train reward models to penalize unfair responses&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;System prompts&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Design prompts that encourage fairness and balance&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;Include explicit instructions to consider diverse perspectives&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Post-processing filters&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Detect and mitigate biased outputs after generation&quot;</span><span class="p">,</span>
            <span class="s2">&quot;limitation&quot;</span><span class="p">:</span> <span class="s2">&quot;May reduce model expressiveness or create new biases&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;examples&quot;</span><span class="p">:</span> <span class="n">bias_examples</span><span class="p">,</span>
        <span class="s2">&quot;evaluation&quot;</span><span class="p">:</span> <span class="n">evaluation_methods</span><span class="p">,</span>
        <span class="s2">&quot;mitigation&quot;</span><span class="p">:</span> <span class="n">mitigation_strategies</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Key challenges with bias include:</p>
<ol class="arabic simple">
<li><p><strong>Sources of Bias</strong>:</p>
<ul class="simple">
<li><p>Training data: Reflects historical and societal inequalities</p></li>
<li><p>Algorithm design: Architecture and objective functions may amplify certain patterns</p></li>
<li><p>Deployment context: How models are used affects fairness implications</p></li>
</ul>
</li>
<li><p><strong>Types of Harm</strong>:</p>
<ul class="simple">
<li><p>Representational harm: Reinforcing stereotypes or negative associations</p></li>
<li><p>Allocational harm: Causing unfair distribution of resources or opportunities</p></li>
<li><p>Quality-of-service disparities: Providing different quality outputs for different groups</p></li>
</ul>
</li>
<li><p><strong>Mitigation Complexities</strong>:</p>
<ul class="simple">
<li><p>Value pluralism: Different communities have different fairness priorities</p></li>
<li><p>Contextual appropriateness: Some distinctions are appropriate in certain contexts</p></li>
<li><p>Trade-offs: Addressing one type of bias may worsen another</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: Humans also exhibit cognitive biases, but social norms, education, and cultural evolution create mechanisms for recognizing and addressing these biases over time. LLMs lack this social feedback loop unless explicitly designed.</p>
</section>
<section id="context-window-limitations">
<h3>12.5.3 Context Window Limitations<a class="headerlink" href="#context-window-limitations" title="Link to this heading">#</a></h3>
<p>The limited context window of LLMs constrains their ability to process and reason over long documents:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">context_window_limitations</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate context window limitations and strategies.&quot;&quot;&quot;</span>
    <span class="c1"># Context window sizes for common models</span>
    <span class="n">model_context_windows</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;GPT-3.5 (Jun 2023)&quot;</span><span class="p">:</span> <span class="s2">&quot;4K tokens (~3,000 words)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;GPT-4 (Mar 2023)&quot;</span><span class="p">:</span> <span class="s2">&quot;8K tokens (~6,000 words)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;GPT-4 Turbo (Dec 2023)&quot;</span><span class="p">:</span> <span class="s2">&quot;128K tokens (~96,000 words)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Claude 2&quot;</span><span class="p">:</span> <span class="s2">&quot;100K tokens (~75,000 words)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;LLaMA 2&quot;</span><span class="p">:</span> <span class="s2">&quot;4K tokens (~3,000 words)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Gemini Ultra&quot;</span><span class="p">:</span> <span class="s2">&quot;32K tokens (~24,000 words)&quot;</span>
    <span class="p">}</span>
    
    <span class="c1"># Challenges related to context windows</span>
    <span class="n">context_challenges</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;challenge&quot;</span><span class="p">:</span> <span class="s2">&quot;Information retrieval&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Finding specific information in long documents&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;Locating a particular clause in a lengthy legal contract&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;challenge&quot;</span><span class="p">:</span> <span class="s2">&quot;Cross-reference reasoning&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Connecting information from different parts of a text&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;Identifying inconsistencies between sections of a research paper&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;challenge&quot;</span><span class="p">:</span> <span class="s2">&quot;Document summarization&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Creating concise overviews of lengthy documents&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;Summarizing a 300-page book into 2 pages&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;challenge&quot;</span><span class="p">:</span> <span class="s2">&quot;Sequential decision making&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Maintaining consistent reasoning across a long conversation&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;Multi-turn dialogue about a complex topic spanning hours&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># Strategies for handling long contexts</span>
    <span class="n">context_strategies</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Chunking and sliding windows&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Break documents into overlapping segments&quot;</span><span class="p">,</span>
            <span class="s2">&quot;implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;Process each chunk separately and combine results&quot;</span><span class="p">,</span>
            <span class="s2">&quot;limitation&quot;</span><span class="p">:</span> <span class="s2">&quot;May lose cross-chunk connections and global context&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Hierarchical summarization&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Summarize sections, then summarize the summaries&quot;</span><span class="p">,</span>
            <span class="s2">&quot;implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;Create multiple levels of abstraction&quot;</span><span class="p">,</span>
            <span class="s2">&quot;limitation&quot;</span><span class="p">:</span> <span class="s2">&quot;Information loss at each summarization step&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Retrieval-based approaches&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Store document chunks in a vector database and retrieve relevant portions&quot;</span><span class="p">,</span>
            <span class="s2">&quot;implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;Use embeddings to find semantically relevant chunks for each query&quot;</span><span class="p">,</span>
            <span class="s2">&quot;limitation&quot;</span><span class="p">:</span> <span class="s2">&quot;Retrieval quality depends on query formulation and embedding quality&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Information distillation&quot;</span><span class="p">,</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Extract and retain only the most important information&quot;</span><span class="p">,</span>
            <span class="s2">&quot;implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;Use models to identify key facts and discard irrelevant details&quot;</span><span class="p">,</span>
            <span class="s2">&quot;limitation&quot;</span><span class="p">:</span> <span class="s2">&quot;Requires determining importance, which depends on downstream tasks&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># Architectural innovations addressing context length</span>
    <span class="n">architectural_innovations</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;innovation&quot;</span><span class="p">:</span> <span class="s2">&quot;Sparse attention patterns&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Use structured sparsity to avoid quadratic scaling&quot;</span><span class="p">,</span>
            <span class="s2">&quot;examples&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Longformer&quot;</span><span class="p">,</span> <span class="s2">&quot;BigBird&quot;</span><span class="p">,</span> <span class="s2">&quot;Reformer&quot;</span><span class="p">]</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;innovation&quot;</span><span class="p">:</span> <span class="s2">&quot;Recurrent memory mechanisms&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Maintain compressed representations of previous context&quot;</span><span class="p">,</span>
            <span class="s2">&quot;examples&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Transformer-XL&quot;</span><span class="p">,</span> <span class="s2">&quot;Memorizing Transformers&quot;</span><span class="p">,</span> <span class="s2">&quot;Retentive Network&quot;</span><span class="p">]</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;innovation&quot;</span><span class="p">:</span> <span class="s2">&quot;Hierarchical encodings&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Process text at multiple levels of granularity&quot;</span><span class="p">,</span>
            <span class="s2">&quot;examples&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Hierarchical Transformers&quot;</span><span class="p">,</span> <span class="s2">&quot;Primer&quot;</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;model_windows&quot;</span><span class="p">:</span> <span class="n">model_context_windows</span><span class="p">,</span>
        <span class="s2">&quot;challenges&quot;</span><span class="p">:</span> <span class="n">context_challenges</span><span class="p">,</span>
        <span class="s2">&quot;strategies&quot;</span><span class="p">:</span> <span class="n">context_strategies</span><span class="p">,</span>
        <span class="s2">&quot;innovations&quot;</span><span class="p">:</span> <span class="n">architectural_innovations</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Key challenges with context windows include:</p>
<ol class="arabic simple">
<li><p><strong>Computational Constraints</strong>:</p>
<ul class="simple">
<li><p>Attention mechanism scales quadratically with sequence length</p></li>
<li><p>Memory requirements increase with context size</p></li>
<li><p>Training difficulty increases with longer sequences</p></li>
</ul>
</li>
<li><p><strong>Cognitive Limitations</strong>:</p>
<ul class="simple">
<li><p>Information retrieval challenges in long contexts</p></li>
<li><p>Maintaining coherence across distant parts of text</p></li>
<li><p>Balancing detail and high-level understanding</p></li>
</ul>
</li>
<li><p><strong>Practical Implications</strong>:</p>
<ul class="simple">
<li><p>Limits use cases requiring whole-document understanding</p></li>
<li><p>Necessitates external memory and retrieval systems</p></li>
<li><p>Creates trade-offs between detail and breadth</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: Humans also have limited working memory but compensate through hierarchical processing, external memory aids, and contextual retrieval. The brain organizes information across multiple timescales, from immediate to episodic memory.</p>
</section>
<section id="reasoning-capabilities">
<h3>12.5.4 Reasoning Capabilities<a class="headerlink" href="#reasoning-capabilities" title="Link to this heading">#</a></h3>
<p>LLMs show both impressive and limited reasoning abilities:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reasoning_capabilities</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Explore reasoning capabilities and limitations in LLMs.&quot;&quot;&quot;</span>
    <span class="c1"># Examples of reasoning successes</span>
    <span class="n">reasoning_successes</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;Logical deduction&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;If all A are B, and all B are C, then all A are C.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;performance&quot;</span><span class="p">:</span> <span class="s2">&quot;LLMs can follow simple syllogistic reasoning consistently.&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;Step-by-step math&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;What is 17 × 24?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;performance&quot;</span><span class="p">:</span> <span class="s2">&quot;With chain-of-thought prompting, can solve by breaking into steps.&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;Commonsense reasoning&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;If I put a book on a shelf and leave the room, where is the book?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;performance&quot;</span><span class="p">:</span> <span class="s2">&quot;Understands object permanence and basic physical causality.&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># Examples of reasoning failures</span>
    <span class="n">reasoning_failures</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;Complex logical puzzles&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;Knights and Knaves puzzles (determining who is telling truth)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Inconsistent tracking of logical constraints across many steps&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;Mathematical proofs&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;Prove the Pythagorean theorem&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;May introduce errors or circular reasoning in multi-step proofs&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;Compositional generalization&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;Applying known rules to novel combinations&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Struggles to systematically apply rules to unfamiliar structures&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="s2">&quot;Planning with constraints&quot;</span><span class="p">,</span>
            <span class="s2">&quot;example&quot;</span><span class="p">:</span> <span class="s2">&quot;Traveling salesman problem with complex constraints&quot;</span><span class="p">,</span>
            <span class="s2">&quot;issue&quot;</span><span class="p">:</span> <span class="s2">&quot;Difficulty tracking multiple interdependent constraints&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># Strategies to improve reasoning</span>
    <span class="n">reasoning_strategies</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Chain of thought prompting&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Encourage step-by-step reasoning&quot;</span><span class="p">,</span>
            <span class="s2">&quot;effectiveness&quot;</span><span class="p">:</span> <span class="s2">&quot;Significantly improves multi-step reasoning and math&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Tree of thoughts&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Explore multiple reasoning paths and select best outcome&quot;</span><span class="p">,</span>
            <span class="s2">&quot;effectiveness&quot;</span><span class="p">:</span> <span class="s2">&quot;Helps with problems requiring search or backtracking&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Self-critique and verification&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Have model evaluate and correct its own reasoning&quot;</span><span class="p">,</span>
            <span class="s2">&quot;effectiveness&quot;</span><span class="p">:</span> <span class="s2">&quot;Can catch some errors, but may miss systematic flaws&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;strategy&quot;</span><span class="p">:</span> <span class="s2">&quot;Tool use&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Augment model with external tools (calculators, code interpreters)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;effectiveness&quot;</span><span class="p">:</span> <span class="s2">&quot;Dramatically improves accuracy for formal reasoning&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;successes&quot;</span><span class="p">:</span> <span class="n">reasoning_successes</span><span class="p">,</span>
        <span class="s2">&quot;failures&quot;</span><span class="p">:</span> <span class="n">reasoning_failures</span><span class="p">,</span>
        <span class="s2">&quot;strategies&quot;</span><span class="p">:</span> <span class="n">reasoning_strategies</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Key aspects of reasoning limitations include:</p>
<ol class="arabic simple">
<li><p><strong>Types of Reasoning Challenges</strong>:</p>
<ul class="simple">
<li><p>Systematic reasoning: Applying rules consistently</p></li>
<li><p>Complex problem-solving: Planning, search, and constraint satisfaction</p></li>
<li><p>Abstraction and generalization: Applying known patterns to new domains</p></li>
<li><p>Self-monitoring: Detecting and correcting errors in reasoning</p></li>
</ul>
</li>
<li><p><strong>Underlying Mechanisms</strong>:</p>
<ul class="simple">
<li><p>Statistical pattern matching vs. rule-based reasoning</p></li>
<li><p>Emergent reasoning capabilities from pattern recognition</p></li>
<li><p>Limited by training objectives focused on prediction</p></li>
</ul>
</li>
<li><p><strong>Implications</strong>:</p>
<ul class="simple">
<li><p>Need for external verification for critical applications</p></li>
<li><p>Potential for augmentation with symbolic systems</p></li>
<li><p>Importance of appropriate task delegation and human oversight</p></li>
</ul>
</li>
</ol>
<p><strong>Biological Parallel</strong>: Human reasoning combines pattern recognition with explicit symbolic manipulation, especially for formal domains like mathematics and logic. The prefrontal cortex plays a key role in abstract reasoning, working with other brain regions to integrate information and monitor errors.</p>
</section>
</section>
<section id="code-lab">
<h2>12.6 Code Lab<a class="headerlink" href="#code-lab" title="Link to this heading">#</a></h2>
<p>In this code lab, we’ll apply the techniques covered in this chapter through hands-on exercises. We’ll implement parameter-efficient fine-tuning, explore prompting strategies, and evaluate model outputs.</p>
<section id="fine-tuning-a-small-llm-with-lora">
<h3>12.6.1 Fine-tuning a Small LLM with LoRA<a class="headerlink" href="#fine-tuning-a-small-llm-with-lora" title="Link to this heading">#</a></h3>
<p>First, let’s implement LoRA fine-tuning on a small language model for a specialized task. We’ll use a pretrained GPT-2 model and adapt it to a specific domain using the PEFT library.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TrainingArguments</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">TaskType</span><span class="p">,</span> <span class="n">PeftConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

<span class="k">def</span><span class="w"> </span><span class="nf">finetune_with_lora</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fine-tune a small LLM using LoRA for parameter-efficient adaptation.&quot;&quot;&quot;</span>
    <span class="c1"># 1. Load base model and tokenizer</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt2&quot;</span>  <span class="c1"># 124M parameter model for demonstration</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    
    <span class="c1"># 2. Define LoRA configuration</span>
    <span class="n">lora_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
        <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>                     <span class="c1"># Rank dimension</span>
        <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>           <span class="c1"># Alpha parameter for LoRA scaling</span>
        <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;c_attn&quot;</span><span class="p">,</span> <span class="s2">&quot;c_proj&quot;</span><span class="p">],</span>  <span class="c1"># Attention layers to adapt</span>
        <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>       <span class="c1"># Dropout probability for LoRA layers</span>
        <span class="n">bias</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>             <span class="c1"># Don&#39;t adapt bias terms</span>
        <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">CAUSAL_LM</span>  <span class="c1"># Task type (for PEFT library)</span>
    <span class="p">)</span>
    
    <span class="c1"># 3. Create PEFT model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">lora_config</span><span class="p">)</span>
    
    <span class="c1"># 4. Compare parameter counts</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="n">trainable_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Trainable parameters: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Percentage of parameters trained: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">trainable_params</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">total_params</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    
    <span class="c1"># 5. Load and prepare dataset (scientific abstracts as an example)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;scientific_papers&quot;</span><span class="p">,</span> <span class="s2">&quot;arxiv&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train[:1000]&quot;</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tokenize texts with appropriate input format.&quot;&quot;&quot;</span>
        <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">abstract</span><span class="p">[:</span><span class="mi">1024</span><span class="p">]</span> <span class="k">for</span> <span class="n">abstract</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;abstract&quot;</span><span class="p">]]</span>  <span class="c1"># Truncate to manageable size</span>
        <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
    
    <span class="n">tokenized_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">preprocess_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># 6. Setup training arguments</span>
    <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./lora-scientific-gpt2&quot;</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Use mixed precision for efficiency</span>
        <span class="n">report_to</span><span class="o">=</span><span class="s2">&quot;none&quot;</span>  <span class="c1"># Disable wandb, etc.</span>
    <span class="p">)</span>
    
    <span class="c1"># 7. Initialize Trainer (import Trainer separately to avoid confusion with custom functions)</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Trainer</span>
    
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_dataset</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="c1"># 8. Train (commented out for demonstration)</span>
    <span class="c1"># trainer.train()</span>
    
    <span class="c1"># 9. Save adapter weights (only the LoRA parameters)</span>
    <span class="c1"># trainer.model.save_pretrained(&quot;./lora-scientific-gpt2-adapter&quot;)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
        <span class="s2">&quot;adapter_type&quot;</span><span class="p">:</span> <span class="s2">&quot;LoRA&quot;</span><span class="p">,</span>
        <span class="s2">&quot;total_params&quot;</span><span class="p">:</span> <span class="n">total_params</span><span class="p">,</span>
        <span class="s2">&quot;trainable_params&quot;</span><span class="p">:</span> <span class="n">trainable_params</span><span class="p">,</span>
        <span class="s2">&quot;efficiency&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">trainable_params</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">total_params</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">%&quot;</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>The above code implements LoRA fine-tuning, enabling us to adapt a pretrained model to scientific text with only about 0.1% of the parameters being updated. This makes fine-tuning practical even on consumer hardware.</p>
</section>
<section id="implementing-prompting-strategies">
<h3>12.6.2 Implementing Prompting Strategies<a class="headerlink" href="#implementing-prompting-strategies" title="Link to this heading">#</a></h3>
<p>Next, let’s explore various prompting techniques and compare their effectiveness on reasoning tasks:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compare_prompting_strategies</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare different prompting strategies on a problem-solving task.&quot;&quot;&quot;</span>
    <span class="c1"># Load model (using a small model for demonstration)</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt2-medium&quot;</span>  <span class="c1"># 355M parameter model</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    
    <span class="c1"># Define prompting techniques to compare</span>
    <span class="n">prompting_techniques</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Direct&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            Question: A garden has roses and tulips. There are 12 flowers in total. </span>
<span class="s2">            If there are 4 more roses than tulips, how many roses are there?</span>
<span class="s2">            Answer:</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="p">,</span>
        
        <span class="s2">&quot;Few-shot&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            Question: A classroom has boys and girls. There are 20 students in total.</span>
<span class="s2">            If there are 6 more girls than boys, how many boys are there?</span>
<span class="s2">            </span>
<span class="s2">            To solve this problem, I&#39;ll use variables:</span>
<span class="s2">            Let b = number of boys</span>
<span class="s2">            Let g = number of girls</span>
<span class="s2">            </span>
<span class="s2">            I know that:</span>
<span class="s2">            b + g = 20 (total students)</span>
<span class="s2">            g = b + 6 (6 more girls than boys)</span>
<span class="s2">            </span>
<span class="s2">            Substituting the second equation into the first:</span>
<span class="s2">            b + (b + 6) = 20</span>
<span class="s2">            2b + 6 = 20</span>
<span class="s2">            2b = 14</span>
<span class="s2">            b = 7</span>
<span class="s2">            </span>
<span class="s2">            Therefore, there are 7 boys in the classroom.</span>
<span class="s2">            </span>
<span class="s2">            Question: A garden has roses and tulips. There are 12 flowers in total. </span>
<span class="s2">            If there are 4 more roses than tulips, how many roses are there?</span>
<span class="s2">            Answer:</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="p">,</span>
        
        <span class="s2">&quot;Zero-shot CoT&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            Question: A garden has roses and tulips. There are 12 flowers in total. </span>
<span class="s2">            If there are 4 more roses than tulips, how many roses are there?</span>
<span class="s2">            </span>
<span class="s2">            Let&#39;s think step by step to solve this problem.</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="p">,</span>
        
        <span class="s2">&quot;Self-Critique&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            Question: A garden has roses and tulips. There are 12 flowers in total. </span>
<span class="s2">            If there are 4 more roses than tulips, how many roses are there?</span>
<span class="s2">            </span>
<span class="s2">            I&#39;ll solve this and then double-check my work for errors.</span>
<span class="s2">        &quot;&quot;&quot;</span>
    <span class="p">}</span>
    
    <span class="c1"># Function to generate completions</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate_completion</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">150</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span> 
                <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
            <span class="p">)</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">completion</span>
    
    <span class="c1"># Generate answers for each technique and assess them</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="k">for</span> <span class="n">technique</span><span class="p">,</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompting_techniques</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># In practice, you&#39;d generate actual completions from the model here</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="n">generate_completion</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        
        <span class="c1"># Simulated assessments (in real use, you would evaluate actual model outputs)</span>
        <span class="c1"># This would require a human evaluator or more sophisticated evaluation</span>
        <span class="n">simulate_accuracy</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;Direct&quot;</span><span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span>  <span class="c1"># Often struggles with word problems</span>
            <span class="s2">&quot;Few-shot&quot;</span><span class="p">:</span> <span class="mf">0.70</span><span class="p">,</span>  <span class="c1"># Benefits from examples</span>
            <span class="s2">&quot;Zero-shot CoT&quot;</span><span class="p">:</span> <span class="mf">0.65</span><span class="p">,</span>  <span class="c1"># Reasoning steps help</span>
            <span class="s2">&quot;Self-Critique&quot;</span><span class="p">:</span> <span class="mf">0.60</span><span class="p">,</span>  <span class="c1"># Some benefit from verification</span>
        <span class="p">}</span>
        
        <span class="n">results</span><span class="p">[</span><span class="n">technique</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
            <span class="s2">&quot;completion&quot;</span><span class="p">:</span> <span class="n">completion</span><span class="p">,</span>
            <span class="s2">&quot;tokens_used&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">)),</span>
            <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="n">simulate_accuracy</span><span class="p">[</span><span class="n">technique</span><span class="p">]</span>
        <span class="p">}</span>
    
    <span class="c1"># Visualization (in a real notebook, this would generate a plot)</span>
    <span class="n">techniques</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[</span><span class="n">results</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">techniques</span><span class="p">]</span>
    <span class="n">token_counts</span> <span class="o">=</span> <span class="p">[</span><span class="n">results</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="s2">&quot;tokens_used&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">techniques</span><span class="p">]</span>
    
    <span class="c1"># Create plot data</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">techniques</span><span class="p">))</span>
    <span class="n">width</span> <span class="o">=</span> <span class="mf">0.35</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
    
    <span class="n">bars1</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
    <span class="n">bars2</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">token_counts</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Tokens Used&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Prompting Technique&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Tokens Used&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Comparing Prompting Techniques: Accuracy vs. Token Usage&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">techniques</span><span class="p">)</span>
    
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
    
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;results&quot;</span><span class="p">:</span> <span class="n">results</span><span class="p">,</span>
        <span class="s2">&quot;best_technique&quot;</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])[</span><span class="mi">0</span><span class="p">],</span>
        <span class="s2">&quot;token_efficiency&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;tokens_used&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]))[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>This exercise demonstrates how different prompting techniques can significantly impact model performance. While few-shot and chain-of-thought prompting use more tokens, they typically yield better results for reasoning tasks.</p>
</section>
<section id="implementing-a-rag-system-to-reduce-hallucinations">
<h3>12.6.3 Implementing a RAG System to Reduce Hallucinations<a class="headerlink" href="#implementing-a-rag-system-to-reduce-hallucinations" title="Link to this heading">#</a></h3>
<p>Now, let’s build a simple Retrieval-Augmented Generation (RAG) system to improve factual accuracy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics.pairwise</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">implement_simple_rag</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implement a basic RAG system to improve factual grounding.&quot;&quot;&quot;</span>
    
    <span class="c1"># 1. Create a small knowledge base (in practice, this would be much larger)</span>
    <span class="n">knowledge_base</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;The Eiffel Tower is located in Paris, France. It was completed in 1889 and stands 330 meters tall.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The Great Wall of China is approximately 21,196 kilometers long. Construction began in the 7th century BCE.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Jupiter is the largest planet in our solar system. It has 79 known moons, including the four large Galilean moons.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Machine learning is a subset of artificial intelligence that enables systems to learn from data rather than explicit programming.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The human brain contains approximately 86 billion neurons connected by trillions of synapses.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Photosynthesis is the process by which plants convert light energy into chemical energy. It produces oxygen as a byproduct.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;DNA (deoxyribonucleic acid) stores genetic information in the form of nucleotide sequences.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Neural networks are computing systems inspired by biological neural networks, forming the basis of many modern AI systems.&quot;</span>
    <span class="p">]</span>
    
    <span class="c1"># 2. Define a function to create embeddings for text</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">create_embeddings</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create embeddings for a list of texts using a pretrained model.&quot;&quot;&quot;</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        
        <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="c1"># Tokenize and prepare for the model</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            
            <span class="c1"># Generate embeddings</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
                
            <span class="c1"># Use mean pooling to get a single vector per text</span>
            <span class="n">embedding</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    
    <span class="c1"># 3. Create embeddings for the knowledge base</span>
    <span class="n">kb_embeddings</span> <span class="o">=</span> <span class="n">create_embeddings</span><span class="p">(</span><span class="n">knowledge_base</span><span class="p">)</span>
    
    <span class="c1"># 4. Function to retrieve relevant information for a query</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">retrieve_relevant_info</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Retrieve top-k relevant passages for a query.&quot;&quot;&quot;</span>
        <span class="c1"># Create embedding for the query</span>
        <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">create_embeddings</span><span class="p">([</span><span class="n">query</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Calculate similarity scores</span>
        <span class="n">similarities</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">([</span><span class="n">query_embedding</span><span class="p">],</span> <span class="n">kb_embeddings</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Get indices of top-k most similar passages</span>
        <span class="n">top_indices</span> <span class="o">=</span> <span class="n">similarities</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="o">-</span><span class="n">top_k</span><span class="p">:][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Return the relevant passages and their similarity scores</span>
        <span class="n">retrieved_info</span> <span class="o">=</span> <span class="p">[</span><span class="n">knowledge_base</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_indices</span><span class="p">]</span>
        <span class="n">retrieved_scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">similarities</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_indices</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">retrieved_info</span><span class="p">,</span> <span class="n">retrieved_scores</span>
    
    <span class="c1"># 5. Function to generate RAG-enhanced responses</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate_rag_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2-medium&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate a response using retrieval-augmented generation.&quot;&quot;&quot;</span>
        <span class="c1"># Retrieve relevant information</span>
        <span class="n">retrieved_info</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">retrieve_relevant_info</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        
        <span class="c1"># Create a RAG prompt with the retrieved information</span>
        <span class="n">rag_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        Based on the following information:</span>
<span class="s2">        </span>
<span class="s2">        </span><span class="si">{</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">retrieved_info</span><span class="p">)</span><span class="si">}</span>
<span class="s2">        </span>
<span class="s2">        Please answer this question: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span>
<span class="s2">        &quot;&quot;&quot;</span>
        
        <span class="c1"># In a real implementation, this would call the model to generate a response</span>
        <span class="c1"># Here we&#39;ll simulate it</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
        
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">rag_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
            <span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span>
            <span class="s2">&quot;retrieved_info&quot;</span><span class="p">:</span> <span class="n">retrieved_info</span><span class="p">,</span>
            <span class="s2">&quot;similarity_scores&quot;</span><span class="p">:</span> <span class="n">scores</span><span class="p">,</span>
            <span class="s2">&quot;rag_prompt&quot;</span><span class="p">:</span> <span class="n">rag_prompt</span><span class="p">,</span>
            <span class="s2">&quot;response&quot;</span><span class="p">:</span> <span class="n">response</span>
        <span class="p">}</span>
    
    <span class="c1"># 6. Compare RAG vs. direct responses for queries</span>
    <span class="n">test_queries</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;How tall is the Eiffel Tower?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;What is neural network in AI?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;How many neurons are in the human brain?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;What is the mechanism behind photosynthesis?&quot;</span>
    <span class="p">]</span>
    
    <span class="c1"># In practice, you would generate and compare actual responses here</span>
    <span class="n">comparison_results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">test_queries</span><span class="p">:</span>
        <span class="n">rag_result</span> <span class="o">=</span> <span class="n">generate_rag_response</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">comparison_results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span>
            <span class="s2">&quot;with_rag&quot;</span><span class="p">:</span> <span class="n">rag_result</span><span class="p">[</span><span class="s2">&quot;response&quot;</span><span class="p">],</span>
            <span class="c1"># Simulate direct response (without retrieval)</span>
            <span class="s2">&quot;without_rag&quot;</span><span class="p">:</span> <span class="s2">&quot;Simulated direct response without knowledge retrieval&quot;</span><span class="p">,</span>
            <span class="s2">&quot;retrieved_documents&quot;</span><span class="p">:</span> <span class="n">rag_result</span><span class="p">[</span><span class="s2">&quot;retrieved_info&quot;</span><span class="p">]</span>
        <span class="p">})</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;knowledge_base_size&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">knowledge_base</span><span class="p">),</span>
        <span class="s2">&quot;comparison_results&quot;</span><span class="p">:</span> <span class="n">comparison_results</span><span class="p">,</span>
        <span class="s2">&quot;rag_benefits&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;Reduces hallucinations by grounding responses in factual information&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Provides source material that can be cited&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Improves answer specificity and detail&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Allows model to access information beyond training data&quot;</span>
        <span class="p">]</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>This RAG system demonstrates how to retrieve relevant information before generating responses, significantly reducing hallucinations and improving factual accuracy.</p>
</section>
<section id="domain-adaptation-case-study-personalizing-a-scientific-assistant">
<h3>12.6.4 Domain Adaptation Case Study: Personalizing a Scientific Assistant<a class="headerlink" href="#domain-adaptation-case-study-personalizing-a-scientific-assistant" title="Link to this heading">#</a></h3>
<p>Finally, let’s explore how to adapt an LLM to a specialized scientific domain, combining the techniques we’ve learned:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">neuroscience_domain_adaptation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Case study on adapting an LLM for specialized neuroscience applications.&quot;&quot;&quot;</span>
    
    <span class="c1"># 1. Define domain-specific knowledge and terminology</span>
    <span class="n">neuroscience_terminology</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Domains&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;neuroanatomy&quot;</span><span class="p">,</span> <span class="s2">&quot;neurophysiology&quot;</span><span class="p">,</span> <span class="s2">&quot;cognitive neuroscience&quot;</span><span class="p">,</span> <span class="s2">&quot;computational neuroscience&quot;</span><span class="p">],</span>
        <span class="s2">&quot;Key Concepts&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;Action potential&quot;</span><span class="p">,</span> <span class="s2">&quot;Synapse&quot;</span><span class="p">,</span> <span class="s2">&quot;Neurotransmitter&quot;</span><span class="p">,</span> <span class="s2">&quot;Neural circuit&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Plasticity&quot;</span><span class="p">,</span> <span class="s2">&quot;Long-term potentiation&quot;</span><span class="p">,</span> <span class="s2">&quot;Cortical column&quot;</span><span class="p">,</span> <span class="s2">&quot;Receptive field&quot;</span>
        <span class="p">],</span>
        <span class="s2">&quot;Research Methods&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;fMRI&quot;</span><span class="p">,</span> <span class="s2">&quot;EEG&quot;</span><span class="p">,</span> <span class="s2">&quot;Single-unit recording&quot;</span><span class="p">,</span> <span class="s2">&quot;Optogenetics&quot;</span><span class="p">,</span> 
            <span class="s2">&quot;Patch clamp&quot;</span><span class="p">,</span> <span class="s2">&quot;Calcium imaging&quot;</span><span class="p">,</span> <span class="s2">&quot;Neural decoding&quot;</span>
        <span class="p">]</span>
    <span class="p">}</span>
    
    <span class="c1"># 2. Create a domain-specific system prompt</span>
    <span class="n">neuroscience_system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    You are NeuroBuddy, an expert neuroscience research assistant with the following capabilities:</span>
<span class="s2">    </span>
<span class="s2">    1. You have specialized knowledge in neuroanatomy, neurophysiology, cognitive neuroscience,</span>
<span class="s2">       and computational neuroscience.</span>
<span class="s2">       </span>
<span class="s2">    2. You can interpret and explain neuroscience research methods including fMRI, EEG, </span>
<span class="s2">       single-unit recordings, optogenetics, and computational modeling approaches.</span>
<span class="s2">       </span>
<span class="s2">    3. You provide accurate, technically precise information about the brain and </span>
<span class="s2">       neural systems, using appropriate terminology and citing relevant research where possible.</span>
<span class="s2">       </span>
<span class="s2">    4. When asked for experimental design advice, you suggest appropriate methods, controls,</span>
<span class="s2">       and analyses based on the specific research question.</span>
<span class="s2">       </span>
<span class="s2">    5. You can translate between neuroscience concepts and machine learning/AI paradigms,</span>
<span class="s2">       highlighting similarities and differences.</span>
<span class="s2">       </span>
<span class="s2">    6. When uncertain, you express your limitations clearly rather than providing </span>
<span class="s2">       potentially incorrect information.</span>
<span class="s2">       </span>
<span class="s2">    Respond with comprehensive, scientifically accurate information while making complex</span>
<span class="s2">    concepts understandable. Use technical terminology where appropriate but explain</span>
<span class="s2">    specialized terms when they might be unfamiliar.</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="c1"># 3. Design few-shot examples for domain grounding</span>
    <span class="n">few_shot_examples</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the difference between place cells and grid cells?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;response&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            Place cells and grid cells are two types of spatial encoding neurons with distinct properties:</span>
<span class="s2">            </span>
<span class="s2">            Place cells:</span>
<span class="s2">            - Found primarily in the hippocampus, especially CA1 and CA3 regions</span>
<span class="s2">            - Fire when an animal is in a specific location in the environment (&quot;place field&quot;)</span>
<span class="s2">            - Each cell responds to a single location, forming a cognitive map</span>
<span class="s2">            - Properties change with environment (remapping)</span>
<span class="s2">            - Discovered by O&#39;Keefe and Dostrovsky in 1971</span>
<span class="s2">            </span>
<span class="s2">            Grid cells:</span>
<span class="s2">            - Found in the medial entorhinal cortex (MEC)</span>
<span class="s2">            - Fire in multiple locations forming a hexagonal grid pattern</span>
<span class="s2">            - The same cell activates at regular spatial intervals</span>
<span class="s2">            - Patterns preserve across environments but may rescale</span>
<span class="s2">            - Discovered by the Moser lab in 2005</span>
<span class="s2">            </span>
<span class="s2">            Together, these cell types form a comprehensive spatial navigation system, with grid cells providing a coordinate system and place cells representing specific locations. This discovery was recognized with the 2014 Nobel Prize in Physiology or Medicine.</span>
<span class="s2">            &quot;&quot;&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;How does optogenetics work?&quot;</span><span class="p">,</span>
            <span class="s2">&quot;response&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            Optogenetics is a technique that combines genetic and optical methods to control and monitor specific cells (typically neurons) with light. Here&#39;s how it works:</span>
<span class="s2">            </span>
<span class="s2">            1. Genetic component:</span>
<span class="s2">               - Light-sensitive proteins (opsins) like Channelrhodopsin-2 (ChR2), Halorhodopsin (NpHR), or Archaerhodopsin (Arch) are introduced into target cells</span>
<span class="s2">               - This is typically done using viral vectors carrying genes for these opsins under cell-type specific promoters</span>
<span class="s2">               - ChR2 activates neurons when exposed to blue light, while NpHR and Arch inhibit neurons with yellow/green light</span>
<span class="s2">            </span>
<span class="s2">            2. Light delivery:</span>
<span class="s2">               - Fiber optic cables or μLED devices are implanted to deliver light to specific brain regions</span>
<span class="s2">               - Light pulses can be precisely timed at millisecond precision</span>
<span class="s2">               - Different wavelengths can activate different opsins</span>
<span class="s2">            </span>
<span class="s2">            3. Experimental applications:</span>
<span class="s2">               - Causal testing of neural circuits (does activating these neurons cause this behavior?)</span>
<span class="s2">               - Mapping connections between brain regions</span>
<span class="s2">               - Therapeutic research (potentially treating conditions by controlling specific circuits)</span>
<span class="s2">            </span>
<span class="s2">            This technique revolutionized neuroscience by allowing precise control of specific neuronal populations with temporal precision matching natural neural dynamics.</span>
<span class="s2">            &quot;&quot;&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># 4. Demonstrate domain-specific fine-tuning data creation</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">create_instruction_tuning_dataset</span><span class="p">():</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create examples for instruction fine-tuning a neuroscience assistant.&quot;&quot;&quot;</span>
        <span class="n">instruction_data</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;instruction&quot;</span><span class="p">:</span> <span class="s2">&quot;Explain the concept of predictive coding in the brain.&quot;</span><span class="p">,</span>
                <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
                <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                Predictive coding is a theoretical framework in neuroscience that proposes the brain continually generates predictions about incoming sensory information, comparing these predictions with actual input to compute prediction errors.</span>
<span class="s2">                </span>
<span class="s2">                Key principles:</span>
<span class="s2">                1. Hierarchical processing: Higher levels in neural hierarchies generate predictions about representations in lower levels</span>
<span class="s2">                2. Bidirectional information flow: Top-down predictions and bottom-up prediction errors</span>
<span class="s2">                3. Precision weighting: The influence of prediction errors is modulated by their reliability</span>
<span class="s2">                </span>
<span class="s2">                This model helps explain phenomena like perception, attention, and learning:</span>
<span class="s2">                - Perception emerges from prediction minimization</span>
<span class="s2">                - Attention is understood as precision optimization</span>
<span class="s2">                - Learning occurs by updating predictive models to minimize future prediction errors</span>
<span class="s2">                </span>
<span class="s2">                Neuroanatomically, predictive coding may be implemented through:</span>
<span class="s2">                - Forward connections carrying prediction errors (typically superficial pyramidal cells)</span>
<span class="s2">                - Backward connections carrying predictions (typically deep pyramidal cells)</span>
<span class="s2">                - Local connections implementing precision-weighting (potentially involving inhibitory interneurons)</span>
<span class="s2">                </span>
<span class="s2">                Influential papers include Karl Friston&#39;s work on the free energy principle and Rao &amp; Ballard&#39;s 1999 paper on predictive coding in the visual cortex.</span>
<span class="s2">                &quot;&quot;&quot;</span>
            <span class="p">},</span>
            <span class="p">{</span>
                <span class="s2">&quot;instruction&quot;</span><span class="p">:</span> <span class="s2">&quot;Compare and contrast LSTMs and biological recurrent neural circuits.&quot;</span><span class="p">,</span>
                <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
                <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                Comparing LSTMs (Long Short-Term Memory networks) and biological recurrent neural circuits:</span>
<span class="s2">                </span>
<span class="s2">                Similarities:</span>
<span class="s2">                1. Recurrent connections allowing information persistence over time</span>
<span class="s2">                2. Selective memory maintenance and forgetting mechanisms</span>
<span class="s2">                3. Gating mechanisms that control information flow</span>
<span class="s2">                4. Ability to learn temporal patterns and sequences</span>
<span class="s2">                </span>
<span class="s2">                Key differences:</span>
<span class="s2">                </span>
<span class="s2">                | Feature | LSTMs | Biological Circuits |</span>
<span class="s2">                |---------|-------|---------------------|</span>
<span class="s2">                | Learning mechanism | Backpropagation through time | Spike-timing dependent plasticity, neuromodulation |</span>
<span class="s2">                | Temporal dynamics | Discrete time steps | Continuous-time dynamics with varying timescales |</span>
<span class="s2">                | Memory mechanisms | Explicit cell state | Multiple mechanisms: synaptic, intrinsic, network |</span>
<span class="s2">                | Unit complexity | Complex LSTM cells with gates | Diverse neuron types with different properties |</span>
<span class="s2">                | Connectivity | Typically fully connected layers | Sparse, structured connectivity |</span>
<span class="s2">                | Computation | Floating-point operations | Spike-based computation with stochastic elements |</span>
<span class="s2">                | Energy efficiency | High computational cost | Remarkably energy efficient |</span>
<span class="s2">                </span>
<span class="s2">                While LSTMs were inspired by biological memory, they primarily serve as functional analogues rather than biologically realistic models. The hippocampal-cortical memory system, prefrontal working memory circuits, and thalamocortical loops represent biological systems with recurrent memory functions that operate on different principles than LSTMs.</span>
<span class="s2">                &quot;&quot;&quot;</span>
            <span class="p">}</span>
        <span class="p">]</span>
        
        <span class="c1"># Format data for LoRA fine-tuning</span>
        <span class="n">formatted_data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">instruction_data</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]:</span>
                <span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;### Instruction:</span><span class="se">\n</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;instruction&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">### Input:</span><span class="se">\n</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">### Response:</span><span class="se">\n</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;### Instruction:</span><span class="se">\n</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;instruction&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">### Response:</span><span class="se">\n</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">formatted_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;raw_data&quot;</span><span class="p">:</span> <span class="n">instruction_data</span><span class="p">,</span>
            <span class="s2">&quot;formatted_data&quot;</span><span class="p">:</span> <span class="n">formatted_data</span><span class="p">,</span>
            <span class="s2">&quot;count&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">instruction_data</span><span class="p">)</span>
        <span class="p">}</span>
    
    <span class="c1"># 5. Create final adaptation strategy combining all approaches</span>
    <span class="n">adaptation_strategy</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Data Preparation&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;Collect domain-specific research papers, textbooks, and lecture notes&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Extract key terminology, concepts, and relationship diagrams&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Create instruction-response pairs for common neuroscience questions&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Include experimental design scenarios and questions crossing neuroscience and AI&quot;</span>
        <span class="p">],</span>
        <span class="s2">&quot;Technical Implementation&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;Use LoRA fine-tuning on pretrained LLM with neuroscience instruction data&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Create embedding database of neuroscience reference materials for RAG&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Design system prompt establishing the assistant&#39;s neuroscience expertise&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Develop domain-specific few-shot examples for complex question types&quot;</span>
        <span class="p">],</span>
        <span class="s2">&quot;Evaluation Methods&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;Technical accuracy assessment by neuroscience experts&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Factual correctness comparison against textbook knowledge&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Citation accuracy for referenced research&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Helpfulness evaluation for experimental design questions&quot;</span>
        <span class="p">]</span>
    <span class="p">}</span>
    
    <span class="c1"># Sample domain adaptation outputs</span>
    <span class="n">sample_queries</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;What&#39;s the difference between supervised and unsupervised learning in terms of neural mechanisms?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Design an experiment to test the role of the hippocampus in spatial memory&quot;</span><span class="p">,</span>
        <span class="s2">&quot;How might predictive coding explain hallucinations in schizophrenia?&quot;</span>
    <span class="p">]</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;system_prompt&quot;</span><span class="p">:</span> <span class="n">neuroscience_system_prompt</span><span class="p">,</span>
        <span class="s2">&quot;few_shot_examples&quot;</span><span class="p">:</span> <span class="n">few_shot_examples</span><span class="p">,</span>
        <span class="s2">&quot;sample_instruction_data&quot;</span><span class="p">:</span> <span class="n">create_instruction_tuning_dataset</span><span class="p">(),</span>
        <span class="s2">&quot;adaptation_strategy&quot;</span><span class="p">:</span> <span class="n">adaptation_strategy</span><span class="p">,</span>
        <span class="s2">&quot;sample_queries&quot;</span><span class="p">:</span> <span class="n">sample_queries</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>This comprehensive case study demonstrates how to combine system prompts, few-shot examples, and domain-specific fine-tuning to create a specialized neuroscience assistant.</p>
</section>
<section id="exploring-model-evaluation-metrics">
<h3>12.6.5 Exploring Model Evaluation Metrics<a class="headerlink" href="#exploring-model-evaluation-metrics" title="Link to this heading">#</a></h3>
<p>Let’s examine how to evaluate LLM outputs for different applications:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">evaluate_llm_outputs</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate metrics and techniques for evaluating LLM outputs.&quot;&quot;&quot;</span>
    
    <span class="c1"># 1. Common evaluation metrics and approaches</span>
    <span class="n">evaluation_metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Automatic Metrics&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;BLEU&quot;</span><span class="p">:</span> <span class="s2">&quot;Measures n-gram overlap with reference texts (common in translation)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ROUGE&quot;</span><span class="p">:</span> <span class="s2">&quot;Recall-oriented metrics for summarization evaluation&quot;</span><span class="p">,</span>
            <span class="s2">&quot;BERTScore&quot;</span><span class="p">:</span> <span class="s2">&quot;Semantic similarity using contextualized embeddings&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Perplexity&quot;</span><span class="p">:</span> <span class="s2">&quot;Measures how well a model predicts a sample (lower is better)&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Human Evaluation Dimensions&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Correctness&quot;</span><span class="p">:</span> <span class="s2">&quot;Factual accuracy of the content&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Relevance&quot;</span><span class="p">:</span> <span class="s2">&quot;Appropriateness to the given query or instruction&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Coherence&quot;</span><span class="p">:</span> <span class="s2">&quot;Logical flow and consistency within the response&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Helpfulness&quot;</span><span class="p">:</span> <span class="s2">&quot;Practical utility for the intended purpose&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Harmlessness&quot;</span><span class="p">:</span> <span class="s2">&quot;Freedom from unsafe, biased, or harmful content&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;LLM-as-Judge&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Description&quot;</span><span class="p">:</span> <span class="s2">&quot;Using another LLM to evaluate model outputs&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Approaches&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="s2">&quot;Pairwise comparisons between model outputs&quot;</span><span class="p">,</span>
                <span class="s2">&quot;Rubric-based scoring against defined criteria&quot;</span><span class="p">,</span>
                <span class="s2">&quot;Error detection and factual verification&quot;</span>
            <span class="p">],</span>
            <span class="s2">&quot;Limitations&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="s2">&quot;Judge models may share biases with evaluated models&quot;</span><span class="p">,</span>
                <span class="s2">&quot;Reliability varies across domains and criteria&quot;</span><span class="p">,</span>
                <span class="s2">&quot;May favor certain response styles&quot;</span>
            <span class="p">]</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="c1"># 2. Example implementation of simple evaluation function</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate_responses</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">reference</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;human&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluate model responses using specified method.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;human&quot;</span><span class="p">:</span>
            <span class="c1"># Simulated human evaluation scores (in practice, real human ratings)</span>
            <span class="n">criteria</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;correctness&quot;</span><span class="p">,</span> <span class="s2">&quot;coherence&quot;</span><span class="p">,</span> <span class="s2">&quot;relevance&quot;</span><span class="p">,</span> <span class="s2">&quot;helpfulness&quot;</span><span class="p">]</span>
            <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
            
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">responses</span><span class="p">):</span>
                <span class="c1"># Simulated scores from 1-5</span>
                <span class="n">results</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;response_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">criterion</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="k">for</span> <span class="n">criterion</span> <span class="ow">in</span> <span class="n">criteria</span>
                <span class="p">}</span>
                
            <span class="k">return</span> <span class="p">{</span>
                <span class="s2">&quot;evaluation_type&quot;</span><span class="p">:</span> <span class="s2">&quot;human&quot;</span><span class="p">,</span>
                <span class="s2">&quot;criteria&quot;</span><span class="p">:</span> <span class="n">criteria</span><span class="p">,</span>
                <span class="s2">&quot;results&quot;</span><span class="p">:</span> <span class="n">results</span><span class="p">,</span>
                <span class="s2">&quot;average_scores&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="n">criterion</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">results</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">criterion</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span> 
                    <span class="k">for</span> <span class="n">criterion</span> <span class="ow">in</span> <span class="n">criteria</span>
                <span class="p">}</span>
            <span class="p">}</span>
            
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;automatic&quot;</span> <span class="ow">and</span> <span class="n">reference</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Simple simulation of automatic metrics</span>
            <span class="c1"># In practice, use libraries like nltk, evaluate, or torchmetrics</span>
            <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
            
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">responses</span><span class="p">):</span>
                <span class="c1"># Dummy calculations - would use actual metrics in practice</span>
                <span class="n">word_overlap</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">reference</span><span class="o">.</span><span class="n">split</span><span class="p">()))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">reference</span><span class="o">.</span><span class="n">split</span><span class="p">()))</span>
                
                <span class="n">results</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;response_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;word_overlap&quot;</span><span class="p">:</span> <span class="n">word_overlap</span><span class="p">,</span>
                    <span class="s2">&quot;length_ratio&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">response</span><span class="p">)</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">reference</span><span class="p">)),</span>
                    <span class="s2">&quot;simulated_bleu&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">word_overlap</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)),</span>
                    <span class="s2">&quot;simulated_bertscore&quot;</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.7</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">word_overlap</span><span class="p">)</span>
                <span class="p">}</span>
                
            <span class="k">return</span> <span class="p">{</span>
                <span class="s2">&quot;evaluation_type&quot;</span><span class="p">:</span> <span class="s2">&quot;automatic&quot;</span><span class="p">,</span>
                <span class="s2">&quot;metrics&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;word_overlap&quot;</span><span class="p">,</span> <span class="s2">&quot;length_ratio&quot;</span><span class="p">,</span> <span class="s2">&quot;simulated_bleu&quot;</span><span class="p">,</span> <span class="s2">&quot;simulated_bertscore&quot;</span><span class="p">],</span>
                <span class="s2">&quot;results&quot;</span><span class="p">:</span> <span class="n">results</span>
            <span class="p">}</span>
            
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;llm_judge&quot;</span><span class="p">:</span>
            <span class="c1"># Simulation of using another LLM to judge responses</span>
            <span class="n">judge_rubric</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;correctness&quot;</span><span class="p">:</span> <span class="s2">&quot;Evaluate factual accuracy on a scale of 1-5&quot;</span><span class="p">,</span>
                <span class="s2">&quot;coherence&quot;</span><span class="p">:</span> <span class="s2">&quot;Evaluate logical flow on a scale of 1-5&quot;</span><span class="p">,</span>
                <span class="s2">&quot;helpfulness&quot;</span><span class="p">:</span> <span class="s2">&quot;Evaluate practical utility on a scale of 1-5&quot;</span>
            <span class="p">}</span>
            
            <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">responses</span><span class="p">):</span>
                <span class="c1"># Simulated LLM judgment (in practice, would call an actual LLM)</span>
                <span class="n">results</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;response_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">criterion</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="k">for</span> <span class="n">criterion</span> <span class="ow">in</span> <span class="n">judge_rubric</span>
                <span class="p">}</span>
                
            <span class="k">return</span> <span class="p">{</span>
                <span class="s2">&quot;evaluation_type&quot;</span><span class="p">:</span> <span class="s2">&quot;llm_judge&quot;</span><span class="p">,</span>
                <span class="s2">&quot;rubric&quot;</span><span class="p">:</span> <span class="n">judge_rubric</span><span class="p">,</span>
                <span class="s2">&quot;results&quot;</span><span class="p">:</span> <span class="n">results</span>
            <span class="p">}</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="s2">&quot;Invalid evaluation method or missing reference&quot;</span><span class="p">}</span>
    
    <span class="c1"># 3. Sample responses to evaluate</span>
    <span class="n">sample_query</span> <span class="o">=</span> <span class="s2">&quot;Explain the concept of neural plasticity.&quot;</span>
    <span class="n">sample_responses</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;Neural plasticity refers to the brain&#39;s ability to change and reorganize itself by forming new neural connections. This property allows the brain to adapt to new experiences, learn, and recover from injuries.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Neural plasticity is how neurons can change. The brain can make new connections between neurons when you learn new things or after an injury. This happens throughout life but is strongest when you&#39;re young.&quot;</span>
    <span class="p">]</span>
    <span class="n">sample_reference</span> <span class="o">=</span> <span class="s2">&quot;Neural plasticity, also known as neuroplasticity, is the ability of neural networks in the brain to change through growth and reorganization. These changes range from individual neuron modifications to large-scale network rewiring. The phenomenon occurs during normal development, learning, and as an adaptive mechanism following brain injury.&quot;</span>
    
    <span class="c1"># 4. Run evaluations (simulated)</span>
    <span class="n">evaluation_results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;human&quot;</span><span class="p">:</span> <span class="n">evaluate_responses</span><span class="p">(</span><span class="n">sample_responses</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;human&quot;</span><span class="p">),</span>
        <span class="s2">&quot;automatic&quot;</span><span class="p">:</span> <span class="n">evaluate_responses</span><span class="p">(</span><span class="n">sample_responses</span><span class="p">,</span> <span class="n">reference</span><span class="o">=</span><span class="n">sample_reference</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;automatic&quot;</span><span class="p">),</span>
        <span class="s2">&quot;llm_judge&quot;</span><span class="p">:</span> <span class="n">evaluate_responses</span><span class="p">(</span><span class="n">sample_responses</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;llm_judge&quot;</span><span class="p">)</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;metrics_overview&quot;</span><span class="p">:</span> <span class="n">evaluation_metrics</span><span class="p">,</span>
        <span class="s2">&quot;sample_query&quot;</span><span class="p">:</span> <span class="n">sample_query</span><span class="p">,</span>
        <span class="s2">&quot;sample_responses&quot;</span><span class="p">:</span> <span class="n">sample_responses</span><span class="p">,</span>
        <span class="s2">&quot;sample_reference&quot;</span><span class="p">:</span> <span class="n">sample_reference</span><span class="p">,</span>
        <span class="s2">&quot;evaluation_results&quot;</span><span class="p">:</span> <span class="n">evaluation_results</span><span class="p">,</span>
        <span class="s2">&quot;best_practices&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;Use multiple evaluation methods for comprehensive assessment&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Define clear evaluation criteria before assessment&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Consider task-specific metrics for specialized applications&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Combine automatic metrics with human or LLM-based evaluation&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Benchmark against established models for comparative analysis&quot;</span>
        <span class="p">]</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Through these exercises, we’ve explored practical implementations of the key concepts covered in this chapter, from parameter-efficient fine-tuning with LoRA to advanced prompting techniques, RAG systems for factual grounding, domain adaptation, and comprehensive evaluation approaches.</p>
</section>
</section>
<section id="take-aways">
<h2>12.7 Take-aways<a class="headerlink" href="#take-aways" title="Link to this heading">#</a></h2>
<div class="important admonition">
<p class="admonition-title">Knowledge Connections</p>
<p><strong>Looking Back</strong></p>
<ul class="simple">
<li><p><strong>Chapter 7 (Information Theory Essentials)</strong>: Information-theoretic principles like entropy and KL divergence underpin LLM training objectives and evaluation metrics, connecting statistical learning to language modeling.</p></li>
<li><p><strong>Chapter 9 (Classical Machine-Learning Foundations)</strong>: LLMs build upon supervised learning paradigms but extend them to self-supervised pretraining, where the model generates its own supervision signal.</p></li>
<li><p><strong>Chapter 10 (Deep Learning)</strong>: The optimization techniques covered in Chapter 10 are essential for training LLMs, with additional considerations for the extreme scale of parameters and compute.</p></li>
<li><p><strong>Chapter 11 (Sequence Models)</strong>: LLMs are direct descendants of transformer architectures from Chapter 11, scaling up the core architecture while introducing innovations to handle longer contexts.</p></li>
</ul>
<p><strong>Looking Forward</strong></p>
<ul class="simple">
<li><p><strong>Chapter 13 (Multimodal Models)</strong>: LLMs serve as a foundation for multimodal architectures that integrate language understanding with other modalities like vision and audio.</p></li>
<li><p><strong>Chapter 14 (Future Directions)</strong>: The scaling laws and emergent abilities of LLMs shown in the scaling law figure point toward future research directions in AI capabilities and limitations.</p></li>
</ul>
</div>
<p>This chapter explored the fundamentals, fine-tuning approaches, and advanced applications of Large Language Models, bridging AI capabilities with neuroscience insights:</p>
<ol class="arabic simple">
<li><p><strong>Architectural Foundations</strong>:</p>
<ul class="simple">
<li><p>LLMs build on transformer architectures with specialized attention mechanisms and tokenization strategies</p></li>
<li><p>Scale plays a crucial role in model capabilities, with emergent abilities appearing at specific thresholds</p></li>
<li><p>Training objectives shape model behavior, with next-token prediction aligning with brain predictive mechanisms</p></li>
</ul>
</li>
<li><p><strong>Fine-tuning Methods</strong>:</p>
<ul class="simple">
<li><p>Full fine-tuning provides maximum adaptation but requires substantial resources</p></li>
<li><p>Parameter-efficient methods like LoRA yield comparable results with minimal parameter updates</p></li>
<li><p>Instruction fine-tuning aligns models with human intent and task understanding</p></li>
<li><p>RLHF leverages human preferences to improve helpfulness, harmlessness, and honesty</p></li>
</ul>
</li>
<li><p><strong>Prompting Techniques</strong>:</p>
<ul class="simple">
<li><p>Zero-shot and few-shot learning enable task adaptation without weight updates</p></li>
<li><p>Chain-of-thought prompting dramatically improves reasoning capabilities</p></li>
<li><p>System prompts establish model behavior patterns and specialized domains</p></li>
<li><p>Effective prompt engineering significantly enhances model outputs</p></li>
</ul>
</li>
<li><p><strong>Biological Parallels</strong>:</p>
<ul class="simple">
<li><p>LLMs exhibit similarities to brain language networks at multiple levels</p></li>
<li><p>Predictive processing is central to both human and artificial language systems</p></li>
<li><p>Attention mechanisms parallel neural selective enhancement</p></li>
<li><p>Both systems must solve the binding problem for compositional representation</p></li>
</ul>
</li>
<li><p><strong>Limitations and Challenges</strong>:</p>
<ul class="simple">
<li><p>Hallucinations require factual grounding and reliability safeguards</p></li>
<li><p>Bias mitigation demands ongoing development of fair representation</p></li>
<li><p>Context window limitations constrain document-level understanding</p></li>
<li><p>Reasoning capabilities show both impressive advances and significant gaps</p></li>
</ul>
</li>
<li><p><strong>Implementation Insights</strong>:</p>
<ul class="simple">
<li><p>Combining fine-tuning, prompting, and retrieval yields powerful applications</p></li>
<li><p>Domain adaptation creates specialized capabilities beyond general models</p></li>
<li><p>Comprehensive evaluation must consider multiple dimensions of performance</p></li>
<li><p>Trade-offs between efficiency, accuracy, and flexibility guide system design</p></li>
</ul>
</li>
</ol>
<p>The integration of neuroscience principles with LLM development creates a virtuous cycle, where brain-inspired mechanisms enhance AI capabilities while AI insights deepen our understanding of human language processing. This cross-disciplinary approach promises continued advances in both fields.</p>
<div style="page-break-before:always;"></div>
<div class="important admonition">
<p class="admonition-title">Chapter Summary</p>
<p>In this chapter, we explored:</p>
<ul class="simple">
<li><p><strong>Transformer-based architecture</strong> principles that power large language models</p></li>
<li><p><strong>Scaling laws</strong> showing how model performance scales with parameters, data, and compute</p></li>
<li><p><strong>Pre-training objectives</strong> like next-token prediction that enable self-supervised learning</p></li>
<li><p><strong>Tokenization strategies</strong> that transform text into a format processable by neural networks</p></li>
<li><p><strong>Fine-tuning techniques</strong> from full model adaptation to parameter-efficient methods like LoRA</p></li>
<li><p><strong>Instruction fine-tuning and RLHF</strong> methods that align models with human preferences</p></li>
<li><p><strong>Prompting strategies</strong> including zero-shot, few-shot, and chain-of-thought approaches</p></li>
<li><p><strong>Neural correlates of language</strong> in the brain’s predictive processing mechanisms</p></li>
<li><p><strong>Current limitations</strong> such as hallucinations, bias, context constraints, and reasoning gaps</p></li>
<li><p><strong>Practical implementations</strong> of LLM techniques through hands-on code examples</p></li>
</ul>
<p>This chapter bridges the computational foundation of large language models with biological language processing, demonstrating how each field informs the other while providing practical guidance for adapting these powerful models to specialized domains.</p>
</div>
</section>
<section id="further-reading-media">
<h2>12.8 Further Reading &amp; Media<a class="headerlink" href="#further-reading-media" title="Link to this heading">#</a></h2>
<section id="foundational-papers">
<h3>Foundational Papers<a class="headerlink" href="#foundational-papers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Brown, T., et al. (2020). <a class="reference external" href="https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">Language Models are Few-Shot Learners</a>. <em>Advances in Neural Information Processing Systems, 33</em>.</p></li>
<li><p>Vaswani, A., et al. (2017). <a class="reference external" href="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">Attention Is All You Need</a>. <em>Advances in Neural Information Processing Systems, 30</em>.</p></li>
<li><p>Kaplan, J., et al. (2020). <a class="reference external" href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a>. <em>arXiv preprint arXiv:2001.08361</em>.</p></li>
</ul>
</section>
<section id="fine-tuning-adaptation">
<h3>Fine-tuning &amp; Adaptation<a class="headerlink" href="#fine-tuning-adaptation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Hu, E., et al. (2021). <a class="reference external" href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a>. <em>International Conference on Learning Representations (ICLR)</em>.</p></li>
<li><p>Ouyang, L., et al. (2022). <a class="reference external" href="https://arxiv.org/abs/2203.02155">Training Language Models to Follow Instructions with Human Feedback</a>. <em>Advances in Neural Information Processing Systems, 35</em>.</p></li>
<li><p>Houlsby, N., et al. (2019). <a class="reference external" href="http://proceedings.mlr.press/v97/houlsby19a.html">Parameter-Efficient Transfer Learning for NLP</a>. <em>International Conference on Machine Learning</em>.</p></li>
</ul>
</section>
<section id="prompting-reasoning">
<h3>Prompting &amp; Reasoning<a class="headerlink" href="#prompting-reasoning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Wei, J., et al. (2022). <a class="reference external" href="https://arxiv.org/abs/2201.11903">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a>. <em>Advances in Neural Information Processing Systems, 35</em>.</p></li>
<li><p>Kojima, T., et al. (2022). <a class="reference external" href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-Shot Reasoners</a>. <em>Advances in Neural Information Processing Systems, 35</em>.</p></li>
<li><p>Reynolds, L., &amp; McDonell, K. (2021). <a class="reference external" href="https://arxiv.org/abs/2102.07350">Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</a>. <em>CHI Conference on Human Factors in Computing Systems</em>.</p></li>
</ul>
</section>
<section id="neuroscience-connections">
<h3>Neuroscience Connections<a class="headerlink" href="#neuroscience-connections" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>McClelland, J. L., et al. (2020). <a class="reference external" href="https://arxiv.org/abs/1912.05877">Extending Machine Language Models toward Human-Level Language Understanding</a>. <em>arXiv preprint arXiv:1912.05877</em>.</p></li>
<li><p>Caucheteux, C., &amp; King, J. R. (2022). <a class="reference external" href="https://www.nature.com/articles/s42003-022-03036-1">Brains and algorithms partially converge in natural language processing</a>. <em>Communications Biology, 5(1)</em>.</p></li>
<li><p>Schrimpf, M., et al. (2021). <a class="reference external" href="https://www.pnas.org/doi/10.1073/pnas.2105646118">The neural architecture of language: Integrative modeling converges on predictive processing</a>. <em>Proceedings of the National Academy of Sciences, 118(45)</em>.</p></li>
</ul>
</section>
<section id="limitations-challenges">
<h3>Limitations &amp; Challenges<a class="headerlink" href="#limitations-challenges" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Bender, E. M., et al. (2021). <a class="reference external" href="https://dl.acm.org/doi/10.1145/3442188.3445922">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</a>. <em>FAccT ‘21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>.</p></li>
<li><p>Bommasani, R., et al. (2021). <a class="reference external" href="https://arxiv.org/abs/2108.07258">On the Opportunities and Risks of Foundation Models</a>. <em>arXiv preprint arXiv:2108.07258</em>.</p></li>
<li><p>Ji, Z., et al. (2023). <a class="reference external" href="https://arxiv.org/abs/2202.03629">Survey of Hallucination in Natural Language Generation</a>. <em>ACM Computing Surveys</em>.</p></li>
</ul>
</section>
<section id="books-resources">
<h3>Books &amp; Resources<a class="headerlink" href="#books-resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Jurafsky, D., &amp; Martin, J. H. (2023). <em><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a></em>. 3rd Edition Draft.</p></li>
<li><p>LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). <a class="reference external" href="https://www.nature.com/articles/nature14539">Deep learning</a>. <em>Nature, 521(7553)</em>, 436-444.</p></li>
<li><p>Cobbe, K., et al. (2021). <a class="reference external" href="https://arxiv.org/abs/2110.14168">Training Verifiers to Solve Math Word Problems</a>. <em>arXiv preprint arXiv:2110.14168</em>.</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "executablebooks/jupyter-book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Richard Young
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>