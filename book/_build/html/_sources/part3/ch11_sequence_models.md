# Chapter 11: Sequence Models: RNN → Attention → Transformer

## 11.0 Chapter Goals
- Understand the evolution of sequence models
- Master recurrent networks, attention, and transformers
- Connect sequence models to temporal processing in the brain
- Implement key sequence modeling architectures

## 11.1 Recurrent Neural Networks
- Vanilla RNNs
- LSTMs and GRUs
- Bidirectional RNNs
- Challenges in training RNNs

## 11.2 Attention Mechanisms
- Self-attention
- Cross-attention
- Multi-head attention
- Attention visualization

## 11.3 Transformer Architecture
- Encoder-decoder structure
- Positional encodings
- Layer normalization
- Feed-forward networks

## 11.4 Neural Sequence Processing
- Temporal dynamics in cortical circuits
- Working memory mechanisms
- Predictive processing
- Hierarchical temporal processing

## 11.5 Applications
- Natural language processing
- Time series forecasting
- Neural sequence decoding
- Generative sequence models

## 11.6 Code Lab
- Implementing an LSTM from components
- Building a self-attention mechanism
- Training a small transformer
- Sequence prediction tasks

## 11.7 Take-aways
- Sequence modeling has evolved towards parallelism
- Attention mechanisms capture long-range dependencies
- Transformers revolutionized sequence processing

## 11.8 Further Reading & Media
- Sutskever et al. (2014) - "Sequence to Sequence Learning with Neural Networks"
- Vaswani et al. (2017) - "Attention Is All You Need"
- Karpathy's "The Unreasonable Effectiveness of RNNs" blog post