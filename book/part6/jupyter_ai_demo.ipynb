{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter AI Integration for NeuroAI\n",
    "\n",
    "This notebook demonstrates how to use Jupyter AI to enhance the learning experience when studying neuroscience and AI concepts. Jupyter AI integrates generative AI capabilities directly into Jupyter notebooks, allowing you to:\n",
    "\n",
    "- Generate code examples based on neuroscience concepts\n",
    "- Get explanations of complex neural network architectures\n",
    "- Debug and improve code implementations\n",
    "- Generate visualizations for neural data\n",
    "- Summarize research papers and concepts\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "To use Jupyter AI, you'll need to install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# Install Jupyter AI and dependencies\n",
    "!pip install -q jupyter-ai jupyter-ai-magics openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installation, you need to enable the extension. In JupyterLab, this happens automatically. In a classic notebook, you may need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Jupyter AI magics\n",
    "%load_ext jupyter_ai_magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Jupyter AI\n",
    "\n",
    "Jupyter AI can be configured to use different AI models:\n",
    "\n",
    "1. OpenAI models (requires API key)\n",
    "2. Anthropic Claude models (requires API key)\n",
    "3. Open source models via Hugging Face or local installation\n",
    "\n",
    "To configure an API key, you can use environment variables or the Jupyter configuration system. For this demo, we'll use a placeholder approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration (for demonstration only)\n",
    "# In a real setup, you would set these via environment variables\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "print(\"Note: For actual use, you'll need to configure your API keys\")\n",
    "print(\"See: https://jupyter-ai.readthedocs.io/en/latest/users/index.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Jupyter AI with NeuroAI\n",
    "\n",
    "Jupyter AI can be used in several ways:\n",
    "\n",
    "1. Using cell magics with `%%ai`\n",
    "2. Using line magics with `%ai`\n",
    "3. Using the chat interface in JupyterLab\n",
    "\n",
    "Let's see some examples of how to use these features for NeuroAI tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Generating Code for Neuroscience Models\n",
    "\n",
    "We can use Jupyter AI to generate code for implementing neuroscience-inspired models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai gpt-4\n",
    "Write a PyTorch implementation of a predictive coding network for visual inputs, based on the PredNet architecture described in Chapter 20. Include comments explaining the biological inspiration for each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In a live Jupyter AI environment, the code above would produce a complete PyTorch implementation of PredNet. Since we don't have API keys configured here, we'll show an example of what the output might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PredNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of the PredNet architecture\n",
    "    \n",
    "    Biological inspiration:\n",
    "    - Representation units: Similar to neural populations in visual cortex that encode features\n",
    "    - Prediction units: Analogous to top-down feedback connections in visual hierarchy\n",
    "    - Error units: Mimics error-signaling neurons that respond to unexpected stimuli\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, representation_channels, prediction_channels):\n",
    "        super(PredNetLayer, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.representation_channels = representation_channels\n",
    "        \n",
    "        # Error computation units (biological: error-signaling neurons)\n",
    "        # Separate units for positive and negative errors mimics ON/OFF pathways\n",
    "        self.error_conv_pos = nn.Conv2d(input_channels, representation_channels, \n",
    "                                        kernel_size=3, padding=1)\n",
    "        self.error_conv_neg = nn.Conv2d(input_channels, representation_channels, \n",
    "                                        kernel_size=3, padding=1)\n",
    "        \n",
    "        # Prediction units (biological: feedback connections)\n",
    "        self.prediction_conv = nn.Conv2d(representation_channels, input_channels, \n",
    "                                         kernel_size=3, padding=1)\n",
    "        \n",
    "        # Representation update network (biological: integration in pyramidal neurons)\n",
    "        self.representation_update = nn.Conv2d(2*representation_channels, \n",
    "                                             representation_channels,\n",
    "                                             kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pooling for feeding to next layer (biological: hierarchical processing)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, input_tensor, representation_tensor, top_down_tensor=None):\n",
    "        # Generate predictions from current representation\n",
    "        predictions = self.prediction_conv(representation_tensor)\n",
    "        \n",
    "        # Calculate prediction errors (biological: error-signaling neurons)\n",
    "        error = input_tensor - predictions\n",
    "        error_pos = F.relu(error)  # Positive errors (ON pathway)\n",
    "        error_neg = F.relu(-error)  # Negative errors (OFF pathway)\n",
    "        \n",
    "        # Process errors through convolutional layers\n",
    "        error_pos_processed = self.error_conv_pos(error_pos)\n",
    "        error_neg_processed = self.error_conv_neg(error_neg)\n",
    "        \n",
    "        # Combine error signals (biological: integration of error signals)\n",
    "        combined_error = torch.cat([error_pos_processed, error_neg_processed], dim=1)\n",
    "        \n",
    "        # Update representation (biological: synaptic update based on errors)\n",
    "        representation_update = self.representation_update(combined_error)\n",
    "        updated_representation = representation_tensor + representation_update\n",
    "        \n",
    "        # If we have top-down input, integrate it (biological: top-down modulation)\n",
    "        if top_down_tensor is not None:\n",
    "            updated_representation = updated_representation + top_down_tensor\n",
    "        \n",
    "        # Pool for next layer (biological: increased receptive field sizes in hierarchy)\n",
    "        pooled_representation = self.pool(updated_representation)\n",
    "        \n",
    "        return error, updated_representation, pooled_representation\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete PredNet architecture with multiple layers\n",
    "    \n",
    "    Biological inspiration:\n",
    "    - Hierarchical structure: Mimics the visual cortical hierarchy (V1, V2, V4, IT)\n",
    "    - Bidirectional processing: Combines bottom-up and top-down information flow\n",
    "    - Prediction error emphasis: Focuses learning on surprising/unexpected information\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, layer_channels):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.num_layers = len(layer_channels)\n",
    "        \n",
    "        # Create PredNet layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_channels = input_channels\n",
    "        for i, out_channels in enumerate(layer_channels):\n",
    "            self.layers.append(PredNetLayer(in_channels, out_channels, out_channels))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Upsampling for top-down connections (biological: feedback projections)\n",
    "        self.upsample = nn.ModuleList()\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.upsample.append(nn.ConvTranspose2d(\n",
    "                layer_channels[i+1], layer_channels[i],\n",
    "                kernel_size=4, stride=2, padding=1\n",
    "            ))\n",
    "    \n",
    "    def forward(self, input_sequence):\n",
    "        batch_size, seq_length, channels, height, width = input_sequence.shape\n",
    "        \n",
    "        # Initialize representations for each layer\n",
    "        representations = [torch.zeros(batch_size, layer.representation_channels, \n",
    "                                     height // (2**i), width // (2**i),\n",
    "                                     device=input_sequence.device)\n",
    "                         for i, layer in enumerate(self.layers)]\n",
    "        \n",
    "        # Initialize errors for each layer\n",
    "        errors = [torch.zeros(batch_size, layer.input_channels,\n",
    "                            height // (2**i), width // (2**i),\n",
    "                            device=input_sequence.device)\n",
    "                 for i, layer in enumerate(self.layers)]\n",
    "        \n",
    "        all_errors = []  # Store errors for all time steps\n",
    "        \n",
    "        # Process sequence\n",
    "        for t in range(seq_length):\n",
    "            current_input = input_sequence[:, t]\n",
    "            frame_errors = []\n",
    "            \n",
    "            # Bottom-up pass (biological: feedforward processing)\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                # Set input for each layer\n",
    "                if i == 0:\n",
    "                    layer_input = current_input\n",
    "                else:\n",
    "                    # Input from errors of previous layer (biological: error propagation)\n",
    "                    layer_input = errors[i-1]\n",
    "                \n",
    "                # Process through layer\n",
    "                error, updated_rep, pooled_rep = layer(layer_input, representations[i])\n",
    "                \n",
    "                # Store results\n",
    "                errors[i] = error\n",
    "                representations[i] = updated_rep\n",
    "                frame_errors.append(error.mean().item())\n",
    "                \n",
    "                # Set input for next layer\n",
    "                if i < self.num_layers - 1:\n",
    "                    next_layer_input = pooled_rep\n",
    "            \n",
    "            # Top-down pass (biological: feedback processing)\n",
    "            for i in reversed(range(self.num_layers - 1)):\n",
    "                # Get top-down signal\n",
    "                top_down = self.upsample[i](representations[i+1])\n",
    "                \n",
    "                # Update representation with top-down input\n",
    "                representations[i] = representations[i] + top_down\n",
    "                \n",
    "            all_errors.append(frame_errors)\n",
    "        \n",
    "        return errors, representations, all_errors\n",
    "\n",
    "# Example usage\n",
    "def create_prednet_model():\n",
    "    \"\"\"Create a PredNet model for video prediction\"\"\"\n",
    "    # Input: RGB images\n",
    "    input_channels = 3\n",
    "    # Layer channels increase with depth (analogous to increasing feature complexity)\n",
    "    layer_channels = [32, 64, 128, 256]\n",
    "    \n",
    "    model = PredNet(input_channels, layer_channels)\n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_prednet_model()\n",
    "print(f\"PredNet model created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Getting Explanations of Complex Concepts\n",
    "\n",
    "Jupyter AI is useful for getting on-demand explanations of complex neuroscience or AI concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai anthropic/claude-3-sonnet\n",
    "Explain the connection between hippocampal replay during sleep and Prioritized Experience Replay in reinforcement learning. Include key similarities and differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output (what Jupyter AI would generate with proper configuration):\n",
    "\n",
    "**Hippocampal Replay vs. Prioritized Experience Replay**\n",
    "\n",
    "**Similarities:**\n",
    "\n",
    "1. **Selective Memory Processing**: Both mechanisms selectively revisit past experiences rather than processing all memories equally.\n",
    "   \n",
    "2. **Learning Efficiency**: Both enhance learning efficiency by focusing computational resources on high-value experiences.\n",
    "   \n",
    "3. **Information Consolidation**: Both help stabilize and integrate new knowledge with existing memories.\n",
    "   \n",
    "4. **Pattern Reactivation**: Both involve reactivating patterns of activity associated with previous experiences.\n",
    "   \n",
    "5. **Stability-Plasticity Balance**: Both help address the stability-plasticity dilemma by allowing new learning without catastrophic forgetting.\n",
    "\n",
    "**Differences:**\n",
    "\n",
    "1. **Priority Mechanism**: \n",
    "   - Hippocampal replay often prioritizes experiences based on emotional salience, novelty, reward prediction errors, and goal relevance.\n",
    "   - PER explicitly uses TD errors (prediction errors) as the priority metric.\n",
    "\n",
    "2. **Temporal Structure**:\n",
    "   - Hippocampal replay occurs primarily during sleep and rest periods, with specific patterns during different sleep stages.\n",
    "   - PER operates continuously during the learning process, interleaved with new experiences.\n",
    "\n",
    "3. **Speed and Compression**:\n",
    "   - Hippocampal replay often occurs at compressed timescales (5-20x faster than real experiences).\n",
    "   - PER doesn't explicitly model temporal compression.\n",
    "\n",
    "4. **Sequence Structure**:\n",
    "   - Hippocampal replay often maintains sequential structure, replaying experiences in forward or reverse order.\n",
    "   - PER samples individual transitions without preserving sequences.\n",
    "\n",
    "5. **Biological Implementation**:\n",
    "   - Hippocampal replay involves coordinated oscillations (sharp-wave ripples) and specific neurochemical environments.\n",
    "   - PER is an algorithmic approximation that captures the functional role but not the biological implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Debugging and Improving Code\n",
    "\n",
    "Jupyter AI can help debug and improve code implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code with a bug\n",
    "def simulate_hodgkin_huxley(I_ext=10, T=100, dt=0.01):\n",
    "    \"\"\"Simulate the Hodgkin-Huxley model of action potential generation\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Constants\n",
    "    E_Na = 55.0   # mV\n",
    "    E_K = -77.0   # mV\n",
    "    E_L = -65.0   # mV\n",
    "    g_Na = 120.0  # mS/cm^2\n",
    "    g_K = 36.0    # mS/cm^2\n",
    "    g_L = 0.3     # mS/cm^2\n",
    "    C_m = 1.0     # uF/cm^2\n",
    "    \n",
    "    # Simulation parameters\n",
    "    steps = int(T / dt)\n",
    "    t = np.arange(0, T, dt)\n",
    "    \n",
    "    # State variables\n",
    "    V = np.zeros(steps)\n",
    "    m = np.zeros(steps)\n",
    "    h = np.zeros(steps)\n",
    "    n = np.zeros(steps)\n",
    "    \n",
    "    # Initialize\n",
    "    V[0] = -65.0  # mV\n",
    "    m[0] = 0.05\n",
    "    h[0] = 0.6\n",
    "    n[0] = 0.32\n",
    "    \n",
    "    # Helper functions for gating variables\n",
    "    def alpha_m(V):\n",
    "        return 0.1 * (V + 40.0) / (1.0 - np.exp(-(V + 40.0) / 10.0))\n",
    "    \n",
    "    def beta_m(V):\n",
    "        return 4.0 * np.exp(-(V + 65.0) / 18.0)\n",
    "    \n",
    "    def alpha_h(V):\n",
    "        return 0.07 * np.exp(-(V + 65.0) / 20.0)\n",
    "    \n",
    "    def beta_h(V):\n",
    "        return 1.0 / (1.0 + np.exp(-(V + 35.0) / 10.0))\n",
    "    \n",
    "    def alpha_n(V):\n",
    "        return 0.01 * (V + 55.0) / (1.0 - np.exp(-(V + 55.0) / 10.0))\n",
    "    \n",
    "    def beta_n(V):\n",
    "        return 0.125 * np.exp(-(V + 65.0) / 80.0)\n",
    "    \n",
    "    # Bug: division by zero in alpha_m when V = -40.0\n",
    "    # Bug: division by zero in alpha_n when V = -55.0\n",
    "    \n",
    "    # Simulation loop\n",
    "    for i in range(1, steps):\n",
    "        # Calculate gating variables\n",
    "        m[i] = m[i-1] + dt * (alpha_m(V[i-1]) * (1 - m[i-1]) - beta_m(V[i-1]) * m[i-1])\n",
    "        h[i] = h[i-1] + dt * (alpha_h(V[i-1]) * (1 - h[i-1]) - beta_h(V[i-1]) * h[i-1])\n",
    "        n[i] = n[i-1] + dt * (alpha_n(V[i-1]) * (1 - n[i-1]) - beta_n(V[i-1]) * n[i-1])\n",
    "        \n",
    "        # Calculate conductances and currents\n",
    "        g_Na_t = g_Na * m[i]**3 * h[i]\n",
    "        g_K_t = g_K * n[i]**4\n",
    "        \n",
    "        I_Na = g_Na_t * (V[i-1] - E_Na)\n",
    "        I_K = g_K_t * (V[i-1] - E_K)\n",
    "        I_L = g_L * (V[i-1] - E_L)\n",
    "        \n",
    "        # Update membrane potential\n",
    "        dV = (I_ext - I_Na - I_K - I_L) / C_m\n",
    "        V[i] = V[i-1] + dt * dV\n",
    "        \n",
    "    return t, V, m, h, n\n",
    "\n",
    "print(\"Code with potential bugs defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai gpt-4\n",
    "There's a bug in my Hodgkin-Huxley model implementation. The code can cause division by zero errors in the alpha_m and alpha_n functions. Can you fix these issues and explain the neurobiological meaning of these functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output (what Jupyter AI would generate with proper configuration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_hodgkin_huxley(I_ext=10, T=100, dt=0.01):\n",
    "    \"\"\"Simulate the Hodgkin-Huxley model of action potential generation\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Constants\n",
    "    E_Na = 55.0   # mV\n",
    "    E_K = -77.0   # mV\n",
    "    E_L = -65.0   # mV\n",
    "    g_Na = 120.0  # mS/cm^2\n",
    "    g_K = 36.0    # mS/cm^2\n",
    "    g_L = 0.3     # mS/cm^2\n",
    "    C_m = 1.0     # uF/cm^2\n",
    "    \n",
    "    # Simulation parameters\n",
    "    steps = int(T / dt)\n",
    "    t = np.arange(0, T, dt)\n",
    "    \n",
    "    # State variables\n",
    "    V = np.zeros(steps)\n",
    "    m = np.zeros(steps)\n",
    "    h = np.zeros(steps)\n",
    "    n = np.zeros(steps)\n",
    "    \n",
    "    # Initialize\n",
    "    V[0] = -65.0  # mV\n",
    "    m[0] = 0.05\n",
    "    h[0] = 0.6\n",
    "    n[0] = 0.32\n",
    "    \n",
    "    # Helper functions for gating variables with fixed division by zero issues\n",
    "    def alpha_m(V):\n",
    "        # Using L'Hôpital's rule for the case when V is close to -40\n",
    "        if np.abs(V + 40.0) < 1e-6:\n",
    "            return 1.0  # Limit value when V = -40\n",
    "        return 0.1 * (V + 40.0) / (1.0 - np.exp(-(V + 40.0) / 10.0))\n",
    "    \n",
    "    def beta_m(V):\n",
    "        return 4.0 * np.exp(-(V + 65.0) / 18.0)\n",
    "    \n",
    "    def alpha_h(V):\n",
    "        return 0.07 * np.exp(-(V + 65.0) / 20.0)\n",
    "    \n",
    "    def beta_h(V):\n",
    "        return 1.0 / (1.0 + np.exp(-(V + 35.0) / 10.0))\n",
    "    \n",
    "    def alpha_n(V):\n",
    "        # Using L'Hôpital's rule for the case when V is close to -55\n",
    "        if np.abs(V + 55.0) < 1e-6:\n",
    "            return 0.1  # Limit value when V = -55\n",
    "        return 0.01 * (V + 55.0) / (1.0 - np.exp(-(V + 55.0) / 10.0))\n",
    "    \n",
    "    def beta_n(V):\n",
    "        return 0.125 * np.exp(-(V + 65.0) / 80.0)\n",
    "    \n",
    "    # Simulation loop\n",
    "    for i in range(1, steps):\n",
    "        # Calculate gating variables\n",
    "        m[i] = m[i-1] + dt * (alpha_m(V[i-1]) * (1 - m[i-1]) - beta_m(V[i-1]) * m[i-1])\n",
    "        h[i] = h[i-1] + dt * (alpha_h(V[i-1]) * (1 - h[i-1]) - beta_h(V[i-1]) * h[i-1])\n",
    "        n[i] = n[i-1] + dt * (alpha_n(V[i-1]) * (1 - n[i-1]) - beta_n(V[i-1]) * n[i-1])\n",
    "        \n",
    "        # Calculate conductances and currents\n",
    "        g_Na_t = g_Na * m[i]**3 * h[i]\n",
    "        g_K_t = g_K * n[i]**4\n",
    "        \n",
    "        I_Na = g_Na_t * (V[i-1] - E_Na)\n",
    "        I_K = g_K_t * (V[i-1] - E_K)\n",
    "        I_L = g_L * (V[i-1] - E_L)\n",
    "        \n",
    "        # Update membrane potential\n",
    "        dV = (I_ext - I_Na - I_K - I_L) / C_m\n",
    "        V[i] = V[i-1] + dt * dV\n",
    "        \n",
    "    return t, V, m, h, n\n",
    "\n",
    "print(\"Fixed code defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neurobiological Meaning of Alpha and Beta Functions**\n",
    "\n",
    "In the Hodgkin-Huxley model, the alpha (α) and beta (β) functions represent voltage-dependent rate constants that control the opening and closing of ion channels:\n",
    "\n",
    "1. **Alpha Functions (α_m, α_h, α_n)**: \n",
    "   - Represent the rate at which the corresponding gates transition from closed to open states\n",
    "   - Neurobiologically, they model the voltage-dependent probability of conformational changes in ion channel proteins that lead to channel opening\n",
    "   - For example, α_m increases with depolarization, representing faster sodium channel activation as voltage increases\n",
    "\n",
    "2. **Beta Functions (β_m, β_h, β_n)**:\n",
    "   - Represent the rate at which the gates transition from open to closed states\n",
    "   - Neurobiologically, they model the rate at which ion channels return to their resting conformation\n",
    "   - For example, β_h increases with depolarization, representing faster sodium channel inactivation\n",
    "\n",
    "3. **Specific Gating Variables**:\n",
    "   - **m**: Controls sodium channel activation (opens quickly with depolarization)\n",
    "   - **h**: Controls sodium channel inactivation (closes slowly with sustained depolarization)\n",
    "   - **n**: Controls potassium channel activation (opens slowly with depolarization)\n",
    "\n",
    "The fixes I implemented use L'Hôpital's rule to handle the special cases where the denominator becomes zero, which happens at specific voltage values (-40 mV for α_m and -55 mV for α_n). These special cases are important physiologically as they represent critical voltages where channel opening rates undergo significant changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Generating and Improving Visualizations\n",
    "\n",
    "Jupyter AI can help create and enhance visualizations of neural data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai anthropic/claude-3-opus\n",
    "Generate code to visualize the attention maps in a Vision Transformer model, similar to what we discussed in Chapter 20. The visualization should show both the image and the corresponding attention patterns between patches. Include interactivity to explore different attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output (what Jupyter AI would generate with proper configuration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "\n",
    "# Function to load an image from URL or file path\n",
    "def load_image(image_path):\n",
    "    \"\"\"Load image from URL or file path\"\"\"\n",
    "    if image_path.startswith('http'):\n",
    "        response = requests.get(image_path)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        img = Image.open(image_path)\n",
    "    return img\n",
    "\n",
    "# Function to extract attention maps from ViT model\n",
    "def get_attention_maps(model, processor, image):\n",
    "    \"\"\"Extract attention maps from Vision Transformer\"\"\"\n",
    "    # Preprocess image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Forward pass with output_attentions=True\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Get attention maps [layers, heads, tokens, tokens]\n",
    "    attention_maps = outputs.attentions\n",
    "    \n",
    "    return attention_maps, inputs.pixel_values\n",
    "\n",
    "# Function to visualize attention maps\n",
    "def visualize_attention(attention_maps, image, layer, head, patch_size=16):\n",
    "    \"\"\"Visualize attention maps for a specific layer and attention head\"\"\"\n",
    "    # Get the specified attention map\n",
    "    attention = attention_maps[layer][0, head].detach().numpy()\n",
    "    \n",
    "    # The first token is the [CLS] token, so we exclude it for visualization\n",
    "    attention = attention[0, 1:]\n",
    "    \n",
    "    # Reshape attention to match image patches\n",
    "    num_patches = int(np.sqrt(attention.shape[0]))\n",
    "    attention_map = attention.reshape(num_patches, num_patches)\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Plot original image\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    # Create grid to show patches\n",
    "    h, w = image.size[1], image.size[0]\n",
    "    for i in range(0, h, patch_size):\n",
    "        axs[0].axhline(i, color='blue', lw=0.5, alpha=0.5)\n",
    "    for j in range(0, w, patch_size):\n",
    "        axs[0].axvline(j, color='blue', lw=0.5, alpha=0.5)\n",
    "    \n",
    "    # Highlight the focus patch (CLS token attends to this patch)\n",
    "    focus_i, focus_j = 0, 0  # By default, use the top-left patch\n",
    "    rect = plt.Rectangle((focus_j, focus_i), patch_size, patch_size,\n",
    "                         edgecolor='red', facecolor='none', lw=2)\n",
    "    axs[0].add_patch(rect)\n",
    "    \n",
    "    # Plot attention map\n",
    "    # Create a custom colormap with better contrast\n",
    "    cmap = LinearSegmentedColormap.from_list('attention_cmap', ['#0000ff', '#00ffff', '#ffff00', '#ff0000'])\n",
    "    im = axs[1].imshow(attention_map, cmap=cmap)\n",
    "    axs[1].set_title(f\"Attention Map (Layer {layer+1}, Head {head+1})\")\n",
    "    fig.colorbar(im, ax=axs[1], shrink=0.7, label='Attention Strength')\n",
    "    \n",
    "    # Add grid to match patches\n",
    "    axs[1].set_xticks(np.arange(-0.5, num_patches, 1), minor=True)\n",
    "    axs[1].set_yticks(np.arange(-0.5, num_patches, 1), minor=True)\n",
    "    axs[1].grid(which='minor', color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Interactive visualization function\n",
    "def interactive_attention_visualization(image_path):\n",
    "    \"\"\"Create interactive widgets to explore ViT attention maps\"\"\"\n",
    "    # Load model and processor\n",
    "    print(\"Loading Vision Transformer model...\")\n",
    "    model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "    \n",
    "    # Load image\n",
    "    image = load_image(image_path)\n",
    "    \n",
    "    # Get attention maps\n",
    "    print(\"Extracting attention maps...\")\n",
    "    attention_maps, _ = get_attention_maps(model, processor, image)\n",
    "    \n",
    "    # Number of layers and heads\n",
    "    num_layers = len(attention_maps)\n",
    "    num_heads = attention_maps[0].shape[1]\n",
    "    \n",
    "    print(f\"Model has {num_layers} layers with {num_heads} attention heads each\")\n",
    "    \n",
    "    # Create interactive widgets\n",
    "    layer_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=num_layers-1,\n",
    "        step=1,\n",
    "        description='Layer:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    head_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=num_heads-1,\n",
    "        step=1,\n",
    "        description='Head:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    # Create interactive visualization\n",
    "    interact(\n",
    "        lambda layer, head: visualize_attention(attention_maps, image, layer, head),\n",
    "        layer=layer_slider,\n",
    "        head=head_slider\n",
    "    )\n",
    "\n",
    "# Example usage - enable this to run the interactive visualization\n",
    "# interactive_attention_visualization('https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo/bobcat.jpg')\n",
    "\n",
    "print(\"Vision Transformer attention visualization code defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Jupyter AI to the Handbook\n",
    "\n",
    "To fully integrate Jupyter AI into the NeuroAI Handbook, we should follow these steps:\n",
    "\n",
    "1. **Installation Instructions**: Add clear instructions for users to install and configure Jupyter AI\n",
    "2. **Configuration Guide**: Provide guidance on setting up API keys for different models\n",
    "3. **Usage Examples**: Add examples for common NeuroAI tasks (like those shown above)\n",
    "4. **Integration with Exercises**: Enhance the existing exercises with AI-assisted options\n",
    "\n",
    "Let's create a mini-guide for users:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Jupyter AI with the NeuroAI Handbook\n",
    "\n",
    "Jupyter AI can significantly enhance your learning experience by providing:\n",
    "\n",
    "- On-demand explanations of complex neuroscience concepts\n",
    "- Code generation for implementing models discussed in the handbook\n",
    "- Help with debugging and improving your implementations\n",
    "- Support for visualizing neural data and model architectures\n",
    "\n",
    "### Quick Start Guide\n",
    "\n",
    "1. Install the required packages:\n",
    "   ```bash\n",
    "   pip install jupyter-ai jupyter-ai-magics openai\n",
    "   ```\n",
    "\n",
    "2. Configure your API keys (we recommend using environment variables):\n",
    "   ```bash\n",
    "   # For OpenAI models\n",
    "   export OPENAI_API_KEY=\"your-api-key\"\n",
    "   \n",
    "   # For Anthropic models\n",
    "   export ANTHROPIC_API_KEY=\"your-api-key\"\n",
    "   ```\n",
    "\n",
    "3. Load the extension in your notebook:\n",
    "   ```python\n",
    "   %load_ext jupyter_ai_magics\n",
    "   ```\n",
    "\n",
    "4. Use the `%%ai` magic to interact with AI models:\n",
    "   ```python\n",
    "   %%ai gpt-4\n",
    "   Explain how the PredNet architecture mimics predictive coding in the visual cortex.\n",
    "   ```\n",
    "\n",
    "### Recommended Use Cases\n",
    "\n",
    "1. **Code Assistance**: Get help implementing algorithms from the handbook\n",
    "2. **Concept Clarification**: Ask for detailed explanations of key concepts\n",
    "3. **Exploration**: Ask \"what if\" questions about model variations\n",
    "4. **Visualization**: Generate code for visualizing model architectures or neural data\n",
    "5. **Debugging**: Get help diagnosing and fixing issues in your implementations\n",
    "\n",
    "### Available Models\n",
    "\n",
    "- **OpenAI**: gpt-3.5-turbo, gpt-4\n",
    "- **Anthropic**: anthropic/claude-3-sonnet, anthropic/claude-3-opus\n",
    "- **Open Source**: Various models via Hugging Face\n",
    "\n",
    "For more detailed information, see the [Jupyter AI documentation](https://jupyter-ai.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Jupyter AI provides powerful AI-assisted features that can significantly enhance the interactive learning experience of the NeuroAI Handbook. By integrating these capabilities, readers can:\n",
    "\n",
    "1. Get personalized explanations of complex neuroscience and AI concepts\n",
    "2. Generate and debug code for implementing models discussed in the handbook\n",
    "3. Create visualizations to better understand neural data and model architectures\n",
    "4. Explore variations and extensions of the models presented in the case studies\n",
    "\n",
    "This integration supports a more dynamic, inquiry-based learning approach that can adapt to each reader's specific interests and questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}