# Chapter 7: Information Theory Essentials

## 7.0 Chapter Goals
- Master core concepts of information theory
- Apply information measures to neural data
- Understand efficient coding principles in the brain
- Implement information-theoretic analyses

## 7.1 Fundamentals of Information Theory
- Entropy and surprise
- Mutual information
- Kullback-Leibler divergence
- Channel capacity

## 7.2 Neural Coding & Efficiency
- Efficient coding hypothesis
- Redundancy reduction
- Sparse coding
- Predictive coding

## 7.3 Information Measures in Neuroscience
- Spike train information
- Neural decoding approaches
- Information bottleneck theory
- Representational similarity analysis

## 7.4 Noise, Variability & Information
- Signal vs noise in neural systems
- Stochastic resonance
- Population coding strategies
- Bayesian inference and uncertainty

## 7.5 Information Flow in Networks
- Directed information
- Transfer entropy
- Causal density
- Integrated information theory

## 7.6 Code Lab
- Computing entropy and mutual information
- Analyzing neural coding efficiency
- Information bottleneck demonstrations
- Transfer entropy in time series

## 7.7 Take-aways
- Information theory provides quantitative tools for neural analysis
- Efficient coding shapes neural representations
- Information flow characterizes network dynamics

## 7.8 Further Reading & Media
- Fairhall et al. (2001) - "Efficiency and ambiguity in an adaptive neural code"
- Timme & Lapish (2018) - "A Tutorial for Information Theory in Neuroscience"
- Tishby & Zaslavsky (2015) - "Deep learning and the information bottleneck principle"