
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 11: Sequence Models: RNN → Attention → Transformer &#8212; The Neuroscience of AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'part3/ch11_sequence_models';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="https://neuroai-handbook.github.io/part3/ch11_sequence_models.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 12: Large Language Models &amp; Fine-Tuning" href="../part4/ch12_large_language_models.html" />
    <link rel="prev" title="Chapter 10: Deep Learning: Training &amp; Optimisation" href="ch10_deep_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nai.png" class="logo__image only-light" alt="The Neuroscience of AI - Home"/>
    <script>document.write(`<img src="../_static/nai.png" class="logo__image only-dark" alt="The Neuroscience of AI - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    The Neuroscience of AI
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I · Brains &amp; Inspiration</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../part1/ch01_intro.html">Chapter 1: Introduction to Neuroscience ↔ AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part1/ch02_neuro_foundations.html">Chapter 2: Neuroscience Foundations for AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part1/ch03_spatial_navigation.html">Chapter 3: Spatial Navigation – Place &amp; Grid Cells</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part1/ch04_perception_pipeline.html">Chapter 4: Perception Pipeline – Visual Cortex → CNNs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II · Brains Meet Math &amp; Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../part2/ch05_brain_networks.html">Chapter 5: Default-Mode vs Executive Control Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part2/ch06_neurostimulation.html">Chapter 6: Neurostimulation &amp; Plasticity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part2/ch07_information_theory.html">Chapter 7: Information Theory Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part2/ch08_data_science_pipeline.html">Chapter 8: Data-Science Pipeline in Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III · Learning Machines</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ch09_ml_foundations.html">Chapter 9: Classical Machine-Learning Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch10_deep_learning.html">Chapter 10: Deep Learning: Training &amp; Optimisation</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 11: Sequence Models: RNN → Attention → Transformer</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV · Frontier Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../part4/ch12_large_language_models.html">Chapter 12: Large Language Models &amp; Fine-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part4/ch13_multimodal_models.html">Chapter 13: Multimodal &amp; Diffusion Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V · Ethics &amp; Futures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../part5/ch15_ethical_ai.html">Chapter 15: Ethical AI - Considerations for NeuroAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part5/ch16_future_directions.html">Chapter 16: Where Next for Neuro-AI?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part VI · Advanced Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../part6/ch17_bci_human_ai_interfaces.html">Brain-Computer Interfaces and Human-AI Interaction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch18_neuromorphic_computing.html">Chapter 18: Neuromorphic Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch19_cognitive_neuro_dl.html">Cognitive Neuroscience and Deep Learning</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../part6/ch20_case_studies.html">Case Studies in NeuroAI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../part6/ch20_interactive.html">Interactive NeuroAI Case Studies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../part6/jupyter_ai_demo.html">AI-Assisted Learning with Jupyter AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../part6/rise_slides_demo.html">Creating Presentations with RISE</a></li>


</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch21_ai_for_neuro_discovery.html">Chapter 21: AI for Neuroscience Discovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch22_embodied_ai_robotics.html">Chapter 22: Embodied AI and Robotics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch24_quantum_computing_neuroai.html">Chapter 24: Quantum Computing and NeuroAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch23_lifelong_learning.html">Chapter 23: Lifelong Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../appendices/glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendices/math_python_refresher.html">Appendix A: Math &amp; Python Mini-Refresher</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendices/dataset_catalogue.html">Appendix B: Dataset Catalogue</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendices/colab_setup.html">Appendix C: Google Colab Setup for NeuroAI</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/edit/master/docs/part3/ch11_sequence_models.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fpart3/ch11_sequence_models.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/part3/ch11_sequence_models.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 11: Sequence Models: RNN → Attention → Transformer</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-neural-networks">11.1 Recurrent Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vanilla-rnns">11.1.1 Vanilla RNNs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vanishing-exploding-gradient-problem">11.1.2 The Vanishing/Exploding Gradient Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-and-grus">11.1.3 LSTMs and GRUs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-architecture">LSTM Architecture</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gru-architecture">GRU Architecture</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bidirectional-rnns">11.1.4 Bidirectional RNNs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-in-neuroscience">11.1.5 Applications in Neuroscience</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanisms">11.2 Attention Mechanisms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-intuition-behind-attention">11.2.1 The Intuition Behind Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">11.2.2 Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">11.2.3 Multi-Head Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-vs-recurrence">11.2.4 Self-Attention vs. Recurrence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention">11.2.5 Scaled Dot-Product Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-visualization">11.2.6 Attention Visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-correlates-of-attention">11.2.7 Neural Correlates of Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-architecture">11.3 Transformer Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-architecture">11.3.1 Overall Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">11.3.2 Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">11.3.3 Decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings">11.3.4 Positional Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-networks">11.3.5 Feed-Forward Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connections-and-layer-normalization">11.3.6 Residual Connections and Layer Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#biological-parallels">11.3.7 Biological Parallels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-sequence-processing">11.4 Neural Sequence Processing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temporal-dynamics-in-cortical-circuits">11.4.1 Temporal Dynamics in Cortical Circuits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#working-memory-mechanisms">11.4.2 Working Memory Mechanisms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-processing">11.4.3 Predictive Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical-temporal-processing">11.4.4 Hierarchical Temporal Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparative-architecture-analysis">11.4.5 Comparative Architecture Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-directions">11.4.6 Future Directions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">11.5 Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#natural-language-processing">11.5.1 Natural Language Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-forecasting">11.5.2 Time Series Forecasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-sequence-decoding">11.5.3 Neural Sequence Decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-sequence-models">11.5.4 Generative Sequence Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application-design-principles">11.5.5 Application Design Principles</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-lab">11.6 Code Lab</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-an-lstm-from-components">11.6.1 Implementing an LSTM from Components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-self-attention-mechanism">11.6.2 Building a Self-Attention Mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-small-transformer">11.6.3 Training a Small Transformer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-sequence-prediction">11.6.4 Neural Sequence Prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-solutions">11.6.5 Exercise Solutions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#take-aways">11.7 Take-aways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading-media">11.8 Further Reading &amp; Media</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-papers">Key Papers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#foundational-papers">Foundational Papers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#neuroscience-connections">Neuroscience Connections</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#books">Books</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-resources">Online Resources</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tutorials-and-blog-posts">Tutorials and Blog Posts</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#video-lectures">Video Lectures</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#code-repositories">Code Repositories</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#research-communities-and-conferences">Research Communities and Conferences</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-11-sequence-models-rnn-attention-transformer">
<h1>Chapter 11: Sequence Models: RNN → Attention → Transformer<a class="headerlink" href="#chapter-11-sequence-models-rnn-attention-transformer" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Learning Objectives</p>
<p>By the end of this chapter, you will be able to:</p>
<ul class="simple">
<li><p><strong>Understand</strong> the evolution of sequence models from RNNs to Transformers</p></li>
<li><p><strong>Master</strong> the architectures and training methods for recurrent networks, attention mechanisms, and transformer models</p></li>
<li><p><strong>Connect</strong> sequence model operations to temporal processing in the brain</p></li>
<li><p><strong>Implement</strong> key sequence modeling architectures for various tasks</p></li>
<li><p><strong>Compare</strong> different approaches to handling sequential data</p></li>
</ul>
</div>
<section id="recurrent-neural-networks">
<h2>11.1 Recurrent Neural Networks<a class="headerlink" href="#recurrent-neural-networks" title="Link to this heading">#</a></h2>
<p>Recurrent Neural Networks (RNNs) are specialized neural networks designed to process sequential data by maintaining an internal state (memory) that captures information about previous inputs. Unlike feedforward networks, RNNs have connections that loop back on themselves, allowing them to persist information across time steps.</p>
<p><img alt="RNN Architectures" src="../_images/rnn_architecture.svg" />
<em>Figure 11.1: Recurrent Neural Network architectures, showing the basic RNN, LSTM, and GRU cells with their internal structures.</em></p>
<section id="vanilla-rnns">
<h3>11.1.1 Vanilla RNNs<a class="headerlink" href="#vanilla-rnns" title="Link to this heading">#</a></h3>
<p>The simplest RNN architecture maintains a hidden state that is updated at each time step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SimpleRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A basic RNN implementation.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            input_size: Size of input features at each time step</span>
<span class="sd">            hidden_size: Size of hidden state</span>
<span class="sd">            output_size: Size of output at each time step</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        
        <span class="c1"># Input to hidden weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i2h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Hidden to output weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h2o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        
        <span class="c1"># Activation function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through the RNN for a single time step.&quot;&quot;&quot;</span>
        <span class="c1"># Combine input and hidden state</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Calculate new hidden state</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">i2h</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        
        <span class="c1"># Calculate output</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h2o</span><span class="p">(</span><span class="n">hidden</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the hidden state with zeros.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">process_sequence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process a sequence of inputs and return all outputs and final hidden state.&quot;&quot;&quot;</span>
        <span class="c1"># Initialize hidden state</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">sequence</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        
        <span class="c1"># Storage for outputs at each time step</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Process each time step</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequence</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)):</span>
            <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">sequence</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">hidden</span><span class="p">)</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            
        <span class="c1"># Stack outputs along time dimension</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span>

<span class="c1"># Example usage on a toy sequence problem</span>
<span class="k">def</span><span class="w"> </span><span class="nf">simple_rnn_example</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate a simple RNN on a toy sequence task.&quot;&quot;&quot;</span>
    <span class="c1"># Create sample data: learning to recognize sequences ending with [1, 2, 3]</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate_sample</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="c1"># Generate random sequence of 0-4</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="n">length</span><span class="p">,))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        
        <span class="c1"># Check if the last three elements are [1, 2, 3]</span>
        <span class="n">target</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">sequence</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span> <span class="k">else</span> <span class="mi">0</span>
        
        <span class="c1"># One-hot encode the sequence</span>
        <span class="n">one_hot_sequence</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">one_hot_sequence</span><span class="p">,</span> <span class="n">target</span>
    
    <span class="c1"># Generate training data</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">generate_sample</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)]</span>
    
    <span class="c1"># Define model</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># One-hot encoding of 5 possible values</span>
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Binary classification</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    
    <span class="c1"># Loss function and optimizer</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    
    <span class="c1"># Training loop</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># Get batch</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
            <span class="n">sequences</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
            <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
            
            <span class="c1"># Pad sequences to the same length</span>
            <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">seq</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">)</span>
            <span class="n">padded_sequences</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">),</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sequences</span><span class="p">):</span>
                <span class="n">padded_sequences</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:</span><span class="n">seq</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">seq</span>
            
            <span class="c1"># Forward pass</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">hidden</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span>
            
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
                <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">padded_sequences</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:],</span> <span class="n">hidden</span><span class="p">)</span>
                <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            
            <span class="c1"># We only care about the output at the last time step</span>
            <span class="n">final_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="c1"># Calculate loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">final_output</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            
            <span class="c1"># Backward pass</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="c1"># Calculate accuracy</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">final_output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">pred</span> <span class="o">==</span> <span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="c1"># Record metrics</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">))</span>
        <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">))</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, Accuracy: </span><span class="si">{</span><span class="n">accuracies</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="c1"># Plot training progress</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training Accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>The RNN updates its hidden state at each time step according to:</p>
<div class="math notranslate nohighlight">
\[h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state at time <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> is the input at time <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(W_{hh}\)</span> is the hidden-to-hidden weights</p></li>
<li><p><span class="math notranslate nohighlight">\(W_{xh}\)</span> is the input-to-hidden weights</p></li>
<li><p><span class="math notranslate nohighlight">\(b_h\)</span> is the hidden bias</p></li>
</ul>
<p>The output at each time step is typically computed as:</p>
<div class="math notranslate nohighlight">
\[y_t = W_{hy} h_t + b_y\]</div>
<p><strong>Biological Parallel</strong>: RNNs resemble recurrent circuits in the brain where neural activity can persist and influence future processing. The prefrontal cortex maintains information over time through recurrent connections, similar to how RNNs maintain a hidden state.</p>
</section>
<section id="the-vanishing-exploding-gradient-problem">
<h3>11.1.2 The Vanishing/Exploding Gradient Problem<a class="headerlink" href="#the-vanishing-exploding-gradient-problem" title="Link to this heading">#</a></h3>
<p>Standard RNNs struggle with long-term dependencies due to the vanishing or exploding gradient problem. When backpropagating through many time steps, gradients can either:</p>
<ol class="arabic simple">
<li><p>Vanish - becoming extremely small, making learning long-range dependencies impossible</p></li>
<li><p>Explode - becoming extremely large, causing unstable training</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_gradient_problems</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize the vanishing gradient problem in RNNs.&quot;&quot;&quot;</span>
    <span class="c1"># Number of time steps</span>
    <span class="n">T</span> <span class="o">=</span> <span class="mi">100</span>
    
    <span class="c1"># Different values for recurrent weights</span>
    <span class="n">recurrent_weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">recurrent_weights</span><span class="p">:</span>
        <span class="c1"># Calculate gradient scaling factor at each time step</span>
        <span class="c1"># For simplicity, we model how the gradient scales based on the recurrent weight</span>
        <span class="n">gradient_scale</span> <span class="o">=</span> <span class="p">[</span><span class="n">weight</span> <span class="o">**</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)]</span>
        
        <span class="c1"># Plot on log scale</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">gradient_scale</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Weight = </span><span class="si">{</span><span class="n">weight</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time Steps Backward&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Gradient Scale Factor (log scale)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Vanishing/Exploding Gradients in RNNs&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>This problem occurs because during backpropagation through time, the gradient is multiplied by the recurrent weight matrix repeatedly, leading to exponential growth or decay.</p>
</section>
<section id="lstms-and-grus">
<h3>11.1.3 LSTMs and GRUs<a class="headerlink" href="#lstms-and-grus" title="Link to this heading">#</a></h3>
<p>To address the vanishing gradient problem, Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) were developed with gating mechanisms to control information flow.</p>
<section id="lstm-architecture">
<h4>LSTM Architecture<a class="headerlink" href="#lstm-architecture" title="Link to this heading">#</a></h4>
<p>LSTMs introduce a cell state and three gates:</p>
<ol class="arabic simple">
<li><p><strong>Forget Gate</strong>: Controls what information to throw away from the cell state</p></li>
<li><p><strong>Input Gate</strong>: Controls what new information to add to the cell state</p></li>
<li><p><strong>Output Gate</strong>: Controls what information from the cell state to output</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CustomLSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A custom LSTM implementation to demonstrate the internal mechanisms.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            input_size: Size of input features</span>
<span class="sd">            hidden_size: Size of hidden state and cell state</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CustomLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        
        <span class="c1"># Forget gate: determine what to remove from cell state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Input gate: determine what to add to cell state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Cell state candidate: new values to add to cell state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_candidate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Output gate: determine what to output from cell state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Activation functions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through the LSTM cell for a single time step.&quot;&quot;&quot;</span>
        <span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span> <span class="o">=</span> <span class="n">hidden</span>
        
        <span class="c1"># Combine input and previous hidden state</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Forget gate: what to forget from cell state</span>
        <span class="n">f_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forget_gate</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        
        <span class="c1"># Input gate: what new information to add</span>
        <span class="n">i_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_gate</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        
        <span class="c1"># Cell candidate: potential new values for cell state</span>
        <span class="n">c_tilde</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cell_candidate</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        
        <span class="c1"># Cell state update</span>
        <span class="n">c_t</span> <span class="o">=</span> <span class="n">f_t</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">c_tilde</span>
        
        <span class="c1"># Output gate: what to expose from cell state</span>
        <span class="n">o_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_gate</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        
        <span class="c1"># Hidden state update</span>
        <span class="n">h_t</span> <span class="o">=</span> <span class="n">o_t</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">(</span><span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span><span class="p">)</span>
</pre></div>
</div>
<p>The LSTM updates are governed by these equations:</p>
<ol class="arabic simple">
<li><p>Forget gate: <span class="math notranslate nohighlight">\(f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\)</span></p></li>
<li><p>Input gate: <span class="math notranslate nohighlight">\(i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)\)</span></p></li>
<li><p>Cell candidate: <span class="math notranslate nohighlight">\(\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)\)</span></p></li>
<li><p>Cell state update: <span class="math notranslate nohighlight">\(C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t\)</span></p></li>
<li><p>Output gate: <span class="math notranslate nohighlight">\(o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)\)</span></p></li>
<li><p>Hidden state update: <span class="math notranslate nohighlight">\(h_t = o_t \odot \tanh(C_t)\)</span></p></li>
</ol>
<p>Where <span class="math notranslate nohighlight">\(\odot\)</span> represents element-wise multiplication.</p>
</section>
<section id="gru-architecture">
<h4>GRU Architecture<a class="headerlink" href="#gru-architecture" title="Link to this heading">#</a></h4>
<p>Gated Recurrent Units (GRUs) are a simplified version of LSTMs with two gates:</p>
<ol class="arabic simple">
<li><p><strong>Reset Gate</strong>: Controls how much of the previous hidden state to use</p></li>
<li><p><strong>Update Gate</strong>: Controls how much of the new hidden state to use</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CustomGRU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A custom GRU implementation to demonstrate the internal mechanisms.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            input_size: Size of input features</span>
<span class="sd">            hidden_size: Size of hidden state</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CustomGRU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        
        <span class="c1"># Reset gate: determine how much of previous hidden state to use</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Update gate: determine how much to update the hidden state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Candidate hidden state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_candidate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Activation functions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through the GRU cell for a single time step.&quot;&quot;&quot;</span>
        <span class="c1"># Combine input and previous hidden state</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Reset gate: how much of the previous hidden state to use</span>
        <span class="n">r_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reset_gate</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        
        <span class="c1"># Update gate: how much to update the hidden state</span>
        <span class="n">z_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_gate</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        
        <span class="c1"># Combined input for candidate hidden state</span>
        <span class="n">reset_combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">r_t</span> <span class="o">*</span> <span class="n">h_prev</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Candidate hidden state</span>
        <span class="n">h_tilde</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_candidate</span><span class="p">(</span><span class="n">reset_combined</span><span class="p">))</span>
        
        <span class="c1"># Hidden state update</span>
        <span class="n">h_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">z_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">h_prev</span> <span class="o">+</span> <span class="n">z_t</span> <span class="o">*</span> <span class="n">h_tilde</span>
        
        <span class="k">return</span> <span class="n">h_t</span>
</pre></div>
</div>
<p>The GRU updates are governed by these equations:</p>
<ol class="arabic simple">
<li><p>Reset gate: <span class="math notranslate nohighlight">\(r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)\)</span></p></li>
<li><p>Update gate: <span class="math notranslate nohighlight">\(z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)\)</span></p></li>
<li><p>Candidate hidden: <span class="math notranslate nohighlight">\(\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)\)</span></p></li>
<li><p>Hidden state update: <span class="math notranslate nohighlight">\(h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t\)</span></p></li>
</ol>
<p><strong>Biological Parallel</strong>: The gating mechanisms in LSTMs and GRUs resemble neuromodulatory systems in the brain that regulate information flow. For example, dopamine can modulate which information is maintained in working memory, similar to how LSTM gates control what information is stored in the cell state.</p>
</section>
</section>
<section id="bidirectional-rnns">
<h3>11.1.4 Bidirectional RNNs<a class="headerlink" href="#bidirectional-rnns" title="Link to this heading">#</a></h3>
<p>In many sequence processing tasks, future context is just as important as past context. Bidirectional RNNs process the sequence in both directions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">BidirectionalRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A simple bidirectional RNN implementation.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            input_size: Size of input features</span>
<span class="sd">            hidden_size: Size of hidden state</span>
<span class="sd">            output_size: Size of output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        
        <span class="c1"># Forward RNN</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forward_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># Backward RNN</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backward_rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># Combined output layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Process sequence in both directions.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input sequence tensor of shape (batch_size, seq_len, input_size)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Forward pass</span>
        <span class="n">forward_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_rnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Backward pass (reverse the sequence)</span>
        <span class="n">reversed_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Reverse along sequence dimension</span>
        <span class="n">backward_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward_rnn</span><span class="p">(</span><span class="n">reversed_x</span><span class="p">)</span>
        <span class="n">backward_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">backward_out</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Flip back to match forward</span>
        
        <span class="c1"># Combine the two directions</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">forward_out</span><span class="p">,</span> <span class="n">backward_out</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Generate output</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>Bidirectional RNNs are particularly useful for tasks like speech recognition, machine translation, and named entity recognition, where the entire sequence is available during inference.</p>
<p><strong>Biological Parallel</strong>: The brain often uses both predictive and retrospective processing when interpreting sequences. For example, in language processing, later words in a sentence can change the interpretation of earlier words.</p>
</section>
<section id="applications-in-neuroscience">
<h3>11.1.5 Applications in Neuroscience<a class="headerlink" href="#applications-in-neuroscience" title="Link to this heading">#</a></h3>
<p>RNNs have been used extensively to model neural circuits and brain functions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_rnn_on_neural_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Example of using RNNs to model neural time series data.&quot;&quot;&quot;</span>
    <span class="c1"># This would typically involve:</span>
    <span class="c1"># 1. Loading neural recording data (e.g., spike trains or calcium imaging)</span>
    <span class="c1"># 2. Preprocessing into appropriate sequences</span>
    <span class="c1"># 3. Training an RNN to predict neural activity or behavior</span>
    
    <span class="c1"># Simulated neural data for demonstration</span>
    <span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">n_trials</span> <span class="o">=</span> <span class="mi">200</span>
    
    <span class="c1"># Simulated spike trains - binary activity patterns</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">neural_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="p">(</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">))</span>
    
    <span class="c1"># Add some structure - make neurons 0-10 active at time 30-40 with higher probability</span>
    <span class="n">neural_data</span><span class="p">[:,</span> <span class="mi">30</span><span class="p">:</span><span class="mi">40</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="p">(</span><span class="n">n_trials</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    
    <span class="c1"># Convert to tensor</span>
    <span class="n">neural_data_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">neural_data</span><span class="p">)</span>
    
    <span class="c1"># Define task: predict activity at t+1 from activity at t</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">neural_data_tensor</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># All timepoints except the last</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">neural_data_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>   <span class="c1"># All timepoints except the first</span>
    
    <span class="c1"># Split into train/test</span>
    <span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">n_trials</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">train_size</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">train_size</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>
    
    <span class="c1"># Define an RNN model (we&#39;ll use PyTorch&#39;s built-in GRU)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>  <span class="c1"># For binary prediction</span>
    <span class="p">)</span>
    
    <span class="c1"># Simplified diagram of RNN modeling neural circuits</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="c1"># Draw the neural data raster plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">neural_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Example Neural Activity&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Neuron&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw the RNN prediction schema</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Neural Data&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;RNN Prediction&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;RNN Modeling Neural Dynamics&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>In neuroscience, RNNs have been used to:</p>
<ol class="arabic simple">
<li><p>Model working memory in the prefrontal cortex</p></li>
<li><p>Simulate motor sequence learning in the basal ganglia</p></li>
<li><p>Capture dynamic responses in sensory cortices</p></li>
<li><p>Model decision-making processes in frontal areas</p></li>
</ol>
<p>The recurrent connectivity in these networks resembles the recurrent circuits found throughout the brain, making them natural models of neural dynamics.</p>
</section>
</section>
<section id="attention-mechanisms">
<h2>11.2 Attention Mechanisms<a class="headerlink" href="#attention-mechanisms" title="Link to this heading">#</a></h2>
<p>While RNNs excel at sequential processing, they struggle with long-range dependencies. Attention mechanisms address this limitation by allowing the model to focus on relevant parts of the input sequence when producing each output element, regardless of their distance.</p>
<p><img alt="Attention Mechanism" src="../_images/attention_mechanism.svg" />
<em>Figure 11.2: Attention mechanism architecture showing query, key, value operations and how attention weights are computed and applied.</em></p>
<section id="the-intuition-behind-attention">
<h3>11.2.1 The Intuition Behind Attention<a class="headerlink" href="#the-intuition-behind-attention" title="Link to this heading">#</a></h3>
<p>Attention mimics a cognitive process: when processing complex information, humans focus on relevant parts while ignoring irrelevant details. For example, when translating a long sentence, a human translator might focus on specific source words when generating each target word.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">attention_intuition</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize the intuition behind attention.&quot;&quot;&quot;</span>
    <span class="c1"># Create a simple sentence for visualization</span>
    <span class="n">source</span> <span class="o">=</span> <span class="s2">&quot;The small cat sleeps on the comfortable blue mat&quot;</span>
    <span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;Le petit chat dort sur le tapis bleu confortable&quot;</span>
    
    <span class="c1"># Split into words</span>
    <span class="n">source_words</span> <span class="o">=</span> <span class="n">source</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">target_words</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    
    <span class="c1"># Simulate attention weights (would normally be learned)</span>
    <span class="c1"># Each row corresponds to a target word, each column to a source word</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">target_words</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">source_words</span><span class="p">)))</span>
    
    <span class="c1"># Set attention based on word alignment (simplified for visualization)</span>
    <span class="n">alignments</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mi">0</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>       <span class="c1"># Le -&gt; The</span>
        <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>       <span class="c1"># petit -&gt; small</span>
        <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span>       <span class="c1"># chat -&gt; cat</span>
        <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span>       <span class="c1"># dort -&gt; sleeps</span>
        <span class="mi">4</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span>       <span class="c1"># sur -&gt; on</span>
        <span class="mi">5</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span>       <span class="c1"># le -&gt; the</span>
        <span class="mi">6</span><span class="p">:</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>    <span class="c1"># tapis bleu -&gt; blue mat</span>
        <span class="mi">7</span><span class="p">:</span> <span class="p">[</span><span class="mi">6</span><span class="p">]</span>        <span class="c1"># confortable -&gt; comfortable</span>
    <span class="p">}</span>
    
    <span class="c1"># Fill in attention weights</span>
    <span class="k">for</span> <span class="n">target_idx</span><span class="p">,</span> <span class="n">source_idxs</span> <span class="ow">in</span> <span class="n">alignments</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">source_idx</span> <span class="ow">in</span> <span class="n">source_idxs</span><span class="p">:</span>
            <span class="n">attention_weights</span><span class="p">[</span><span class="n">target_idx</span><span class="p">,</span> <span class="n">source_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">source_idxs</span><span class="p">)</span>
    
    <span class="c1"># Create visualization</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;YlOrRd&#39;</span><span class="p">)</span>
    
    <span class="c1"># Add labels</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">source_words</span><span class="p">)),</span> <span class="n">source_words</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">target_words</span><span class="p">)),</span> <span class="n">target_words</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Source (English)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Target (French)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Attention Weights in Translation&#39;</span><span class="p">)</span>
    
    <span class="c1"># Add a colorbar</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Attention Weight&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
</section>
<section id="self-attention">
<h3>11.2.2 Self-Attention<a class="headerlink" href="#self-attention" title="Link to this heading">#</a></h3>
<p>Self-attention allows a sequence to attend to itself, capturing dependencies between elements regardless of their distance. The key innovation is computing attention weights using queries and keys derived from the same sequence.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Self-attention mechanism.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_size: Dimensionality of input vectors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        
        <span class="c1"># Linear projections for query, key, and value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Scaling factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">hidden_size</span><span class="p">]))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply self-attention to input sequence.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor of shape [batch_size, seq_len, hidden_size]</span>
<span class="sd">            mask: Optional mask tensor of shape [batch_size, seq_len, seq_len]</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            attended: Output tensor after self-attention</span>
<span class="sd">            attention_weights: Attention weight matrix</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="c1"># Linear projections</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [batch_size, seq_len, hidden_size]</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># [batch_size, seq_len, hidden_size]</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [batch_size, seq_len, hidden_size]</span>
        
        <span class="c1"># Compute attention scores</span>
        <span class="c1"># q @ k.transpose(-2, -1) =&gt; [batch_size, seq_len, seq_len]</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        
        <span class="c1"># Apply mask if provided (useful for padding or causal attention)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e10</span><span class="p">)</span>
        
        <span class="c1"># Softmax to get attention weights</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Apply attention weights to values</span>
        <span class="n">attended</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">attended</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
<p>The self-attention operation is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q\)</span> (queries), <span class="math notranslate nohighlight">\(K\)</span> (keys), and <span class="math notranslate nohighlight">\(V\)</span> (values) are linear projections of the input</p></li>
<li><p><span class="math notranslate nohighlight">\(d_k\)</span> is the dimensionality of the key vectors</p></li>
<li><p>The scaling factor <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{d_k}}\)</span> prevents the softmax from reaching regions with extremely small gradients</p></li>
</ul>
<p><strong>Biological Parallel</strong>: Selective attention in the brain allows for focusing on relevant stimuli while suppressing irrelevant information. The thalamus and prefrontal cortex work together to control which information receives processing priority, similar to how attention weights prioritize certain parts of the input.</p>
</section>
<section id="multi-head-attention">
<h3>11.2.3 Multi-Head Attention<a class="headerlink" href="#multi-head-attention" title="Link to this heading">#</a></h3>
<p>Multi-head attention runs several attention mechanisms in parallel, allowing the model to jointly attend to information from different representation subspaces.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Multi-head attention mechanism.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_size: Dimensionality of input vectors</span>
<span class="sd">            num_heads: Number of attention heads</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="k">assert</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;hidden_size must be divisible by num_heads&quot;</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="c1"># Linear projections for query, key, and value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Output projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Scaling factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">]))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply multi-head attention.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            query: Query tensor [batch_size, query_len, hidden_size]</span>
<span class="sd">            key: Key tensor [batch_size, key_len, hidden_size]</span>
<span class="sd">            value: Value tensor [batch_size, key_len, hidden_size]</span>
<span class="sd">            mask: Optional mask tensor [batch_size, query_len, key_len]</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            attended: Output tensor after multi-head attention</span>
<span class="sd">            attention_weights: Attention weight tensor for each head</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Linear projections</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>  <span class="c1"># [batch_size, query_len, hidden_size]</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>      <span class="c1"># [batch_size, key_len, hidden_size]</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>  <span class="c1"># [batch_size, key_len, hidden_size]</span>
        
        <span class="c1"># Reshape for multi-head attention</span>
        <span class="c1"># [batch_size, seq_len, hidden_size] -&gt; [batch_size, seq_len, num_heads, head_dim]</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        
        <span class="c1"># Transpose to [batch_size, num_heads, seq_len, head_dim]</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Compute attention scores</span>
        <span class="c1"># [batch_size, num_heads, query_len, key_len]</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        
        <span class="c1"># Apply mask if provided</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Expand mask for multiple heads</span>
            <span class="c1"># [batch_size, query_len, key_len] -&gt; [batch_size, 1, query_len, key_len]</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e10</span><span class="p">)</span>
        
        <span class="c1"># Softmax to get attention weights</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Apply attention weights to values</span>
        <span class="c1"># [batch_size, num_heads, query_len, head_dim]</span>
        <span class="n">attended</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        
        <span class="c1"># Transpose and reshape back</span>
        <span class="c1"># [batch_size, num_heads, query_len, head_dim] -&gt; [batch_size, query_len, num_heads, head_dim]</span>
        <span class="n">attended</span> <span class="o">=</span> <span class="n">attended</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        
        <span class="c1"># [batch_size, query_len, hidden_size]</span>
        <span class="n">attended</span> <span class="o">=</span> <span class="n">attended</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Final linear projection</span>
        <span class="n">attended</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span><span class="p">(</span><span class="n">attended</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">attended</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
<p>Multi-head attention expands on the basic attention mechanism with multiple attention “heads” operating in parallel, each looking at different aspects of the data:</p>
<div class="math notranslate nohighlight">
\[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O\]</div>
<p>Where:</p>
<ul class="simple">
<li><p>Each <span class="math notranslate nohighlight">\(\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span></p></li>
<li><p>The <span class="math notranslate nohighlight">\(W\)</span> matrices are learned projection matrices</p></li>
<li><p><span class="math notranslate nohighlight">\(W^O\)</span> is the output projection</p></li>
</ul>
</section>
<section id="self-attention-vs-recurrence">
<h3>11.2.4 Self-Attention vs. Recurrence<a class="headerlink" href="#self-attention-vs-recurrence" title="Link to this heading">#</a></h3>
<p>Self-attention offers several advantages over recurrent networks:</p>
<ol class="arabic simple">
<li><p><strong>Parallelization</strong>: Unlike RNNs, which process sequences step-by-step, self-attention processes all sequence elements simultaneously.</p></li>
<li><p><strong>Long-range dependencies</strong>: Attention directly connects any two positions in the sequence, allowing for efficient modeling of long-range dependencies.</p></li>
<li><p><strong>Interpretability</strong>: Attention weights can be visualized to understand which input elements the model focuses on when generating each output.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compare_rnn_attention_complexity</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare computational complexity of RNNs vs. Attention&quot;&quot;&quot;</span>
    <span class="n">sequence_lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1001</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    
    <span class="c1"># Computational complexity</span>
    <span class="n">rnn_sequential_ops</span> <span class="o">=</span> <span class="n">sequence_lengths</span>  <span class="c1"># O(n) time steps for sequential processing</span>
    <span class="n">attention_parallel_ops</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">sequence_lengths</span><span class="p">)</span>  <span class="c1"># O(1) parallel processing</span>
    <span class="n">attention_memory_cost</span> <span class="o">=</span> <span class="n">sequence_lengths</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># O(n²) attention matrix</span>
    
    <span class="c1"># Plot comparison</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sequence_lengths</span><span class="p">,</span> <span class="n">rnn_sequential_ops</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;RNN (Sequential Steps)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sequence_lengths</span><span class="p">,</span> <span class="n">attention_parallel_ops</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Attention (Parallel)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sequence Length&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sequential Operations&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Computational Complexity (Time)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sequence_lengths</span><span class="p">,</span> <span class="n">attention_memory_cost</span><span class="p">,</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Attention Matrix Size&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sequence Length&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Memory Cost&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Memory Requirements&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
</section>
<section id="scaled-dot-product-attention">
<h3>11.2.5 Scaled Dot-Product Attention<a class="headerlink" href="#scaled-dot-product-attention" title="Link to this heading">#</a></h3>
<p>The core attention mechanism in modern architectures is scaled dot-product attention:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute scaled dot-product attention.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        query: Query tensor [batch_size, seq_len, d_k]</span>
<span class="sd">        key: Key tensor [batch_size, seq_len, d_k]</span>
<span class="sd">        value: Value tensor [batch_size, seq_len, d_v]</span>
<span class="sd">        mask: Optional mask tensor [batch_size, seq_len, seq_len]</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        output: Attended values</span>
<span class="sd">        attention: Attention weights</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Compute attention scores</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
    
    <span class="c1"># Apply mask if provided</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e10</span><span class="p">)</span>
    
    <span class="c1"># Apply softmax to get attention weights</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Apply attention weights to values</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention</span>
</pre></div>
</div>
</section>
<section id="attention-visualization">
<h3>11.2.6 Attention Visualization<a class="headerlink" href="#attention-visualization" title="Link to this heading">#</a></h3>
<p>Visualizing attention weights can provide insight into how the model processes sequences:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">visualize_attention</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a visualization of attention patterns.&quot;&quot;&quot;</span>
    <span class="c1"># Sample sentence</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;The quick brown fox jumps over the lazy dog&quot;</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    
    <span class="c1"># Create simulated attention matrices</span>
    <span class="n">num_words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    
    <span class="c1"># Self-attention: diagonal dominant (attends to self and nearby words)</span>
    <span class="n">self_attention</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_words</span><span class="p">,</span> <span class="n">num_words</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_words</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_words</span><span class="p">):</span>
            <span class="n">self_attention</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nb">abs</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">j</span><span class="p">))</span>
    
    <span class="c1"># Normalize rows</span>
    <span class="n">self_attention</span> <span class="o">=</span> <span class="n">self_attention</span> <span class="o">/</span> <span class="n">self_attention</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Subject-verb attention: highlights grammatical relationships</span>
    <span class="n">subj_verb_attention</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_words</span><span class="p">,</span> <span class="n">num_words</span><span class="p">))</span>
    <span class="n">subject_idx</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># &#39;fox&#39;</span>
    <span class="n">verb_idx</span> <span class="o">=</span> <span class="mi">4</span>     <span class="c1"># &#39;jumps&#39;</span>
    <span class="n">subj_verb_attention</span><span class="p">[</span><span class="n">subject_idx</span><span class="p">,</span> <span class="n">verb_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.7</span>
    <span class="n">subj_verb_attention</span><span class="p">[</span><span class="n">verb_idx</span><span class="p">,</span> <span class="n">subject_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.7</span>
    
    <span class="c1"># Fill in other relationships</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_words</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_words</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">subject_idx</span> <span class="ow">and</span> <span class="n">j</span> <span class="o">!=</span> <span class="n">verb_idx</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">verb_idx</span> <span class="ow">and</span> <span class="n">j</span> <span class="o">!=</span> <span class="n">subject_idx</span><span class="p">:</span>
                <span class="n">subj_verb_attention</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_words</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># Plot attention matrices</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="c1"># Plot self-attention</span>
    <span class="n">im1</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">self_attention</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;YlOrRd&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Self-Attention Pattern&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_words</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_words</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span> <span class="n">rotation_mode</span><span class="o">=</span><span class="s2">&quot;anchor&quot;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Plot subject-verb attention</span>
    <span class="n">im2</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">subj_verb_attention</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;YlOrRd&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Subject-Verb Attention&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_words</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_words</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span> <span class="n">rotation_mode</span><span class="o">=</span><span class="s2">&quot;anchor&quot;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="c1"># Add annotations</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_words</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_words</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">self_attention</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.2</span><span class="p">:</span>
                <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">self_attention</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> 
                             <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span> <span class="k">if</span> <span class="n">self_attention</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="s2">&quot;black&quot;</span><span class="p">)</span>
                
            <span class="k">if</span> <span class="n">subj_verb_attention</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.2</span><span class="p">:</span>
                <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">subj_verb_attention</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> 
                             <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span> <span class="k">if</span> <span class="n">subj_verb_attention</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="s2">&quot;black&quot;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
</section>
<section id="neural-correlates-of-attention">
<h3>11.2.7 Neural Correlates of Attention<a class="headerlink" href="#neural-correlates-of-attention" title="Link to this heading">#</a></h3>
<p>The attention mechanisms in deep learning have interesting parallels with attention systems in the brain:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">neural_attention_parallels</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate parallels between artificial and neural attention.&quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    
    <span class="c1"># Create a simple diagram</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Neural Attention in the Brain&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw brain regions involved in attention</span>
    <span class="n">circle1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="mf">0.15</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;#FFC78E&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;PFC&#39;</span><span class="p">)</span>
    <span class="n">circle2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;#8EADFC&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Thalamus&#39;</span><span class="p">)</span>
    <span class="n">circle3</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">),</span> <span class="mf">0.12</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;#8EFCB8&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Visual Cortex&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circle1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circle2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circle3</span><span class="p">)</span>
    
    <span class="c1"># Add labels</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;PFC&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;Thalamus&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;Visual</span><span class="se">\n</span><span class="s1">Cortex&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw connections</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    
    <span class="c1"># Add explanatory text</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s2">&quot;The prefrontal cortex (PFC) directs attention via the thalamus,</span><span class="se">\n</span><span class="s2">selectively enhancing processing in sensory areas&quot;</span><span class="p">,</span> 
             <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>
    
    <span class="c1"># Machine attention mechanism</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Artificial Attention Mechanism&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw components</span>
    <span class="n">rect1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">),</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;#FFC78E&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">rect2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">),</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;#8EADFC&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">rect3</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">),</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;#8EFCB8&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect3</span><span class="p">)</span>
    
    <span class="c1"># Add labels</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.275</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;Query&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.575</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;Attention</span><span class="se">\n</span><span class="s1">Weights&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.875</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw connections</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    
    <span class="c1"># Add explanatory text</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="s2">&quot;Neural network attention uses queries to compute weights</span><span class="se">\n</span><span class="s2">that determine which values are most relevant&quot;</span><span class="p">,</span> 
             <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>These parallels include:</p>
<ol class="arabic simple">
<li><p><strong>Selective Enhancement</strong>: Both neural and artificial attention selectively enhance processing of relevant information.</p></li>
<li><p><strong>Top-down Control</strong>: The prefrontal cortex provides top-down control in the brain, similar to how queries direct attention in artificial systems.</p></li>
<li><p><strong>Resource Allocation</strong>: Both systems efficiently allocate limited processing resources to the most important inputs.</p></li>
<li><p><strong>Context Integration</strong>: Both integrate contextual information to determine what’s relevant in the current situation.</p></li>
</ol>
</section>
</section>
<section id="transformer-architecture">
<h2>11.3 Transformer Architecture<a class="headerlink" href="#transformer-architecture" title="Link to this heading">#</a></h2>
<p>The Transformer architecture, introduced in the landmark paper “Attention Is All You Need” (Vaswani et al., 2017), revolutionized sequence processing by eliminating recurrence entirely and relying solely on attention mechanisms.</p>
<p><img alt="Transformer Architecture" src="../_images/transformer_architecture.svg" />
<em>Figure 11.3: The Transformer architecture featuring an encoder-decoder structure with multi-head attention, positional encodings, and feed-forward networks.</em></p>
<section id="overall-architecture">
<h3>11.3.1 Overall Architecture<a class="headerlink" href="#overall-architecture" title="Link to this heading">#</a></h3>
<p>The Transformer follows an encoder-decoder structure:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> 
                 <span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Full Transformer architecture for sequence-to-sequence tasks.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            src_vocab_size: Size of source vocabulary</span>
<span class="sd">            tgt_vocab_size: Size of target vocabulary</span>
<span class="sd">            d_model: Model dimension (embedding size)</span>
<span class="sd">            n_heads: Number of attention heads</span>
<span class="sd">            n_layers: Number of encoder/decoder layers</span>
<span class="sd">            d_ff: Hidden dimension in feed-forward networks</span>
<span class="sd">            max_seq_len: Maximum sequence length for positional encodings</span>
<span class="sd">            dropout: Dropout rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># Embeddings and positional encodings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_positional_encoding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        
        <span class="c1"># Encoder and decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TransformerDecoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        
        <span class="c1"># Final linear layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>
        
        <span class="c1"># Dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="c1"># Initialize parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_parameters</span><span class="p">()</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">create_positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create sinusoidal positional encodings.&quot;&quot;&quot;</span>
        <span class="c1"># Create a tensor for positions</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        
        <span class="c1"># Create a tensor for dimension indices</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        
        <span class="c1"># Create positional encoding</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">positions</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>  <span class="c1"># Even dimensions</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">positions</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>  <span class="c1"># Odd dimensions</span>
        
        <span class="c1"># Add batch dimension and register as buffer (not a parameter)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">pe</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">init_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize model parameters.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through the Transformer.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            src: Source sequence [batch_size, src_len]</span>
<span class="sd">            tgt: Target sequence [batch_size, tgt_len]</span>
<span class="sd">            src_mask: Mask for source self-attention</span>
<span class="sd">            tgt_mask: Mask for target self-attention (usually causal)</span>
<span class="sd">            src_padding_mask: Mask for source padding</span>
<span class="sd">            tgt_padding_mask: Mask for target padding</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            output: Vocabulary distributions [batch_size, tgt_len, tgt_vocab_size]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get sequence lengths</span>
        <span class="n">src_len</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">tgt_len</span> <span class="o">=</span> <span class="n">tgt</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Embed and add positional encoding</span>
        <span class="n">src_embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">src_embedded</span> <span class="o">=</span> <span class="n">src_embedded</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">src_len</span><span class="p">]</span>
        <span class="n">src_embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">src_embedded</span><span class="p">)</span>
        
        <span class="n">tgt_embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embedding</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">tgt_embedded</span> <span class="o">=</span> <span class="n">tgt_embedded</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">tgt_len</span><span class="p">]</span>
        <span class="n">tgt_embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">tgt_embedded</span><span class="p">)</span>
        
        <span class="c1"># Encoder pass</span>
        <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src_embedded</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">src_padding_mask</span><span class="p">)</span>
        
        <span class="c1"># Decoder pass</span>
        <span class="n">decoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">tgt_embedded</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">,</span> <span class="n">tgt_padding_mask</span><span class="p">,</span> <span class="n">src_padding_mask</span><span class="p">)</span>
        
        <span class="c1"># Final projection to vocabulary</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>The Transformer consists of two main components:</p>
<ol class="arabic simple">
<li><p><strong>Encoder</strong>: Processes the input sequence into a continuous representation</p></li>
<li><p><strong>Decoder</strong>: Generates the output sequence based on the encoder representation and previous outputs</p></li>
</ol>
</section>
<section id="encoder">
<h3>11.3.2 Encoder<a class="headerlink" href="#encoder" title="Link to this heading">#</a></h3>
<p>The encoder consists of N identical layers, each with two sub-layers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransformerEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Single Transformer encoder layer.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            d_model: Model dimension</span>
<span class="sd">            n_heads: Number of attention heads</span>
<span class="sd">            d_ff: Hidden dimension in feed-forward network</span>
<span class="sd">            dropout: Dropout rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># Multi-head self-attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        
        <span class="c1"># Feed-forward network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="c1"># Layer normalization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        
        <span class="c1"># Dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through encoder layer.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor [batch_size, seq_len, d_model]</span>
<span class="sd">            mask: Optional attention mask</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            x: Output tensor [batch_size, seq_len, d_model]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Self-attention sub-layer with residual connection and layer normalization</span>
        <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Feed-forward sub-layer with residual connection and layer normalization</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>The complete encoder stacks multiple encoder layers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Full Transformer encoder with N layers.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            d_model: Model dimension</span>
<span class="sd">            n_heads: Number of attention heads</span>
<span class="sd">            n_layers: Number of encoder layers</span>
<span class="sd">            d_ff: Hidden dimension in feed-forward network</span>
<span class="sd">            dropout: Dropout rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># Stack of encoder layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through the encoder.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor [batch_size, seq_len, d_model]</span>
<span class="sd">            mask: Self-attention mask</span>
<span class="sd">            padding_mask: Mask for padding tokens</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            x: Encoded representation [batch_size, seq_len, d_model]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Apply padding mask to attention mask if provided</span>
        <span class="k">if</span> <span class="n">padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">padding_mask</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">&amp;</span> <span class="n">padding_mask</span>
        
        <span class="c1"># Pass through each encoder layer</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="decoder">
<h3>11.3.3 Decoder<a class="headerlink" href="#decoder" title="Link to this heading">#</a></h3>
<p>The decoder is similar to the encoder but has an additional cross-attention layer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransformerDecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Single Transformer decoder layer.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            d_model: Model dimension</span>
<span class="sd">            n_heads: Number of attention heads</span>
<span class="sd">            d_ff: Hidden dimension in feed-forward network</span>
<span class="sd">            dropout: Dropout rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerDecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># Multi-head self-attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        
        <span class="c1"># Multi-head cross-attention to encoder outputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        
        <span class="c1"># Feed-forward network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="c1"># Layer normalization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        
        <span class="c1"># Dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through decoder layer.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor [batch_size, tgt_len, d_model]</span>
<span class="sd">            encoder_output: Output from encoder [batch_size, src_len, d_model]</span>
<span class="sd">            tgt_mask: Mask for target self-attention (usually causal)</span>
<span class="sd">            tgt_padding_mask: Mask for target padding</span>
<span class="sd">            src_padding_mask: Mask for source padding</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            x: Output tensor [batch_size, tgt_len, d_model]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Self-attention sub-layer with residual connection and layer normalization</span>
        <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Cross-attention sub-layer with residual connection and layer normalization</span>
        <span class="n">cross_attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">src_padding_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">cross_attn_output</span><span class="p">))</span>
        
        <span class="c1"># Feed-forward sub-layer with residual connection and layer normalization</span>
        <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>The complete decoder stacks multiple decoder layers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Full Transformer decoder with N layers.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            d_model: Model dimension</span>
<span class="sd">            n_heads: Number of attention heads</span>
<span class="sd">            n_layers: Number of decoder layers</span>
<span class="sd">            d_ff: Hidden dimension in feed-forward network</span>
<span class="sd">            dropout: Dropout rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># Stack of decoder layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">TransformerDecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through the decoder.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor [batch_size, tgt_len, d_model]</span>
<span class="sd">            encoder_output: Output from encoder [batch_size, src_len, d_model]</span>
<span class="sd">            tgt_mask: Mask for target self-attention (usually causal)</span>
<span class="sd">            tgt_padding_mask: Mask for target padding</span>
<span class="sd">            src_padding_mask: Mask for source padding</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            x: Decoded representation [batch_size, tgt_len, d_model]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Pass through each decoder layer</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">,</span> <span class="n">tgt_padding_mask</span><span class="p">,</span> <span class="n">src_padding_mask</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="positional-encodings">
<h3>11.3.4 Positional Encodings<a class="headerlink" href="#positional-encodings" title="Link to this heading">#</a></h3>
<p>Since the Transformer doesn’t use recurrence or convolution, it needs a way to incorporate sequence order. Positional encodings add positional information to the input embeddings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">create_positional_encodings</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create sinusoidal positional encodings.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        max_len: Maximum sequence length</span>
<span class="sd">        d_model: Model dimension</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        pos_encoding: Positional encoding tensor [1, max_len, d_model]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Create a tensor for positions</span>
    <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Create a tensor for dimension indices</span>
    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
    
    <span class="c1"># Create positional encoding</span>
    <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pos_encoding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">positions</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>  <span class="c1"># Even dimensions</span>
    <span class="n">pos_encoding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">positions</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>  <span class="c1"># Odd dimensions</span>
    
    <span class="c1"># Add batch dimension</span>
    <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">pos_encoding</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">pos_encoding</span>

<span class="k">def</span><span class="w"> </span><span class="nf">visualize_positional_encodings</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize positional encodings.&quot;&quot;&quot;</span>
    <span class="c1"># Create positional encodings</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">create_positional_encodings</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="c1"># Plot as a heatmap</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding Dimension&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Position in Sequence&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sinusoidal Positional Encodings&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    
    <span class="c1"># Plot a few dimensions across positions</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">63</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">127</span><span class="p">]:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="n">dim</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Dim </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Position&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Positional Encoding Values by Dimension&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>The positional encodings use sine and cosine functions of different frequencies:</p>
<div class="math notranslate nohighlight">
\[PE_{(pos, 2i)} = \sin\left(pos / 10000^{2i/d_{model}}\right)\]</div>
<div class="math notranslate nohighlight">
\[PE_{(pos, 2i+1)} = \cos\left(pos / 10000^{2i/d_{model}}\right)\]</div>
<p>This approach allows the model to easily learn to attend to relative positions since <span class="math notranslate nohighlight">\(PE_{pos+k}\)</span> can be represented as a linear function of <span class="math notranslate nohighlight">\(PE_{pos}\)</span>.</p>
</section>
<section id="feed-forward-networks">
<h3>11.3.5 Feed-Forward Networks<a class="headerlink" href="#feed-forward-networks" title="Link to this heading">#</a></h3>
<p>Each encoder and decoder layer contains a position-wise feed-forward network:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Position-wise feed-forward network.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            d_model: Model dimension</span>
<span class="sd">            d_ff: Hidden dimension</span>
<span class="sd">            dropout: Dropout rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionwiseFeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply feed-forward network to input.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor [batch_size, seq_len, d_model]</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            output: Transformed tensor [batch_size, seq_len, d_model]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>These networks apply two linear transformations with a ReLU activation in between:</p>
<div class="math notranslate nohighlight">
\[FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2\]</div>
<p>The feed-forward networks process each position independently, which is why they’re sometimes called “position-wise” feed-forward networks.</p>
</section>
<section id="residual-connections-and-layer-normalization">
<h3>11.3.6 Residual Connections and Layer Normalization<a class="headerlink" href="#residual-connections-and-layer-normalization" title="Link to this heading">#</a></h3>
<p>The Transformer uses residual connections around each sub-layer, followed by layer normalization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">AddNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Residual connection followed by layer normalization.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            size: Feature dimension</span>
<span class="sd">            dropout: Dropout rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AddNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sublayer_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply residual connection and layer normalization.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor</span>
<span class="sd">            sublayer_output: Output from sublayer</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            normalized: Normalized output with residual connection</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Add residual connection and normalize</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">sublayer_output</span><span class="p">))</span>
</pre></div>
</div>
<p>Layer normalization normalizes the inputs across the feature dimension, stabilizing the network’s activations:</p>
<div class="math notranslate nohighlight">
\[LayerNorm(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are the mean and standard deviation of the inputs</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learned parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant for numerical stability</p></li>
</ul>
</section>
<section id="biological-parallels">
<h3>11.3.7 Biological Parallels<a class="headerlink" href="#biological-parallels" title="Link to this heading">#</a></h3>
<p>The Transformer architecture has several interesting parallels with neural processing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">transformer_brain_parallels</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate parallels between Transformers and brain processing.&quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    
    <span class="c1"># Create a two-row comparison</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Parallel Processing in Transformers&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw Transformer components</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">x_pos</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mf">0.2</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">rectangle</span><span class="p">((</span><span class="n">x_pos</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;lightblue&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x_pos</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x_pos</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Token </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    
    <span class="c1"># Add attention illustration</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span> <span class="k">else</span> <span class="mf">0.8</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">0.1</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mf">0.2</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="mf">0.2</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    
    <span class="c1"># Second row for brain</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Distributed Processing in Brain Networks&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw brain regions</span>
    <span class="n">regions</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)]</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">regions</span><span class="p">):</span>
        <span class="n">circle</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;#FFC78E&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circle</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;R</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    
    <span class="c1"># Draw connections between regions</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">regions</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">regions</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="p">[</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    
    <span class="c1"># Add explanatory text</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s2">&quot;Both systems feature distributed parallel processing</span><span class="se">\n</span><span class="s2">with selective connections between elements&quot;</span><span class="p">,</span> 
             <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>Key parallels include:</p>
<ol class="arabic simple">
<li><p><strong>Parallel Processing</strong>: The brain processes information in parallel across multiple regions, similar to how Transformers process all sequence positions simultaneously.</p></li>
<li><p><strong>Selective Attention</strong>: Neural attention processes selectively enhance specific information paths, similar to attention mechanisms in Transformers.</p></li>
<li><p><strong>Hierarchical Processing</strong>: Both the brain and Transformers use hierarchical layers of processing, with higher levels building on lower-level representations.</p></li>
<li><p><strong>Distributed Representations</strong>: Neural processing involves distributed representations across populations of neurons, similar to the distributed embeddings in Transformers.</p></li>
</ol>
<p>However, these are high-level analogies rather than direct functional equivalents.</p>
</section>
</section>
<section id="neural-sequence-processing">
<h2>11.4 Neural Sequence Processing<a class="headerlink" href="#neural-sequence-processing" title="Link to this heading">#</a></h2>
<p>The brain is fundamentally a sequence processing system. From processing sensory streams to controlling motor behaviors, neural circuits specialize in handling temporally structured information. This section explores how biological sequence processing relates to artificial sequence models we’ve discussed.</p>
<p><img alt="Brain Sequence Processing" src="../_images/brain_sequence_processing.svg" />
<em>Figure 11.4: Comparison of sequence processing mechanisms in the brain and neural networks, highlighting temporal dynamics, working memory, and hierarchical processing.</em></p>
<section id="temporal-dynamics-in-cortical-circuits">
<h3>11.4.1 Temporal Dynamics in Cortical Circuits<a class="headerlink" href="#temporal-dynamics-in-cortical-circuits" title="Link to this heading">#</a></h3>
<p>Cortical circuits exhibit rich temporal dynamics that enable sequence processing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">simulate_cortical_dynamics</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simulate temporal dynamics in a recurrent cortical circuit.&quot;&quot;&quot;</span>
    <span class="c1"># Parameters</span>
    <span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">n_time</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="mi">10</span>       <span class="c1"># Time constant (ms)</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="mi">1</span>         <span class="c1"># Simulation time step (ms)</span>
    
    <span class="c1"># Create recurrent connection matrix (random but sparse)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.05</span>
    <span class="n">W</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Sparsity</span>
    
    <span class="c1"># Make the network stable by scaling connection weights</span>
    <span class="n">W</span> <span class="o">=</span> <span class="mf">0.95</span> <span class="o">*</span> <span class="n">W</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">W</span><span class="p">)))</span>
    
    <span class="c1"># Simulate network activity</span>
    <span class="n">activity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_time</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">))</span>
    
    <span class="c1"># Initial impulse to subset of neurons</span>
    <span class="n">activity</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">20</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
    
    <span class="c1"># Run simulation with Euler integration</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_time</span><span class="p">):</span>
        <span class="c1"># Update: dx/dt = -x/tau + W·x</span>
        <span class="n">activity</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">activity</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">dt</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">activity</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">tau</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">activity</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">W</span><span class="p">))</span>
        
        <span class="c1"># Add some noise</span>
        <span class="n">activity</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
    
    <span class="c1"># Visualize the dynamics</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    
    <span class="c1"># Plot full activity matrix</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">activity</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Activity&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (ms)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Neuron&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Temporal Dynamics in Recurrent Cortical Circuit&#39;</span><span class="p">)</span>
    
    <span class="c1"># Plot activity of selected neurons</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">activity</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">i</span><span class="o">*</span><span class="mf">0.2</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (ms)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Neuron Activity (offset for visibility)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Temporal Evolution of Neural Activity&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>Cortical circuits exhibit several key properties that support sequence processing:</p>
<ol class="arabic simple">
<li><p><strong>Persistent Activity</strong>: Recurrent connections enable activity to persist after stimulation ends, creating a form of working memory.</p></li>
<li><p><strong>Sequential Activation</strong>: Asymmetric connectivity can lead to waves of sequential neural activation, creating temporal patterns.</p></li>
<li><p><strong>Temporal Integration</strong>: Neurons integrate inputs over time, with different time constants for different cell types.</p></li>
<li><p><strong>Oscillatory Dynamics</strong>: Neural populations often display rhythmic activity patterns (theta, gamma oscillations) that provide temporal organization.</p></li>
</ol>
<p><strong>Connection to AI Models</strong>: These properties are analogous to the hidden state dynamics in RNNs. The time constant (τ) of biological neurons resembles the gating mechanisms in LSTMs that control information flow over time.</p>
</section>
<section id="working-memory-mechanisms">
<h3>11.4.2 Working Memory Mechanisms<a class="headerlink" href="#working-memory-mechanisms" title="Link to this heading">#</a></h3>
<p>The brain maintains and manipulates sequential information through working memory systems:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">NeuralWorkingMemoryModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A model of prefrontal cortex working memory inspired by biological mechanisms.&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">memory_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralWorkingMemoryModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span> <span class="o">=</span> <span class="n">memory_size</span>
        
        <span class="c1"># Input processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">memory_size</span><span class="p">)</span>
        
        <span class="c1"># Maintenance mechanism (recurrent connections)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maintenance_cell</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRUCell</span><span class="p">(</span><span class="n">memory_size</span><span class="p">,</span> <span class="n">memory_size</span><span class="p">)</span>
        
        <span class="c1"># Gating mechanisms (inspired by PFC-basal ganglia loops)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">memory_size</span><span class="p">,</span> <span class="n">memory_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">prev_memory</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Process a single timestep.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Current input [batch_size, input_size]</span>
<span class="sd">            prev_memory: Previous memory state [batch_size, memory_size]</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            memory: Updated memory state</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Determine update proportion using gate</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">prev_memory</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">update_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_gate</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        
        <span class="c1"># Process input</span>
        <span class="n">input_repr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        
        <span class="c1"># Maintain previous memory through recurrent connections</span>
        <span class="n">maintained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maintenance_cell</span><span class="p">(</span><span class="n">prev_memory</span><span class="p">,</span> <span class="n">prev_memory</span><span class="p">)</span>
        
        <span class="c1"># Gated update of memory</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">update_weight</span><span class="p">)</span> <span class="o">*</span> <span class="n">maintained</span> <span class="o">+</span> <span class="n">update_weight</span> <span class="o">*</span> <span class="n">input_repr</span>
        
        <span class="k">return</span> <span class="n">memory</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">simulate_wm_task</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence_length</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simulate a delayed match-to-sample working memory task.&quot;&quot;&quot;</span>
        <span class="c1"># Initialize memory</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span><span class="p">)</span>
        
        <span class="c1"># Storage for memory states over time</span>
        <span class="n">memory_states</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Create sample input sequence (one item to remember, then distractors)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">)</span>
        
        <span class="c1"># Target item at first position</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span>
        
        <span class="c1"># Distractors at other positions</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_length</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">)</span>
        
        <span class="c1"># Probe at last position (50% match, 50% non-match)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span>  <span class="c1"># Match</span>
            <span class="n">is_match</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">)</span>  <span class="c1"># Non-match</span>
            <span class="n">is_match</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="c1"># Process sequence</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">):</span>
            <span class="n">memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">memory</span><span class="p">)</span>
            <span class="n">memory_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">memory</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        
        <span class="c1"># Stack memory states over time</span>
        <span class="n">memory_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">memory_states</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">memory_states</span><span class="p">,</span> <span class="n">is_match</span>
</pre></div>
</div>
<p>The prefrontal cortex (PFC) implements working memory through:</p>
<ol class="arabic simple">
<li><p><strong>Persistent Neural Activity</strong>: Sustained firing in PFC neurons maintains information over delays.</p></li>
<li><p><strong>Selective Gating</strong>: Basal ganglia circuits control what information enters working memory, similar to the gates in LSTMs.</p></li>
<li><p><strong>Dynamic Coding</strong>: Working memory representations evolve over time while maintaining task-relevant information.</p></li>
<li><p><strong>Capacity Limits</strong>: Neural working memory has limited capacity, requiring filtering mechanisms.</p></li>
</ol>
<p><strong>Connection to AI Models</strong>: These properties align with gated recurrent networks. The maintenance cell in LSTMs resembles persistent activity in prefrontal neurons, while the gates mirror the selective filtering functions of basal ganglia circuits.</p>
</section>
<section id="predictive-processing">
<h3>11.4.3 Predictive Processing<a class="headerlink" href="#predictive-processing" title="Link to this heading">#</a></h3>
<p>The brain actively predicts upcoming sensory inputs and actions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_predictive_coding</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simulate predictive coding in sensory processing.&quot;&quot;&quot;</span>
    <span class="c1"># Parameters</span>
    <span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">100</span>
    
    <span class="c1"># Create a predictable pattern with occasional violations</span>
    <span class="n">pattern_length</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">base_pattern</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">pattern_length</span><span class="p">))</span>
    
    <span class="c1"># Repeat the pattern with occasional violations</span>
    <span class="n">repeats</span> <span class="o">=</span> <span class="n">n_timesteps</span> <span class="o">//</span> <span class="n">pattern_length</span>
    <span class="n">stimulus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">base_pattern</span><span class="p">,</span> <span class="n">repeats</span><span class="p">)</span>
    
    <span class="c1"># Add violations (pattern breaks)</span>
    <span class="n">violation_points</span> <span class="o">=</span> <span class="p">[</span><span class="mi">35</span><span class="p">,</span> <span class="mi">75</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">vp</span> <span class="ow">in</span> <span class="n">violation_points</span><span class="p">:</span>
        <span class="n">stimulus</span><span class="p">[</span><span class="n">vp</span><span class="p">:</span><span class="n">vp</span><span class="o">+</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">stimulus</span><span class="p">[</span><span class="n">vp</span><span class="p">:</span><span class="n">vp</span><span class="o">+</span><span class="mi">5</span><span class="p">]</span>  <span class="c1"># Invert the pattern</span>
    
    <span class="c1"># Simulate predictive network</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">stimulus</span><span class="p">)</span>
    <span class="n">prediction_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">stimulus</span><span class="p">)</span>
    
    <span class="c1"># Simple prediction: next value is previous value (for illustration)</span>
    <span class="n">predictions</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">stimulus</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># Calculate prediction errors</span>
    <span class="n">prediction_errors</span> <span class="o">=</span> <span class="n">stimulus</span> <span class="o">-</span> <span class="n">predictions</span>
    
    <span class="c1"># Visualize</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
    
    <span class="c1"># Plot stimulus</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">stimulus</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">vp</span> <span class="ow">in</span> <span class="n">violation_points</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="n">vp</span><span class="p">,</span> <span class="n">vp</span><span class="o">+</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sensory Input Signal&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Amplitude&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Plot predictions</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">vp</span> <span class="ow">in</span> <span class="n">violation_points</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="n">vp</span><span class="p">,</span> <span class="n">vp</span><span class="o">+</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Neural Predictions&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Amplitude&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Plot prediction errors</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">prediction_errors</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">vp</span> <span class="ow">in</span> <span class="n">violation_points</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="n">vp</span><span class="p">,</span> <span class="n">vp</span><span class="o">+</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Prediction Errors&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Error Magnitude&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>Predictive processing is a fundamental principle of neural computation:</p>
<ol class="arabic simple">
<li><p><strong>Predictive Coding</strong>: The brain generates predictions about upcoming sensory inputs based on learned internal models.</p></li>
<li><p><strong>Error Signaling</strong>: Prediction errors (differences between expectations and actual inputs) drive learning and updating of internal models.</p></li>
<li><p><strong>Hierarchical Predictions</strong>: Higher brain areas generate predictions for lower areas in a cascade of top-down influences.</p></li>
<li><p><strong>Temporal Prediction</strong>: The brain anticipates not just what will happen but when it will happen, encoding temporal expectations.</p></li>
</ol>
<p><strong>Connection to AI Models</strong>: These mechanisms relate to sequence models like RNNs and transformers that learn to predict the next element in a sequence. Language models are fundamentally prediction systems, similar to the brain’s predictive processing architecture.</p>
</section>
<section id="hierarchical-temporal-processing">
<h3>11.4.4 Hierarchical Temporal Processing<a class="headerlink" href="#hierarchical-temporal-processing" title="Link to this heading">#</a></h3>
<p>The brain processes temporal information at multiple timescales in a hierarchical manner:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">visualize_hierarchical_processing</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize hierarchical temporal processing in the brain and neural networks.&quot;&quot;&quot;</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    
    <span class="c1"># Brain hierarchy</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Hierarchical Temporal Processing in the Brain&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw brain regions</span>
    <span class="n">regions</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Primary Sensory</span><span class="se">\n</span><span class="s1">(ms timescale)&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">:</span> <span class="mf">1.5</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;#FFCCCC&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Secondary Sensory</span><span class="se">\n</span><span class="s1">(10s-100s ms)&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;#FFE5CC&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Association Cortex</span><span class="se">\n</span><span class="s1">(seconds)&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">:</span> <span class="mf">2.5</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;#FFFFCC&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;PFC/Hippocampus</span><span class="se">\n</span><span class="s1">(minutes-hours)&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;#E5FFCC&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Default Mode Network</span><span class="se">\n</span><span class="s1">(days-years)&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">:</span> <span class="mf">3.5</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;#CCE5FF&#39;</span><span class="p">}</span>
    <span class="p">]</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">regions</span><span class="p">):</span>
        <span class="n">rect</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mi">5</span><span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;width&#39;</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">r</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]),</span> <span class="n">r</span><span class="p">[</span><span class="s1">&#39;width&#39;</span><span class="p">],</span> <span class="mf">0.7</span><span class="p">,</span> 
                             <span class="n">facecolor</span><span class="o">=</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;color&#39;</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">r</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">+</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">r</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">],</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
        
        <span class="c1"># Draw connections</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">prev_r</span> <span class="o">=</span> <span class="n">regions</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">prev_r</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">+</span><span class="mf">0.7</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">r</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">prev_r</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> 
                     <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    
    <span class="c1"># Neural network hierarchy</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Hierarchical Processing in Neural Sequence Models&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw network layers</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Input Layer</span><span class="se">\n</span><span class="s1">(Character/Token)&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">:</span> <span class="mf">1.5</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;#FFCCCC&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Lower Layers</span><span class="se">\n</span><span class="s1">(Syntax, Local Patterns)&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;#FFE5CC&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Middle Layers</span><span class="se">\n</span><span class="s1">(Semantics, Phrases)&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">:</span> <span class="mf">2.5</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;#FFFFCC&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Upper Layers</span><span class="se">\n</span><span class="s1">(Context, Discourse)&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;#E5FFCC&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Output Layer</span><span class="se">\n</span><span class="s1">(Predictions, Generation)&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">:</span> <span class="mf">3.5</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;#CCE5FF&#39;</span><span class="p">}</span>
    <span class="p">]</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
        <span class="n">rect</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mi">5</span><span class="o">-</span><span class="n">l</span><span class="p">[</span><span class="s1">&#39;width&#39;</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]),</span> <span class="n">l</span><span class="p">[</span><span class="s1">&#39;width&#39;</span><span class="p">],</span> <span class="mf">0.7</span><span class="p">,</span> 
                             <span class="n">facecolor</span><span class="o">=</span><span class="n">l</span><span class="p">[</span><span class="s1">&#39;color&#39;</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">+</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">],</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
        
        <span class="c1"># Draw connections</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">prev_l</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">prev_l</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">+</span><span class="mf">0.7</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">prev_l</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> 
                     <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>
</pre></div>
</div>
<p>The brain’s temporal processing follows a hierarchical organization:</p>
<ol class="arabic simple">
<li><p><strong>Temporal Integration Windows</strong>: Different brain regions operate at different timescales:</p>
<ul class="simple">
<li><p>Primary sensory areas: Millisecond timescale</p></li>
<li><p>Secondary areas: Tens to hundreds of milliseconds</p></li>
<li><p>Association areas: Seconds</p></li>
<li><p>Prefrontal cortex: Minutes to hours</p></li>
<li><p>Default mode network: Days to years</p></li>
</ul>
</li>
<li><p><strong>Abstraction Hierarchy</strong>: Higher brain areas extract increasingly abstract temporal patterns from the input.</p></li>
<li><p><strong>Temporal Receptive Fields</strong>: Similar to spatial receptive fields, neurons have temporal receptive fields spanning different durations.</p></li>
<li><p><strong>Nested Oscillations</strong>: Neural oscillations form a nested hierarchy (theta, alpha, beta, gamma) that helps organize temporal processing.</p></li>
</ol>
<p><strong>Connection to AI Models</strong>: This hierarchy parallels how transformer models process sequences:</p>
<ul class="simple">
<li><p>Lower layers capture local patterns and syntax</p></li>
<li><p>Middle layers process semantic relationships</p></li>
<li><p>Upper layers integrate broader context and discourse information</p></li>
</ul>
<p>The attention span in different transformer layers resembles the temporal integration windows in the cortical hierarchy.</p>
</section>
<section id="comparative-architecture-analysis">
<h3>11.4.5 Comparative Architecture Analysis<a class="headerlink" href="#comparative-architecture-analysis" title="Link to this heading">#</a></h3>
<p>We can directly compare sequence processing in neural networks and biological systems:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compare_neural_and_artificial_sequence_models</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a table comparing biological and artificial sequence processing.&quot;&quot;&quot;</span>
    <span class="c1"># Create figure and axis</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Data for the table</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span> <span class="s1">&#39;Biological Systems&#39;</span><span class="p">,</span> <span class="s1">&#39;RNNs&#39;</span><span class="p">,</span> <span class="s1">&#39;Transformers&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;Processing</span><span class="se">\n</span><span class="s1">Architecture&#39;</span><span class="p">,</span> <span class="s1">&#39;Recurrent circuits</span><span class="se">\n</span><span class="s1">with lateral connections&#39;</span><span class="p">,</span> <span class="s1">&#39;Sequential processing</span><span class="se">\n</span><span class="s1">with recurrent state&#39;</span><span class="p">,</span> <span class="s1">&#39;Parallel processing</span><span class="se">\n</span><span class="s1">with attention&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;Information</span><span class="se">\n</span><span class="s1">Storage&#39;</span><span class="p">,</span> <span class="s1">&#39;Persistent activity and</span><span class="se">\n</span><span class="s1">synaptic changes&#39;</span><span class="p">,</span> <span class="s1">&#39;Hidden state vectors&#39;</span><span class="p">,</span> <span class="s1">&#39;Self-attention patterns&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;Temporal</span><span class="se">\n</span><span class="s1">Range&#39;</span><span class="p">,</span> <span class="s1">&#39;Multiple timescales across</span><span class="se">\n</span><span class="s1">brain hierarchy&#39;</span><span class="p">,</span> <span class="s1">&#39;Limited by vanishing</span><span class="se">\n</span><span class="s1">gradients&#39;</span><span class="p">,</span> <span class="s1">&#39;Full sequence</span><span class="se">\n</span><span class="s1">visibility&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;Parallel</span><span class="se">\n</span><span class="s1">Processing&#39;</span><span class="p">,</span> <span class="s1">&#39;Massively parallel&#39;</span><span class="p">,</span> <span class="s1">&#39;Limited (sequential)&#39;</span><span class="p">,</span> <span class="s1">&#39;Highly parallel&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;Modularity&#39;</span><span class="p">,</span> <span class="s1">&#39;Specialized regions</span><span class="se">\n</span><span class="s1">and pathways&#39;</span><span class="p">,</span> <span class="s1">&#39;Specialized gates</span><span class="se">\n</span><span class="s1">(LSTM, GRU)&#39;</span><span class="p">,</span> <span class="s1">&#39;Multi-head attention</span><span class="se">\n</span><span class="s1">for different patterns&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;Computational</span><span class="se">\n</span><span class="s1">Cost&#39;</span><span class="p">,</span> <span class="s1">&#39;Energy efficient&#39;</span><span class="p">,</span> <span class="s1">&#39;Low computation</span><span class="se">\n</span><span class="s1">High latency&#39;</span><span class="p">,</span> <span class="s1">&#39;High computation</span><span class="se">\n</span><span class="s1">Low latency&#39;</span><span class="p">],</span>
        <span class="p">[</span><span class="s1">&#39;Developmental</span><span class="se">\n</span><span class="s1">Trajectory&#39;</span><span class="p">,</span> <span class="s1">&#39;Progressive specialization</span><span class="se">\n</span><span class="s1">through experience&#39;</span><span class="p">,</span> <span class="s1">&#39;Fixed architecture</span><span class="se">\n</span><span class="s1">after training&#39;</span><span class="p">,</span> <span class="s1">&#39;Fixed architecture</span><span class="se">\n</span><span class="s1">after training&#39;</span><span class="p">],</span>
    <span class="p">]</span>
    
    <span class="c1"># Create table</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">table</span><span class="p">(</span>
        <span class="n">cellText</span><span class="o">=</span><span class="n">rows</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
        <span class="n">colLabels</span><span class="o">=</span><span class="n">rows</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
        <span class="n">cellLoc</span><span class="o">=</span><span class="s1">&#39;center&#39;</span>
    <span class="p">)</span>
    
    <span class="c1"># Style the table</span>
    <span class="n">table</span><span class="o">.</span><span class="n">auto_set_font_size</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">table</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">table</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">)</span>
    
    <span class="c1"># Color the header row</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">cell</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">_cells</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="p">)]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rows</span><span class="p">[</span><span class="mi">0</span><span class="p">]))):</span>
        <span class="n">cell</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s1">&#39;#4C72B0&#39;</span><span class="p">)</span>
        <span class="n">cell</span><span class="o">.</span><span class="n">set_text_props</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
    
    <span class="c1"># Alternate row colors for readability</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">rows</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rows</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
            <span class="n">cell</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">_cells</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)]</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s1">&#39;#F4F4F4&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Comparison of Sequence Processing in Biological and Artificial Systems&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">fig</span>
</pre></div>
</div>
<p>This comparison highlights how artificial sequence models have both converged with and diverged from biological sequence processing mechanisms.</p>
</section>
<section id="future-directions">
<h3>11.4.6 Future Directions<a class="headerlink" href="#future-directions" title="Link to this heading">#</a></h3>
<p>The future of neural sequence models may involve greater inspiration from neuroscience:</p>
<ol class="arabic simple">
<li><p><strong>Adaptive Timescales</strong>: Models with dynamic time constants that adapt to input statistics, similar to sensory adaptation in the brain.</p></li>
<li><p><strong>Predictive Learning</strong>: Self-supervised architectures that learn by predicting future inputs, mimicking the brain’s predictive processing.</p></li>
<li><p><strong>Memory-Attention Integration</strong>: Hybrid models combining the strengths of memory-based systems (like hippocampus) and attention-based systems (like working memory).</p></li>
<li><p><strong>Hierarchical Temporal Abstraction</strong>: Models that explicitly represent information at multiple timescales, similar to the cortical hierarchy.</p></li>
<li><p><strong>Energy-Efficient Processing</strong>: Sparse, event-driven computation inspired by the brain’s efficient processing mechanisms.</p></li>
</ol>
<p>The bidirectional inspiration between neuroscience and AI will continue to drive innovations in sequence modeling, with each field informing the other.</p>
</section>
</section>
<section id="applications">
<h2>11.5 Applications<a class="headerlink" href="#applications" title="Link to this heading">#</a></h2>
<p>Sequence models have transformed numerous fields by enabling machines to process and generate sequential data. This section explores key applications that bridge computational neuroscience and artificial intelligence.</p>
<section id="natural-language-processing">
<h3>11.5.1 Natural Language Processing<a class="headerlink" href="#natural-language-processing" title="Link to this heading">#</a></h3>
<p>Language is perhaps the most prominent application of sequence models, with transformers revolutionizing the field:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_language_model</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate a simple language model application.&quot;&quot;&quot;</span>
    <span class="c1"># Sample text</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;The brain processes language through a hierarchical network. Areas like Broca&#39;s and Wernicke&#39;s regions coordinate to understand and produce speech.&quot;</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    
    <span class="c1"># Create vocabulary and word-to-index mapping</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
    <span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="n">idx2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    
    <span class="c1"># Prepare input sequences and targets for next-word prediction</span>
    <span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">input_sequences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">-</span> <span class="n">sequence_length</span><span class="p">):</span>
        <span class="n">input_seq</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">sequence_length</span><span class="p">]</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">sequence_length</span><span class="p">]</span>
        
        <span class="n">input_sequences</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">input_seq</span><span class="p">])</span>
        <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word2idx</span><span class="p">[</span><span class="n">target</span><span class="p">])</span>
    
    <span class="c1"># Convert to tensors</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">input_sequences</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
    
    <span class="c1"># Define a simple RNN language model</span>
    <span class="k">class</span><span class="w"> </span><span class="nc">SimpleLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">SimpleLanguageModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
            
        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="c1"># x shape: [batch_size, sequence_length]</span>
            <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [batch_size, sequence_length, embedding_dim]</span>
            <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>  <span class="c1"># output: [batch_size, sequence_length, hidden_dim]</span>
            
            <span class="c1"># We only care about the final time step for next word prediction</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># [batch_size, vocab_size]</span>
            <span class="k">return</span> <span class="n">prediction</span>
    
    <span class="c1"># Example of model instantiation</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
    <span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleLanguageModel</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
    
    <span class="c1"># Visualize the language modeling process</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw the input sequence and target visualization</span>
    <span class="n">example_idx</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># Choose an example to visualize</span>
    <span class="n">input_seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx2word</span><span class="p">[</span><span class="n">idx</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">X</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]]</span>
    <span class="n">target_word</span> <span class="o">=</span> <span class="n">idx2word</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
    
    <span class="c1"># Create text explanation</span>
    <span class="n">explanation</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Input Sequence: </span><span class="se">\&quot;</span><span class="si">{</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span><span class="si">}</span><span class="se">\&quot;\n</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Target Word: </span><span class="se">\&quot;</span><span class="si">{</span><span class="n">target_word</span><span class="si">}</span><span class="se">\&quot;\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;Language models learn to predict the next word given a context.</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;Neural networks for language processing parallel the brain&#39;s hierarchical language system:</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Word embeddings → Semantic representations in temporal lobe</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Sequential processing → Left-hemisphere language pathways</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Prediction mechanisms → Predictive processing in auditory cortex</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Contextual integration → Working memory in prefrontal cortex&quot;</span>
    <span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">explanation</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
             <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;lightyellow&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round,pad=1&#39;</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>Key innovations in language models and their neuroscience connections include:</p>
<ol class="arabic simple">
<li><p><strong>Word Embeddings</strong>: Neural representations that capture semantic relationships between words, analogous to distributed semantic representations in the temporal lobe.</p></li>
<li><p><strong>Contextual Processing</strong>: Modern language models like BERT and GPT use context to disambiguate words, similar to the brain’s use of context in language comprehension.</p></li>
<li><p><strong>Syntactic Structure</strong>: Models implicitly learn syntactic dependencies, mirroring the brain’s left-hemisphere language pathways.</p></li>
<li><p><strong>Prediction and Surprisal</strong>: Language models predict upcoming words, just as the brain’s auditory cortex generates predictions during speech processing.</p></li>
</ol>
</section>
<section id="time-series-forecasting">
<h3>11.5.2 Time Series Forecasting<a class="headerlink" href="#time-series-forecasting" title="Link to this heading">#</a></h3>
<p>Sequence models excel at forecasting future values in time series data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_time_series_forecasting</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show time series forecasting with sequence models.&quot;&quot;&quot;</span>
    <span class="c1"># Generate synthetic time series with multiple components</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_points</span> <span class="o">=</span> <span class="mi">200</span>
    
    <span class="c1"># Create time points</span>
    <span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_points</span><span class="p">)</span>
    
    <span class="c1"># Components</span>
    <span class="n">trend</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">time</span>
    <span class="n">seasonal</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">time</span> <span class="o">/</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_points</span><span class="p">)</span>
    
    <span class="c1"># Combine components</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">trend</span> <span class="o">+</span> <span class="n">seasonal</span> <span class="o">+</span> <span class="n">noise</span>
    
    <span class="c1"># Split into train/test</span>
    <span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">n_points</span><span class="p">)</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="n">train_size</span><span class="p">]</span>
    <span class="n">test_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>
    
    <span class="c1"># Function to create windowed data</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">create_windows</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">window_size</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">):</span>
            <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">window_size</span><span class="p">])</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">window_size</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Create windowed data</span>
    <span class="n">window_size</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">create_windows</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">window_size</span><span class="p">)</span>
    
    <span class="c1"># Prepare for PyTorch</span>
    <span class="n">X_train_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Add feature dimension</span>
    <span class="n">y_train_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># Define a simple LSTM model for forecasting</span>
    <span class="k">class</span><span class="w"> </span><span class="nc">LSTMForecaster</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">LSTMForecaster</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
            
        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="c1"># x shape: [batch_size, sequence_length, input_dim]</span>
            <span class="n">lstm_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="c1"># Take only the last time step</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">y_pred</span>
    
    <span class="c1"># Example forecasting</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forecast</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">n_future</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Last window from data</span>
        <span class="n">current_window</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">window_size</span><span class="p">:]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_future</span><span class="p">):</span>
            <span class="c1"># Convert to tensor</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">current_window</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Get prediction</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">next_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            
            <span class="c1"># Add prediction to the list</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_pred</span><span class="p">)</span>
            
            <span class="c1"># Update window</span>
            <span class="n">current_window</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_window</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">next_pred</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">predictions</span>
    
    <span class="c1"># Plot the data and forecasting concept</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    
    <span class="c1"># Plot original data and forecasting window</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original Data&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">train_size</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train/Test Split&#39;</span><span class="p">)</span>
    
    <span class="c1"># Highlight an example window</span>
    <span class="n">window_start</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">[</span><span class="n">window_start</span><span class="p">:</span><span class="n">window_start</span><span class="o">+</span><span class="n">window_size</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">window_start</span><span class="p">:</span><span class="n">window_start</span><span class="o">+</span><span class="n">window_size</span><span class="p">],</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Input Window&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">[</span><span class="n">window_start</span><span class="o">+</span><span class="n">window_size</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">window_start</span><span class="o">+</span><span class="n">window_size</span><span class="p">],</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Target Value&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Time Series Forecasting with Sequence Models&#39;</span><span class="p">)</span>
    
    <span class="c1"># Illustrate prediction mechanisms</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;Time Series Forecasting and Neural Processing</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;Both brains and neural networks process time series in similar ways:</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Sliding window processing → Visual/auditory temporal integration</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Memory cells (LSTM/GRU) → Working memory in prefrontal cortex</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Multi-timescale analysis → Hierarchical processing in sensory pathways</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Prediction error learning → Predictive coding in sensory cortices</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;Applications span climate prediction, financial forecasting, healthcare monitoring,</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;neural signal processing, and brain-computer interfaces.&quot;</span>
    <span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
             <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;lightblue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round,pad=1&#39;</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>Time series forecasting applications include:</p>
<ol class="arabic simple">
<li><p><strong>Neural Signal Prediction</strong>: Forecasting EEG/MEG signals for brain-computer interfaces.</p></li>
<li><p><strong>Clinical Monitoring</strong>: Predicting patient vital signs in intensive care settings.</p></li>
<li><p><strong>Brain State Transitions</strong>: Modeling transitions between different brain states during cognition or sleep.</p></li>
<li><p><strong>Movement Prediction</strong>: Forecasting limb movements from neural activity for prosthetics.</p></li>
</ol>
<p>The biological parallel lies in how the brain itself constantly predicts future sensory inputs and outcomes based on current and past information.</p>
</section>
<section id="neural-sequence-decoding">
<h3>11.5.3 Neural Sequence Decoding<a class="headerlink" href="#neural-sequence-decoding" title="Link to this heading">#</a></h3>
<p>Sequence models can decode neural signals into meaningful outputs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">neural_sequence_decoding</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate neural sequence decoding applications.&quot;&quot;&quot;</span>
    <span class="c1"># Simulated neural sequence data</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_time</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">50</span>
    
    <span class="c1"># Create oscillatory patterns that represent different &quot;states&quot;</span>
    <span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n_time</span><span class="p">)</span>
    
    <span class="c1"># State 1: High frequency oscillation</span>
    <span class="n">state1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="n">time</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_neurons</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># State 2: Low frequency oscillation</span>
    <span class="n">state2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">time</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_neurons</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># Combine states and add noise</span>
    <span class="n">neural_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">state1</span><span class="p">,</span> <span class="n">state2</span><span class="p">])</span>
    <span class="n">neural_data</span> <span class="o">+=</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_time</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">)</span>
    
    <span class="c1"># Create &quot;behavioral&quot; output - we&#39;ll decode two motor states</span>
    <span class="c1"># State A: first half of time</span>
    <span class="c1"># State B: second half of time</span>
    <span class="n">motor_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_time</span><span class="p">)</span>
    <span class="n">motor_state</span><span class="p">[</span><span class="n">n_time</span><span class="o">//</span><span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="c1"># Plot the data and decoding concept</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    
    <span class="c1"># Plot neural data</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">neural_data</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Activity&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n_time</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Neuron&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Simulated Neural Activity&#39;</span><span class="p">)</span>
    
    <span class="c1"># Plot motor state</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">motor_state</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n_time</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Motor State&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Behavioral Output to Decode&#39;</span><span class="p">)</span>
    
    <span class="c1"># Illustration of decoding approach</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;Neural Sequence Decoding</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;Converting neural activity patterns into meaningful outputs:</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Brain-Computer Interfaces: Decode neural signals for device control</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Neurorehabilitation: Translate movement intentions to prosthetic control</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Speech Decoding: Convert neural activity to speech output</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Cognitive State Classification: Identify mental states from brain activity</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;Modern approaches use sequence models (RNNs/Transformers) to capture temporal</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;dependencies in neural data. This parallels how different brain regions interpret</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;signals from other areas to coordinate complex behaviors.&quot;</span>
    <span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
             <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;lightgreen&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round,pad=1&#39;</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>Neural sequence decoding applications include:</p>
<ol class="arabic simple">
<li><p><strong>Motor Decoding</strong>: Translating neural activity from motor cortex into movement commands for prosthetic limbs or cursor control.</p></li>
<li><p><strong>Speech Decoding</strong>: Converting neural signals from language areas into synthesized speech or text.</p></li>
<li><p><strong>Cognitive State Classification</strong>: Identifying mental states, attention levels, or emotions from neural time series.</p></li>
<li><p><strong>Neural Prosthetics</strong>: Creating closed-loop systems that both decode intentions and deliver stimulation.</p></li>
</ol>
<p>The bidirectional relationship between neuroscience and AI is particularly strong here: AI helps decode brain activity, while knowledge of neural coding informs better AI architectures.</p>
</section>
<section id="generative-sequence-models">
<h3>11.5.4 Generative Sequence Models<a class="headerlink" href="#generative-sequence-models" title="Link to this heading">#</a></h3>
<p>Sequence models can generate new data sequences with properties similar to their training data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generative_sequence_models</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate generative sequence models.&quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    
    <span class="c1"># Create a visualization of generative models</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Generate example sequences for display</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="c1"># Text generation</span>
    <span class="n">text_prompt</span> <span class="o">=</span> <span class="s2">&quot;The brain processes information through&quot;</span>
    <span class="n">text_completion</span> <span class="o">=</span> <span class="s2">&quot; complex networks of neurons that encode and transmit signals using both electrical and chemical mechanisms.&quot;</span>
    
    <span class="c1"># Music generation (simplified as a waveform)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">music_sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="n">t</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="n">t</span><span class="p">)</span>
    <span class="n">music_sample</span> <span class="o">+=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
    
    <span class="c1"># Neural activity generation (simplified)</span>
    <span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">30</span>
    <span class="n">n_time</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">neural_activity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_time</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">))</span>
    
    <span class="c1"># Create some patterned activity</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">):</span>
        <span class="n">rate</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
        <span class="n">phase</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
        <span class="n">neural_activity</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_time</span><span class="p">)</span> <span class="o">+</span> <span class="n">phase</span><span class="p">)</span>
    
    <span class="n">neural_activity</span> <span class="o">+=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_time</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">)</span>
    
    <span class="c1"># Create visualization</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
    
    <span class="c1"># 1. Text Generation</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">311</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="s2">&quot;Text Generation:&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Prompt: &quot;</span><span class="si">{</span><span class="n">text_prompt</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Completion: &quot;</span><span class="si">{</span><span class="n">text_completion</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
             <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">))</span>
    
    <span class="c1"># 2. Music Generation</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">312</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">music_sample</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Music Generation: Waveform Example&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time&quot;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Amplitude&quot;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># 3. Neural Activity Generation</span>
    <span class="n">ax3</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">313</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">neural_activity</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Neural Activity Generation&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time&quot;</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Neuron&quot;</span><span class="p">)</span>
    
    <span class="c1"># Add explanatory text overlay</span>
    <span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;Generative Sequence Models</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;Applications bridging AI and neuroscience:</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Text Generation: Creating coherent language (like GPT models)</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Music Synthesis: Composing music with temporal structure</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Neural Activity Simulation: Generating realistic neural recordings</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Movement Synthesis: Creating naturalistic motion sequences</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;Biological Parallels:</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Imagination in the brain involves generating sequences of neural activity</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• During planning, the hippocampus generates sequences of place cell activity</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Dreams are generated sequences of neural patterns during sleep</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;• Motor planning involves simulating sequences of movements before execution&quot;</span>
    <span class="p">)</span>
    
    <span class="n">fig</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
             <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#FFFFCC&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round,pad=1&#39;</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">plt</span>
</pre></div>
</div>
<p>Generative sequence model applications include:</p>
<ol class="arabic simple">
<li><p><strong>Neural Simulation</strong>: Generating realistic neural spike train data for hypothesis testing and model validation.</p></li>
<li><p><strong>Brain-Inspired Content Creation</strong>: Using neural sequence generation principles to create art, music, or narrative.</p></li>
<li><p><strong>Cognitive Modeling</strong>: Simulating thought processes by generating sequences of cognitive states.</p></li>
<li><p><strong>Therapeutic Applications</strong>: Generating personalized auditory or visual stimuli for neurological rehabilitation.</p></li>
</ol>
<p>The process of generating sequences in artificial models parallels how the brain generates sequences during imagination, planning, and dreaming.</p>
</section>
<section id="application-design-principles">
<h3>11.5.5 Application Design Principles<a class="headerlink" href="#application-design-principles" title="Link to this heading">#</a></h3>
<p>When designing sequence model applications that bridge neuroscience and AI, consider these principles:</p>
<ol class="arabic simple">
<li><p><strong>Temporal Scale Matching</strong>: Ensure your model’s temporal dynamics match the timescale of the neural process being modeled.</p></li>
<li><p><strong>Interpretability</strong>: Design models that allow insight into their internal representations, particularly for neuroscience applications.</p></li>
<li><p><strong>Bidirectional Transfer</strong>: Apply neuroscience insights to improve AI designs, and use AI to generate testable neuroscience hypotheses.</p></li>
<li><p><strong>Context Sensitivity</strong>: Account for context effects in sequence processing, as both brains and effective AI models are highly context-sensitive.</p></li>
<li><p><strong>Multimodal Integration</strong>: Combine information across modalities, as the brain does not process sequences in isolated channels.</p></li>
</ol>
<p>These applications demonstrate how sequence models serve as a bridge between computational neuroscience and artificial intelligence, with each field informing and enhancing the other.</p>
</section>
</section>
<section id="code-lab">
<h2>11.6 Code Lab<a class="headerlink" href="#code-lab" title="Link to this heading">#</a></h2>
<p>This hands-on section provides practical exercises that will help you implement and experiment with sequence models. The exercises progress from basic recurrent networks to transformers, reinforcing the concepts covered in this chapter.</p>
<section id="implementing-an-lstm-from-components">
<h3>11.6.1 Implementing an LSTM from Components<a class="headerlink" href="#implementing-an-lstm-from-components" title="Link to this heading">#</a></h3>
<p>In this exercise, we’ll build an LSTM cell from scratch to understand its internal mechanisms:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LSTMCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Custom LSTM cell implementation from basic components.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize LSTM cell components.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            input_size: Dimension of input features</span>
<span class="sd">            hidden_size: Dimension of hidden state and cell state</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># Forget gate components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_gate_x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_gate_h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Input gate components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_gate_x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_gate_h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Cell candidate components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Output gate components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_gate_x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_gate_h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Activation functions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through LSTM cell.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor of shape [batch_size, input_size]</span>
<span class="sd">            state: Tuple (h, c) containing previous hidden state and cell state</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            h_next: Next hidden state</span>
<span class="sd">            c_next: Next cell state</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span> <span class="o">=</span> <span class="n">state</span>
        
        <span class="c1"># Forget gate: what to forget from cell state</span>
        <span class="n">f_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forget_gate_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">forget_gate_h</span><span class="p">(</span><span class="n">h_prev</span><span class="p">))</span>
        
        <span class="c1"># Input gate: what new information to add</span>
        <span class="n">i_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_gate_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_gate_h</span><span class="p">(</span><span class="n">h_prev</span><span class="p">))</span>
        
        <span class="c1"># Cell candidate: potential new values to add to cell state</span>
        <span class="n">c_tilde</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cell_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_h</span><span class="p">(</span><span class="n">h_prev</span><span class="p">))</span>
        
        <span class="c1"># Cell state update</span>
        <span class="n">c_next</span> <span class="o">=</span> <span class="n">f_t</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">c_tilde</span>
        
        <span class="c1"># Output gate: what to output from cell state</span>
        <span class="n">o_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_gate_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_gate_h</span><span class="p">(</span><span class="n">h_prev</span><span class="p">))</span>
        
        <span class="c1"># Hidden state update</span>
        <span class="n">h_next</span> <span class="o">=</span> <span class="n">o_t</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c_next</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">h_next</span><span class="p">,</span> <span class="n">c_next</span>
    
<span class="k">class</span><span class="w"> </span><span class="nc">LSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LSTM network using our custom cell.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm_cell</span> <span class="o">=</span> <span class="n">LSTMCell</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Process sequence through LSTM.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor of shape [batch_size, seq_len, input_size]</span>
<span class="sd">            state: Initial state tuple (h_0, c_0) or None</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            outputs: Tensor of output predictions</span>
<span class="sd">            state: Final state tuple (h_n, c_n)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        
        <span class="c1"># Initialize hidden state and cell state if not provided</span>
        <span class="k">if</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">h_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">c_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span><span class="p">)</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Process each time step</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_cell</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span><span class="p">)</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h_t</span><span class="p">)</span>
        
        <span class="c1"># Stack outputs along sequence dimension</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Apply output layer to each time step</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">state</span>

<span class="c1"># Exercise 1: Generate synthetic data and test the LSTM</span>
<span class="k">def</span><span class="w"> </span><span class="nf">exercise1_test_lstm</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate a simple sequence dataset and train our custom LSTM.&quot;&quot;&quot;</span>
    <span class="c1"># Generate sine wave data</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="c1"># Create a noisy sine wave sequence</span>
    <span class="n">time_steps</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">series</span> <span class="o">=</span> <span class="mf">0.8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">time_steps</span><span class="p">))</span> <span class="o">+</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.05</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">time_steps</span><span class="p">))</span>
    <span class="n">series</span> <span class="o">+=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">time_steps</span><span class="p">)</span>
    
    <span class="c1"># Normalize data</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">series</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">series</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    
    <span class="c1"># Create input/output sequences for prediction</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">create_sequences</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">):</span>
        <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">seq_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:(</span><span class="n">i</span> <span class="o">+</span> <span class="n">seq_length</span><span class="p">)]</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">seq_length</span><span class="p">]</span>
            <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
    
    <span class="c1"># Create sequences with length 20</span>
    <span class="n">seq_length</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_sequences</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
    
    <span class="c1"># Reshape X for LSTM input [samples, time steps, features]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Split into train and test sets</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>
    
    <span class="c1"># Convert to PyTorch tensors</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Initialize model</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    
    <span class="c1"># Loss and optimizer</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    
    <span class="c1"># Training parameters</span>
    <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    
    <span class="c1"># For storing metrics</span>
    <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Training loop</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="c1"># Mini-batch training</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># Get mini-batch</span>
            <span class="n">batch_X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
            <span class="n">batch_y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
            
            <span class="c1"># Forward pass</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">batch_y</span><span class="p">)</span>
            
            <span class="c1"># Backward and optimize</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="c1"># Record training loss</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">train_outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
            <span class="n">train_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">train_outputs</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>
            <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        
        <span class="c1"># Print progress</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">], Loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="c1"># Test the model</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># Get predictions on test set</span>
        <span class="n">test_outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">test_predictions</span> <span class="o">=</span> <span class="n">test_outputs</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
        <span class="c1"># Mean squared error on test set</span>
        <span class="n">test_mse</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">test_outputs</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Test MSE: </span><span class="si">{</span><span class="n">test_mse</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="c1"># Visualize predictions</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="c1"># Plot training loss</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MSE Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Plot predictions on a portion of test data</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">sample_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Values&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_predictions</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predictions&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;LSTM Predictions vs. True Values&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sample Index&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>

<span class="c1"># Call the exercise function</span>
<span class="c1"># exercise1_test_lstm()</span>
</pre></div>
</div>
<p>When implemented, this LSTM has several key differences from built-in PyTorch LSTMs:</p>
<ol class="arabic simple">
<li><p>It creates distinct linear layers for each gate component rather than a single matrix multiplication.</p></li>
<li><p>It processes one time step at a time rather than using optimized batch operations.</p></li>
<li><p>It explicitly implements the gating mechanisms to provide better clarity on how LSTMs work.</p></li>
</ol>
<p>Try experimenting with the hyperparameters and extending it with features like:</p>
<ul class="simple">
<li><p>Bidirectional processing</p></li>
<li><p>Multi-layer architecture</p></li>
<li><p>Different initialization schemes</p></li>
</ul>
</section>
<section id="building-a-self-attention-mechanism">
<h3>11.6.2 Building a Self-Attention Mechanism<a class="headerlink" href="#building-a-self-attention-mechanism" title="Link to this heading">#</a></h3>
<p>In this exercise, we’ll implement a self-attention mechanism and visualize attention patterns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Self-attention module from scratch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize self-attention module.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            embed_dim: Dimension of input embeddings</span>
<span class="sd">            num_heads: Number of attention heads</span>
<span class="sd">            dropout: Dropout probability</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Embedding dimension must be divisible by number of heads&quot;</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="c1"># Linear projections</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        
        <span class="c1"># Dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="c1"># Scaling factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply self-attention to input.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor of shape [batch_size, seq_len, embed_dim]</span>
<span class="sd">            mask: Optional mask tensor of shape [batch_size, seq_len, seq_len]</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            output: Attention output</span>
<span class="sd">            attention_weights: Attention weights for visualization</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="c1"># Linear projections</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Reshape for multi-head attention</span>
        <span class="c1"># [batch_size, seq_len, embed_dim] -&gt; [batch_size, seq_len, num_heads, head_dim]</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        
        <span class="c1"># Transpose to [batch_size, num_heads, seq_len, head_dim]</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Compute attention scores</span>
        <span class="c1"># [batch_size, num_heads, seq_len, seq_len]</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span>
        
        <span class="c1"># Apply mask if provided</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        
        <span class="c1"># Softmax to get attention weights</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span>
        
        <span class="c1"># Apply attention to values</span>
        <span class="c1"># [batch_size, num_heads, seq_len, head_dim]</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        
        <span class="c1"># Transpose back and reshape</span>
        <span class="c1"># [batch_size, seq_len, num_heads, head_dim] -&gt; [batch_size, seq_len, embed_dim]</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        
        <span class="c1"># Final linear projection</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_proj</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>

<span class="k">def</span><span class="w"> </span><span class="nf">exercise2_test_attention</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test and visualize the attention mechanism.&quot;&quot;&quot;</span>
    <span class="c1"># Create toy sequence data</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="c1"># Random token IDs</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
    
    <span class="c1"># Embedding layer</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
    
    <span class="c1"># Get embeddings</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    
    <span class="c1"># Create self-attention layer with 4 heads</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    
    <span class="c1"># Apply self-attention</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># Create more interpretable example with actual words</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;The&quot;</span><span class="p">,</span> <span class="s2">&quot;quick&quot;</span><span class="p">,</span> <span class="s2">&quot;brown&quot;</span><span class="p">,</span> <span class="s2">&quot;fox&quot;</span><span class="p">,</span> <span class="s2">&quot;jumps&quot;</span><span class="p">,</span> <span class="s2">&quot;over&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;lazy&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">]</span>
    <span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    
    <span class="c1"># Create one-hot encodings for words</span>
    <span class="n">one_hot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)):</span>
        <span class="n">one_hot</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
    
    <span class="c1"># Create simple embeddings by adding position information</span>
    <span class="n">position_factor</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">simple_embeddings</span> <span class="o">=</span> <span class="n">one_hot</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)):</span>
        <span class="n">simple_embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">position_factor</span> <span class="o">*</span> <span class="n">i</span>
    
    <span class="c1"># Expand dimensions for batch</span>
    <span class="n">simple_embeddings</span> <span class="o">=</span> <span class="n">simple_embeddings</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># Apply attention to these embeddings</span>
    <span class="n">simple_attention</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">simple_attn_weights</span> <span class="o">=</span> <span class="n">simple_attention</span><span class="p">(</span><span class="n">simple_embeddings</span><span class="p">)</span>
    
    <span class="c1"># Visualize attention patterns</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    
    <span class="c1"># Plot attention weights for each head</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
        <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">h</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Head </span><span class="si">{</span><span class="n">h</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> Attention&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Key Position&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Query Position&#39;</span><span class="p">)</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    
    <span class="c1"># Visualize attention for the word example</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">simple_attn_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Self-Attention Patterns for Sentence&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)),</span> <span class="n">words</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)),</span> <span class="n">words</span><span class="p">)</span>
    
    <span class="c1"># Add attention values</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)):</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">simple_attn_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">value</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> 
                    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span> <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>

<span class="c1"># Call the exercise function</span>
<span class="c1"># exercise2_test_attention()</span>
</pre></div>
</div>
<p>Experiment with the attention mechanism by:</p>
<ol class="arabic simple">
<li><p>Adding positional encodings to see how they affect attention patterns</p></li>
<li><p>Implementing causal attention (masking future tokens) for autoregressive models</p></li>
<li><p>Testing with different numbers of attention heads</p></li>
<li><p>Visualizing attention patterns on real sentences</p></li>
</ol>
</section>
<section id="training-a-small-transformer">
<h3>11.6.3 Training a Small Transformer<a class="headerlink" href="#training-a-small-transformer" title="Link to this heading">#</a></h3>
<p>In this exercise, we’ll implement a small transformer model for sequence prediction:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Positional encoding for transformer models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="c1"># Create positional encodings</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        
        <span class="c1"># Apply sine to even positions and cosine to odd positions</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        
        <span class="c1"># Add batch dimension and register as buffer (not a parameter)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;pe&#39;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add positional encoding to input.&quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SimpleTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A simple transformer model for sequence prediction.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># Input embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        
        <span class="c1"># Positional encoding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="c1"># Transformer encoder</span>
        <span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dim_feedforward</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">encoder_layers</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">)</span>
        
        <span class="c1"># Output layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through the transformer.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            src: Input tensor [batch_size, seq_len, input_dim]</span>
<span class="sd">            src_mask: Optional mask for padding or attention directionality</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            output: Predictions [batch_size, seq_len, output_dim]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Embed input</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        
        <span class="c1"># Add positional encoding</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        
        <span class="c1"># Transpose for transformer input [seq_len, batch_size, d_model]</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="n">embedded</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Apply transformer encoder</span>
        <span class="n">transformer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_encoder</span><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="c1"># Transpose back to [batch_size, seq_len, d_model]</span>
        <span class="n">transformer_output</span> <span class="o">=</span> <span class="n">transformer_output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Apply output layer</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">transformer_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>

<span class="k">def</span><span class="w"> </span><span class="nf">exercise3_transformer_for_sine</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train a simple transformer for sine wave prediction.&quot;&quot;&quot;</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
    
    <span class="c1"># Generate sine wave data</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate_sine_data</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">prediction_step</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate sine wave data with multiple features.&quot;&quot;&quot;</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
            <span class="c1"># Random sine wave parameters</span>
            <span class="n">amplitude</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
            <span class="n">frequency</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
            <span class="n">phase</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
            
            <span class="c1"># Generate time points</span>
            <span class="n">time_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">prediction_step</span><span class="p">)</span>
            
            <span class="c1"># Generate sine wave</span>
            <span class="n">sine_wave</span> <span class="o">=</span> <span class="n">amplitude</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">frequency</span> <span class="o">*</span> <span class="n">time_points</span> <span class="o">+</span> <span class="n">phase</span><span class="p">)</span>
            
            <span class="c1"># Add noise</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sine_wave</span><span class="p">))</span>
            <span class="n">noisy_sine</span> <span class="o">=</span> <span class="n">sine_wave</span> <span class="o">+</span> <span class="n">noise</span>
            
            <span class="c1"># Create features (current value and sin/cos of time)</span>
            <span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
                <span class="n">t</span> <span class="o">=</span> <span class="n">time_points</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">features</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">noisy_sine</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># Current value</span>
                <span class="n">features</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>      <span class="c1"># Time feature 1</span>
                <span class="n">features</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>      <span class="c1"># Time feature 2</span>
            
            <span class="c1"># Target is future value</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">sine_wave</span><span class="p">[</span><span class="n">prediction_step</span><span class="p">:</span><span class="n">seq_len</span><span class="o">+</span><span class="n">prediction_step</span><span class="p">]</span>
            
            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
            <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
    
    <span class="c1"># Generate data</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_sine_data</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">prediction_step</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    
    <span class="c1"># Create 80/20 train-test split</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="c1"># Convert to PyTorch tensors</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Add feature dimension</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Create sequence mask (for causal attention)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate_square_subsequent_mask</span><span class="p">(</span><span class="n">sz</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate a mask for causal attention.&quot;&quot;&quot;</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">sz</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">mask</span>
    
    <span class="c1"># Model parameters</span>
    <span class="n">input_dim</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Current value + 2 time features</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">32</span>   <span class="c1"># Transformer hidden dimension</span>
    <span class="n">nhead</span> <span class="o">=</span> <span class="mi">4</span>      <span class="c1"># Number of attention heads</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># Number of transformer layers</span>
    <span class="n">output_dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Predicting a single value</span>
    
    <span class="c1"># Initialize model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleTransformer</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
    
    <span class="c1"># Loss and optimizer</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    
    <span class="c1"># Training parameters</span>
    <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">30</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    
    <span class="c1"># For storing metrics</span>
    <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Training loop</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="c1"># Create random permutation</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
        
        <span class="c1"># Mini-batch training</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># Get mini-batch indices</span>
            <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
            
            <span class="c1"># Get mini-batch data</span>
            <span class="n">batch_X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
            <span class="n">batch_y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
            
            <span class="c1"># Forward pass</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">)</span>
            
            <span class="c1"># Backward and optimize</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="c1"># Record average training loss</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>
        
        <span class="c1"># Print progress</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">], Loss: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="c1"># Test the model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># Get predictions on test set</span>
        <span class="n">test_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">test_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">test_outputs</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Test MSE: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="c1"># Visualize results</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    
    <span class="c1"># Plot training loss</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MSE Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Plot example predictions</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># Select a random example from test set</span>
    <span class="n">example_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="n">input_seq</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">example_idx</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Get actual values from input</span>
    <span class="n">true_future</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">example_idx</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">predicted_future</span> <span class="o">=</span> <span class="n">test_outputs</span><span class="p">[</span><span class="n">example_idx</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="c1"># Time points for x-axis</span>
    <span class="n">all_steps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">true_future</span><span class="p">))</span>
    
    <span class="c1"># Plot input sequence</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">all_steps</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)],</span> <span class="n">input_seq</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Input Sequence&#39;</span><span class="p">)</span>
    
    <span class="c1"># Plot true future and predictions</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">all_steps</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">input_seq</span><span class="p">):],</span> <span class="n">true_future</span><span class="p">,</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Future&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">all_steps</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">input_seq</span><span class="p">):],</span> <span class="n">predicted_future</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Future&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Transformer Sequence Prediction&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span>

<span class="c1"># Call the exercise function</span>
<span class="c1"># exercise3_transformer_for_sine()</span>
</pre></div>
</div>
<p>Extend this exercise by:</p>
<ol class="arabic simple">
<li><p>Implementing a full encoder-decoder transformer for sequence-to-sequence tasks</p></li>
<li><p>Adding an autoregressive inference mode for generating sequences</p></li>
<li><p>Experimenting with different attention patterns (local attention, sparse attention)</p></li>
<li><p>Trying the transformer on different time series forecasting tasks</p></li>
</ol>
</section>
<section id="neural-sequence-prediction">
<h3>11.6.4 Neural Sequence Prediction<a class="headerlink" href="#neural-sequence-prediction" title="Link to this heading">#</a></h3>
<p>In this exercise, we’ll implement a model that predicts neural activity patterns, similar to what might be used in brain-computer interfaces:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">exercise4_neural_sequence_prediction</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Predict neural activity patterns using sequence models.&quot;&quot;&quot;</span>
    <span class="c1"># Generate synthetic neural data with temporal patterns</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="c1"># Simulation parameters</span>
    <span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">20</span>        <span class="c1"># Number of neurons</span>
    <span class="n">n_timepoints</span> <span class="o">=</span> <span class="mi">500</span>    <span class="c1"># Number of timepoints</span>
    <span class="n">n_trials</span> <span class="o">=</span> <span class="mi">100</span>        <span class="c1"># Number of trials/examples</span>
    
    <span class="c1"># Create base oscillatory patterns (theta, alpha, beta, gamma ranges)</span>
    <span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_timepoints</span><span class="p">)</span>
    <span class="n">patterns</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">time</span><span class="p">),</span>  <span class="c1"># Theta (~5Hz)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">time</span><span class="p">),</span>   <span class="c1"># Alpha (~10Hz)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">time</span><span class="p">),</span>   <span class="c1"># Beta (~20Hz)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mf">0.4</span> <span class="o">*</span> <span class="n">time</span><span class="p">)</span>    <span class="c1"># Gamma (~40Hz)</span>
    <span class="p">]</span>
    
    <span class="c1"># Generate trials with mixed oscillations</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">n_timepoints</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="c1"># Randomly assign neurons to different oscillatory patterns</span>
        <span class="n">neuron_patterns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">patterns</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">n_neurons</span><span class="p">)</span>
        
        <span class="c1"># Generate activity for each neuron</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">):</span>
            <span class="c1"># Base pattern</span>
            <span class="n">base_pattern</span> <span class="o">=</span> <span class="n">patterns</span><span class="p">[</span><span class="n">neuron_patterns</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
            
            <span class="c1"># Add phase shift and amplitude variation</span>
            <span class="n">phase_shift</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
            <span class="n">amplitude</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
            
            <span class="c1"># Generate activity with noise</span>
            <span class="n">activity</span> <span class="o">=</span> <span class="n">amplitude</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">time</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.05</span> <span class="o">+</span> <span class="mf">0.15</span> <span class="o">*</span> <span class="n">neuron_patterns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">patterns</span><span class="p">))</span> <span class="o">+</span> <span class="n">phase_shift</span><span class="p">)</span>
            <span class="n">activity</span> <span class="o">+=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_timepoints</span><span class="p">)</span>
            
            <span class="c1"># Store in data array</span>
            <span class="n">data</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">activity</span>
    
    <span class="c1"># Split the data into input and target sequences</span>
    <span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># Input sequence length</span>
    <span class="n">prediction_length</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of future timepoints to predict</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Create input/output pairs</span>
    <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="c1"># Multiple starting points per trial</span>
        <span class="n">max_start</span> <span class="o">=</span> <span class="n">n_timepoints</span> <span class="o">-</span> <span class="n">sequence_length</span> <span class="o">-</span> <span class="n">prediction_length</span>
        <span class="n">step</span> <span class="o">=</span> <span class="n">max_start</span> <span class="o">//</span> <span class="mi">5</span>  <span class="c1"># 5 sequences per trial</span>
        
        <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_start</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">sequence_length</span>
            <span class="c1"># Input: sequence of length &#39;sequence_length&#39;</span>
            <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>
            <span class="c1"># Target: next &#39;prediction_length&#39; timepoints</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">trial</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span><span class="n">end</span><span class="o">+</span><span class="n">prediction_length</span><span class="p">])</span>
    
    <span class="c1"># Convert to numpy arrays</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Split into train and test</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="c1"># Create PyTorch tensors</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
    
    <span class="c1"># Define a model that combines LSTM and attention</span>
    <span class="k">class</span><span class="w"> </span><span class="nc">NeuralSequencePredictor</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">pred_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">NeuralSequencePredictor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_len</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pred_len</span> <span class="o">=</span> <span class="n">pred_len</span>
            
            <span class="c1"># LSTM layer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            
            <span class="c1"># Self-attention layer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">)</span>
            
            <span class="c1"># Output projection</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span> <span class="o">*</span> <span class="n">pred_len</span><span class="p">)</span>
            
        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Forward pass through the model.</span>
<span class="sd">            </span>
<span class="sd">            Args:</span>
<span class="sd">                x: Input tensor [batch_size, seq_len, input_dim]</span>
<span class="sd">                </span>
<span class="sd">            Returns:</span>
<span class="sd">                predictions: Output tensor [batch_size, pred_len, output_dim]</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            
            <span class="c1"># Process through LSTM</span>
            <span class="n">lstm_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="c1"># Apply self-attention</span>
            <span class="n">attended</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>
            
            <span class="c1"># Project to output</span>
            <span class="c1"># We only need the final state to start predicting the future</span>
            <span class="n">final_state</span> <span class="o">=</span> <span class="n">attended</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="c1"># Project to multiple future timesteps</span>
            <span class="n">projection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">final_state</span><span class="p">)</span>
            
            <span class="c1"># Reshape to [batch_size, pred_len, output_dim]</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">projection</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">)</span>
            
            <span class="k">return</span> <span class="n">predictions</span>
    
    <span class="c1"># Initialize model</span>
    <span class="n">input_dim</span> <span class="o">=</span> <span class="n">n_neurons</span>  <span class="c1"># Number of input features (neurons)</span>
    <span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">64</span>        <span class="c1"># Hidden dimension</span>
    <span class="n">output_dim</span> <span class="o">=</span> <span class="n">n_neurons</span> <span class="c1"># Number of output features (predicting all neurons)</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">NeuralSequencePredictor</span><span class="p">(</span>
        <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
        <span class="n">seq_len</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">,</span>
        <span class="n">pred_len</span><span class="o">=</span><span class="n">prediction_length</span>
    <span class="p">)</span>
    
    <span class="c1"># Loss and optimizer</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    
    <span class="c1"># Training parameters</span>
    <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">30</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    
    <span class="c1"># For storing training metrics</span>
    <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Training loop</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="c1"># Create random permutation</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
        
        <span class="c1"># Mini-batch training</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># Get mini-batch indices</span>
            <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
            
            <span class="c1"># Get mini-batch data</span>
            <span class="n">batch_X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
            <span class="n">batch_y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
            
            <span class="c1"># Forward pass</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">)</span>
            
            <span class="c1"># Backward and optimize</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_indices</span><span class="p">)</span>
        
        <span class="c1"># Average loss for the epoch</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>
        
        <span class="c1"># Print progress</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">], Loss: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="c1"># Evaluate on test set</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">test_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">test_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">test_predictions</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Test MSE: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="c1"># Visualize results</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
    
    <span class="c1"># Plot training loss</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MSE Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Select an example to visualize</span>
    <span class="n">example_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    
    <span class="c1"># Plot input sequence for a single neuron</span>
    <span class="n">neuron_idx</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># First neuron for visualization</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">),</span> <span class="n">X_test</span><span class="p">[</span><span class="n">example_idx</span><span class="p">,</span> <span class="p">:,</span> <span class="n">neuron_idx</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Input Sequence&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">+</span> <span class="n">prediction_length</span><span class="p">),</span> 
             <span class="n">y_test</span><span class="p">[</span><span class="n">example_idx</span><span class="p">,</span> <span class="p">:,</span> <span class="n">neuron_idx</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Future&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">+</span> <span class="n">prediction_length</span><span class="p">),</span> 
             <span class="n">test_predictions</span><span class="p">[</span><span class="n">example_idx</span><span class="p">,</span> <span class="p">:,</span> <span class="n">neuron_idx</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Future&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_length</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Neuron </span><span class="si">{</span><span class="n">neuron_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> Activity Prediction&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Activity&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Plot heatmap of all neurons</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    
    <span class="c1"># Combine input and true/predicted future for visualization</span>
    <span class="n">full_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">sequence_length</span> <span class="o">+</span> <span class="n">prediction_length</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">))</span>
    <span class="n">full_true</span><span class="p">[:</span><span class="n">sequence_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">full_true</span><span class="p">[</span><span class="n">sequence_length</span><span class="p">:]</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="n">full_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">sequence_length</span> <span class="o">+</span> <span class="n">prediction_length</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">))</span>
    <span class="n">full_pred</span><span class="p">[:</span><span class="n">sequence_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">full_pred</span><span class="p">[</span><span class="n">sequence_length</span><span class="p">:]</span> <span class="o">=</span> <span class="n">test_predictions</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="c1"># Calculate error</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">full_true</span> <span class="o">-</span> <span class="n">full_pred</span><span class="p">)</span>
    
    <span class="c1"># Create a figure with subplots for all three heatmaps</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
    
    <span class="c1"># Plot true activity</span>
    <span class="n">im0</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">full_true</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;True Neural Activity&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Neuron&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_length</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># Plot predicted activity</span>
    <span class="n">im1</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">full_pred</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Predicted Neural Activity&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Neuron&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_length</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="c1"># Plot prediction error</span>
    <span class="n">im2</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">error</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;YlOrRd&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Prediction Error&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Neuron&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sequence_length</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">plt</span><span class="p">,</span> <span class="n">fig</span>

<span class="c1"># Call the exercise function</span>
<span class="c1"># exercise4_neural_sequence_prediction()</span>
</pre></div>
</div>
<p>Try extending this neural prediction exercise by:</p>
<ol class="arabic simple">
<li><p>Using real neural data from open datasets (e.g., Allen Brain Observatory)</p></li>
<li><p>Implementing an encoder-decoder architecture for longer-term predictions</p></li>
<li><p>Adding biological constraints to the model architecture</p></li>
<li><p>Visualizing the learned attention patterns to see which neurons interact</p></li>
</ol>
</section>
<section id="exercise-solutions">
<h3>11.6.5 Exercise Solutions<a class="headerlink" href="#exercise-solutions" title="Link to this heading">#</a></h3>
<p>These exercises provide hands-on experience with the sequence models described in this chapter. Run them individually to explore and modify the implementations as you learn.</p>
<p>To execute an exercise, uncomment the function call at the end of each code block and run the cell. The exercises progress in difficulty and build upon concepts introduced earlier in the chapter.</p>
<p>By understanding these implementations and experimenting with the code, you’ll gain deeper insights into how sequence models work and how they can be applied to neuroscience and AI tasks.</p>
</section>
</section>
<section id="take-aways">
<h2>11.7 Take-aways<a class="headerlink" href="#take-aways" title="Link to this heading">#</a></h2>
<div class="important admonition">
<p class="admonition-title">Knowledge Connections</p>
<p><strong>Looking Back</strong></p>
<ul class="simple">
<li><p><strong>Chapter 3 (Spatial Navigation)</strong>: Recurrent neural networks share conceptual similarities with hippocampal place cells’ temporal information processing for predictive navigation</p></li>
<li><p><strong>Chapter 7 (Information Theory)</strong>: Sequential information processing relies on principles of information flow and entropy across temporal dimensions</p></li>
<li><p><strong>Chapter 9 (ML Foundations)</strong>: Classical sequence models like HMMs and CRFs form the statistical foundation for modern sequence processing</p></li>
<li><p><strong>Chapter 10 (Deep Learning)</strong>: Core neural network operations and backpropagation principles are extended to sequence domains via backpropagation through time</p></li>
</ul>
<p><strong>Looking Forward</strong></p>
<ul class="simple">
<li><p><strong>Chapter 12 (Large Language Models)</strong>: Transformer architectures from this chapter form the foundation for LLMs with scaled attention mechanisms</p></li>
<li><p><strong>Chapter 13 (Multimodal Models)</strong>: Sequence processing techniques are extended to handle multiple modalities through cross-attention and embedding alignment</p></li>
<li><p><strong>Chapter 14 (Future Directions)</strong>: Innovations in sequence modeling contribute to neuromorphic computing and brain-inspired AI architectures</p></li>
</ul>
</div>
<p>This chapter has covered the evolution of sequence models from recurrent networks to transformers, connecting these AI architectures to neural processing mechanisms in the brain. Here are the key insights:</p>
<ol class="arabic simple">
<li><p><strong>Evolutionary Trajectory</strong>: Sequence modeling has evolved from inherently sequential recurrent networks (RNNs, LSTMs, GRUs) toward parallelizable attention-based architectures (transformers). This mirrors how the brain combines both recurrent local circuits and distributed global processing.</p></li>
<li><p><strong>Biological Inspiration</strong>: Many features of modern sequence models have parallels in brain function:</p>
<ul class="simple">
<li><p>Gating mechanisms in LSTMs parallel selective filtering in prefrontal-basal ganglia circuits</p></li>
<li><p>Attention mechanisms resemble selective attention in thalamo-cortical systems</p></li>
<li><p>Multi-timescale processing occurs in both transformer layers and the cortical hierarchy</p></li>
</ul>
</li>
<li><p><strong>Computational Trade-offs</strong>: Different architectures involve trade-offs between:</p>
<ul class="simple">
<li><p>Computational efficiency vs. biological plausibility</p></li>
<li><p>Sequential processing vs. parallelism</p></li>
<li><p>Memory usage vs. contextual range</p></li>
<li><p>Inductive biases vs. flexibility</p></li>
</ul>
</li>
<li><p><strong>Temporal Integration</strong>: Both biological and artificial systems must solve the problem of integrating information across time. The brain uses recurrent connections with various time constants, while artificial systems use either recurrent connections (RNNs) or explicit attention to previous time points (transformers).</p></li>
<li><p><strong>Bidirectional Inspiration</strong>: The relationship between neuroscience and AI is increasingly bidirectional:</p>
<ul class="simple">
<li><p>Neuroscience inspires new AI architectures (e.g., attention mechanisms)</p></li>
<li><p>AI models generate hypotheses about neural computation (e.g., predictive coding)</p></li>
<li><p>Both fields inform each other through shared mathematical frameworks</p></li>
</ul>
</li>
<li><p><strong>Emergent Properties</strong>: As sequence models scale up, they demonstrate emergent capabilities that weren’t explicitly programmed, similar to how neural systems show emergent cognitive abilities. This has led to foundation models that capture complex sequential dependencies across multiple domains.</p></li>
<li><p><strong>Applications Bridging Fields</strong>: Sequence models provide a shared framework for applications spanning neuroscience and AI, from neural decoding and brain-computer interfaces to natural language processing and time series forecasting.</p></li>
</ol>
<p>The rapid advancement of sequence models represents one of the most successful areas of cross-fertilization between neuroscience and artificial intelligence, with each field benefiting from insights gained in the other.</p>
</section>
<section id="further-reading-media">
<h2>11.8 Further Reading &amp; Media<a class="headerlink" href="#further-reading-media" title="Link to this heading">#</a></h2>
<section id="key-papers">
<h3>Key Papers<a class="headerlink" href="#key-papers" title="Link to this heading">#</a></h3>
<section id="foundational-papers">
<h4>Foundational Papers<a class="headerlink" href="#foundational-papers" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Hochreiter, S., &amp; Schmidhuber, J. (1997). <strong>Long short-term memory</strong>. <em>Neural Computation, 9(8)</em>, 1735-1780.</p></li>
<li><p>Vaswani, A., et al. (2017). <strong>Attention is all you need</strong>. <em>Advances in Neural Information Processing Systems</em>, 5998-6008.</p></li>
<li><p>Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). <strong>Sequence to sequence learning with neural networks</strong>. <em>Advances in Neural Information Processing Systems</em>, 3104-3112.</p></li>
<li><p>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2015). <strong>Neural machine translation by jointly learning to align and translate</strong>. <em>International Conference on Learning Representations</em>.</p></li>
</ul>
</section>
<section id="neuroscience-connections">
<h4>Neuroscience Connections<a class="headerlink" href="#neuroscience-connections" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Wang, J., et al. (2018). <strong>Prefrontal cortex as a meta-reinforcement learning system</strong>. <em>Nature Neuroscience, 21(6)</em>, 860-868.</p></li>
<li><p>Friston, K. (2010). <strong>The free-energy principle: A unified brain theory?</strong> <em>Nature Reviews Neuroscience, 11(2)</em>, 127-138.</p></li>
<li><p>Kell, A. J., &amp; McDermott, J. H. (2019). <strong>Deep neural network models of sensory systems: Windows onto the role of task constraints</strong>. <em>Current Opinion in Neurobiology, 55</em>, 121-132.</p></li>
<li><p>Yamins, D. L., &amp; DiCarlo, J. J. (2016). <strong>Using goal-driven deep learning models to understand sensory cortex</strong>. <em>Nature Neuroscience, 19(3)</em>, 356-365.</p></li>
</ul>
</section>
</section>
<section id="books">
<h3>Books<a class="headerlink" href="#books" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <strong>Deep Learning</strong>. MIT Press. <a class="reference external" href="https://www.deeplearningbook.org/">Online version</a></p></li>
<li><p>Sejnowski, T. J. (2018). <strong>The Deep Learning Revolution</strong>. MIT Press.</p></li>
<li><p>Williams, R. J., &amp; Zipser, D. (2006). <strong>A learning algorithm for continually running fully recurrent neural networks</strong>. <em>Neural Computation</em>.</p></li>
</ul>
</section>
<section id="online-resources">
<h3>Online Resources<a class="headerlink" href="#online-resources" title="Link to this heading">#</a></h3>
<section id="tutorials-and-blog-posts">
<h4>Tutorials and Blog Posts<a class="headerlink" href="#tutorials-and-blog-posts" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Karpathy, A. (2015). <a class="reference external" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"><strong>The Unreasonable Effectiveness of Recurrent Neural Networks</strong></a></p></li>
<li><p>Olah, C. (2015). <a class="reference external" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"><strong>Understanding LSTM Networks</strong></a></p></li>
<li><p>Alammar, J. (2018). <a class="reference external" href="https://jalammar.github.io/illustrated-transformer/"><strong>The Illustrated Transformer</strong></a></p></li>
<li><p>Lillicrap, T. P., &amp; Santoro, A. (2019). <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0959438818302009"><strong>Backpropagation through time and the brain</strong></a></p></li>
</ul>
</section>
<section id="video-lectures">
<h4>Video Lectures<a class="headerlink" href="#video-lectures" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Lecture Series: <a class="reference external" href="https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z"><strong>Stanford CS224n: Natural Language Processing with Deep Learning</strong></a></p></li>
<li><p>Lecture Series: <a class="reference external" href="https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF"><strong>DeepMind x UCL: Deep Learning Lectures</strong></a></p></li>
</ul>
</section>
<section id="code-repositories">
<h4>Code Repositories<a class="headerlink" href="#code-repositories" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"><strong>PyTorch Sequence Models Tutorial</strong></a></p></li>
<li><p><a class="reference external" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html"><strong>The Annotated Transformer</strong></a> - Harvard NLP’s implementation of the transformer with detailed annotations</p></li>
<li><p><a class="reference external" href="https://github.com/huggingface/transformers"><strong>Hugging Face Transformers</strong></a> - State-of-the-art transformer implementations</p></li>
<li><p><a class="reference external" href="https://github.com/nasiridrishi/NeuroAI-Papers"><strong>NeuroAI Papers</strong></a> - Curated list of papers at the intersection of neuroscience and AI</p></li>
</ul>
</section>
</section>
<section id="research-communities-and-conferences">
<h3>Research Communities and Conferences<a class="headerlink" href="#research-communities-and-conferences" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Conference on Neural Information Processing Systems (NeurIPS)</p></li>
<li><p>International Conference on Learning Representations (ICLR)</p></li>
<li><p>Organization for Computational Neurosciences (OCNS)</p></li>
<li><p>Cognitive Computational Neuroscience (CCN)</p></li>
</ul>
<p>These resources span from introductory to advanced and cover both theoretical foundations and practical implementations of sequence models and their connections to neuroscience.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "executablebooks/jupyter-book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./part3"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ch10_deep_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 10: Deep Learning: Training &amp; Optimisation</p>
      </div>
    </a>
    <a class="right-next"
       href="../part4/ch12_large_language_models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 12: Large Language Models &amp; Fine-Tuning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-neural-networks">11.1 Recurrent Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vanilla-rnns">11.1.1 Vanilla RNNs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vanishing-exploding-gradient-problem">11.1.2 The Vanishing/Exploding Gradient Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstms-and-grus">11.1.3 LSTMs and GRUs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-architecture">LSTM Architecture</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gru-architecture">GRU Architecture</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bidirectional-rnns">11.1.4 Bidirectional RNNs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-in-neuroscience">11.1.5 Applications in Neuroscience</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanisms">11.2 Attention Mechanisms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-intuition-behind-attention">11.2.1 The Intuition Behind Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">11.2.2 Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">11.2.3 Multi-Head Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-vs-recurrence">11.2.4 Self-Attention vs. Recurrence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention">11.2.5 Scaled Dot-Product Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-visualization">11.2.6 Attention Visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-correlates-of-attention">11.2.7 Neural Correlates of Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-architecture">11.3 Transformer Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-architecture">11.3.1 Overall Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">11.3.2 Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">11.3.3 Decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings">11.3.4 Positional Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-networks">11.3.5 Feed-Forward Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connections-and-layer-normalization">11.3.6 Residual Connections and Layer Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#biological-parallels">11.3.7 Biological Parallels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-sequence-processing">11.4 Neural Sequence Processing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temporal-dynamics-in-cortical-circuits">11.4.1 Temporal Dynamics in Cortical Circuits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#working-memory-mechanisms">11.4.2 Working Memory Mechanisms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-processing">11.4.3 Predictive Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical-temporal-processing">11.4.4 Hierarchical Temporal Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparative-architecture-analysis">11.4.5 Comparative Architecture Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#future-directions">11.4.6 Future Directions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">11.5 Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#natural-language-processing">11.5.1 Natural Language Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-forecasting">11.5.2 Time Series Forecasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-sequence-decoding">11.5.3 Neural Sequence Decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-sequence-models">11.5.4 Generative Sequence Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application-design-principles">11.5.5 Application Design Principles</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-lab">11.6 Code Lab</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-an-lstm-from-components">11.6.1 Implementing an LSTM from Components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-self-attention-mechanism">11.6.2 Building a Self-Attention Mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-small-transformer">11.6.3 Training a Small Transformer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-sequence-prediction">11.6.4 Neural Sequence Prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-solutions">11.6.5 Exercise Solutions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#take-aways">11.7 Take-aways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading-media">11.8 Further Reading &amp; Media</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-papers">Key Papers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#foundational-papers">Foundational Papers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#neuroscience-connections">Neuroscience Connections</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#books">Books</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-resources">Online Resources</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tutorials-and-blog-posts">Tutorials and Blog Posts</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#video-lectures">Video Lectures</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#code-repositories">Code Repositories</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#research-communities-and-conferences">Research Communities and Conferences</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Richard Young
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>