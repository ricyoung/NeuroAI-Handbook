
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 4: Perception Pipeline – Visual Cortex → CNNs &#8212; The Neuroscience of AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'part1/ch04_perception_pipeline';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="https://neuroai-handbook.github.io/part1/ch04_perception_pipeline.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 5: Default-Mode vs Executive Control Networks" href="../part2/ch05_brain_networks.html" />
    <link rel="prev" title="Chapter 3: Spatial Navigation – Place &amp; Grid Cells" href="ch03_spatial_navigation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nai.png" class="logo__image only-light" alt="The Neuroscience of AI - Home"/>
    <script>document.write(`<img src="../_static/nai.png" class="logo__image only-dark" alt="The Neuroscience of AI - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    The Neuroscience of AI
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Cover</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../cover.html">Cover</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I · Brains &amp; Inspiration</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ch01_intro.html">Chapter 1: Introduction to Neuroscience ↔ AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch02_neuro_foundations.html">Chapter 2: Neuroscience Foundations for AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch03_spatial_navigation.html">Chapter 3: Spatial Navigation – Place &amp; Grid Cells</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 4: Perception Pipeline – Visual Cortex → CNNs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II · Brains Meet Math &amp; Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../part2/ch05_brain_networks.html">Chapter 5: Default-Mode vs Executive Control Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part2/ch06_neurostimulation.html">Chapter 6: Neurostimulation &amp; Plasticity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part2/ch07_information_theory.html">Chapter 7: Information Theory Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part2/ch08_data_science_pipeline.html">Chapter 8: Data-Science Pipeline in Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III · Learning Machines</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../part3/ch09_ml_foundations.html">Chapter 9: Classical Machine-Learning Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/ch10_deep_learning.html">Chapter 10: Deep Learning: Training &amp; Optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/ch11_sequence_models.html">Chapter 11: Sequence Models: RNN → Attention → Transformer</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV · Frontier Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../part4/ch12_large_language_models.html">Chapter 12: Large Language Models &amp; Fine-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part4/ch13_multimodal_models.html">Chapter 13: Multimodal &amp; Diffusion Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V · Ethics &amp; Futures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../part5/ch15_ethical_ai.html">Chapter 15: Ethical AI - Considerations for NeuroAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part5/ch16_future_directions.html">Chapter 16: Where Next for Neuro-AI?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part VI · Advanced Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../part6/ch17_bci_human_ai_interfaces.html">Brain-Computer Interfaces and Human-AI Interaction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch18_neuromorphic_computing.html">Chapter 18: Neuromorphic Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch19_cognitive_neuro_dl.html">Cognitive Neuroscience and Deep Learning</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../part6/ch20_case_studies.html">Case Studies in NeuroAI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../part6/ch20_interactive.html">Interactive NeuroAI Case Studies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../part6/jupyter_ai_demo.html">AI-Assisted Learning with Jupyter AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../part6/rise_slides_demo.html">Creating Presentations with RISE</a></li>


</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch21_ai_for_neuro_discovery.html">Chapter 21: AI for Neuroscience Discovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch22_embodied_ai_robotics.html">Chapter 22: Embodied AI and Robotics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch24_quantum_computing_neuroai.html">Chapter 24: Quantum Computing and NeuroAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part6/ch23_lifelong_learning.html">Chapter 23: Lifelong Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../appendices/glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendices/math_python_refresher.html">Appendix A: Math &amp; Python Mini-Refresher</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendices/dataset_catalogue.html">Appendix B: Dataset Catalogue</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendices/colab_setup.html">Appendix C: Google Colab Setup for NeuroAI</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/edit/master/docs/part1/ch04_perception_pipeline.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fpart1/ch04_perception_pipeline.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/part1/ch04_perception_pipeline.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 4: Perception Pipeline – Visual Cortex → CNNs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-goals">4.0 Chapter Goals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-system-architecture">4.1 Visual System Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-retina-to-lgn-to-primary-visual-cortex">4.1.1 From Retina to LGN to Primary Visual Cortex</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ventral-vs-dorsal-streams-what-vs-where">4.1.2 Ventral vs Dorsal Streams: “What” vs “Where”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical-organization-of-visual-cortex">4.1.3 Hierarchical Organization of Visual Cortex</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feedback-connections-and-top-down-processing">4.1.4 Feedback Connections and Top-Down Processing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#receptive-fields">4.2 Receptive Fields</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-complex-and-hypercomplex-cells">4.2.1 Simple, Complex, and Hypercomplex Cells</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gabor-filters-and-orientation-selectivity">4.2.2 Gabor Filters and Orientation Selectivity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#center-surround-organization">4.2.3 Center-Surround Organization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-hierarchy-from-v1-to-it">4.2.4 Feature Hierarchy from V1 to IT</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn-parallels">4.3 CNN Parallels</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-layers-as-receptive-fields">4.3.1 Convolutional Layers as Receptive Fields</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-operations-and-invariance">4.3.2 Pooling Operations and Invariance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-hierarchies-in-deep-networks">4.3.3 Feature Hierarchies in Deep Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-of-cnn-features">4.3.4 Visualization of CNN Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-biology-to-deep-learning">4.4 From Biology to Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hubel-wiesel-s-discoveries-neocognitron-lenet-alexnet">4.4.1 Hubel &amp; Wiesel’s Discoveries → Neocognitron → LeNet → AlexNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#biological-constraints-vs-engineering-solutions">4.4.2 Biological Constraints vs Engineering Solutions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-and-gain-control-mechanisms">4.4.3 Normalization and Gain Control Mechanisms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanisms">4.4.4 Attention Mechanisms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-feedforward-processing">4.5 Beyond Feedforward Processing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrence-in-visual-processing">4.5.1 Recurrence in Visual Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-coding-and-generative-models">4.5.2 Predictive Coding and Generative Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-of-context-and-prior-knowledge">4.5.3 Integration of Context and Prior Knowledge</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-free-energy-principle">4.5.4 The Free Energy Principle</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-lab">4.6 Code Lab</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-simple-gabor-filters">4.6.1 Implementing Simple Gabor Filters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-basic-convolutional-layer">4.6.2 Building a Basic Convolutional Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-visualization-techniques">4.6.3 Feature Visualization Techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-with-pre-trained-cnns">4.6.4 Transfer Learning with Pre-trained CNNs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#take-aways">4.7 Take-aways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading-media">4.8 Further Reading &amp; Media</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#foundational-papers">Foundational Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#biological-vision">Biological Vision</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn-visualization-analysis">CNN Visualization &amp; Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-topics">Advanced Topics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#books">Books</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#in-depth-content">In-Depth Content</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-human-visual-system">The Human Visual System</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#receptive-fields-and-hierarchies">Receptive Fields and Hierarchies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks">Convolutional Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-biological-inspiration">Examples of Biological Inspiration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-pytorch-code-examples">Python/PyTorch Code Examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#suggested-readings">Suggested Readings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supplementary-videos-lectures">Supplementary Videos/Lectures</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-4-perception-pipeline-visual-cortex-cnns">
<h1>Chapter 4: Perception Pipeline – Visual Cortex → CNNs<a class="headerlink" href="#chapter-4-perception-pipeline-visual-cortex-cnns" title="Link to this heading">#</a></h1>
<section id="chapter-goals">
<h2>4.0 Chapter Goals<a class="headerlink" href="#chapter-goals" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Trace the visual processing pipeline from retina to higher cortical areas</p></li>
<li><p>Understand receptive fields and hierarchical feature extraction</p></li>
<li><p>Connect biological vision to convolutional neural networks</p></li>
<li><p>Implement simple visual filters and feature detectors</p></li>
</ul>
</section>
<section id="visual-system-architecture">
<h2>4.1 Visual System Architecture<a class="headerlink" href="#visual-system-architecture" title="Link to this heading">#</a></h2>
<p>The visual system represents one of the most extensively studied sensory pathways in neuroscience, offering rich insights that have directly influenced artificial intelligence. This section explores how visual information flows from the eye through successively more complex processing stages.</p>
<p><img alt="Visual Processing Pathway" src="../_images/visual_pathway.svg" />
<em>Figure 4.1: The visual processing pathway from retina through subcortical structures to cortical areas, showing parallel processing streams.</em></p>
<section id="from-retina-to-lgn-to-primary-visual-cortex">
<h3>4.1.1 From Retina to LGN to Primary Visual Cortex<a class="headerlink" href="#from-retina-to-lgn-to-primary-visual-cortex" title="Link to this heading">#</a></h3>
<p>Visual processing begins in the retina, which is actually an extension of the brain rather than a simple sensor:</p>
<p><strong>Retinal Processing</strong>:</p>
<ul class="simple">
<li><p>Photoreceptors (rods and cones) transduce light into neural signals</p></li>
<li><p>Horizontal, bipolar, and amacrine cells perform initial processing</p></li>
<li><p>Retinal ganglion cells form center-surround receptive fields</p></li>
<li><p>~1 million ganglion cell axons form the optic nerve from each eye</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.patches</span><span class="w"> </span><span class="kn">import</span> <span class="n">Circle</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">ndimage</span>

<span class="k">def</span><span class="w"> </span><span class="nf">center_surround_receptive_field</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">center_sigma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">surround_sigma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">center_weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">surround_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate a center-surround receptive field similar to retinal ganglion cells.&quot;&quot;&quot;</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
    
    <span class="c1"># Create center and surround Gaussians</span>
    <span class="n">center</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">xx</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">yy</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">center_sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">surround</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">xx</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">yy</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">surround_sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    
    <span class="c1"># Combine to form center-surround</span>
    <span class="n">receptive_field</span> <span class="o">=</span> <span class="n">center_weight</span> <span class="o">*</span> <span class="n">center</span> <span class="o">-</span> <span class="n">surround_weight</span> <span class="o">*</span> <span class="n">surround</span>
    
    <span class="k">return</span> <span class="n">receptive_field</span>

<span class="c1"># Create ON-center and OFF-center receptive fields</span>
<span class="n">on_center_rf</span> <span class="o">=</span> <span class="n">center_surround_receptive_field</span><span class="p">(</span><span class="n">center_weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">surround_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">off_center_rf</span> <span class="o">=</span> <span class="o">-</span><span class="n">on_center_rf</span>

<span class="c1"># Visualize the receptive fields</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">im1</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">on_center_rf</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu_r&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;ON-center Receptive Field&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">im2</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">off_center_rf</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu_r&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;OFF-center Receptive Field&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="c1"># plt.show()  # Uncomment to display</span>
</pre></div>
</div>
<p><strong>Lateral Geniculate Nucleus (LGN)</strong>:</p>
<ul class="simple">
<li><p>Subcortical relay station in the thalamus</p></li>
<li><p>Maintains retinotopic organization (spatial mapping)</p></li>
<li><p>Separates input into parallel channels:</p>
<ul>
<li><p>Parvocellular pathway (P cells): High resolution, color</p></li>
<li><p>Magnocellular pathway (M cells): Motion, low contrast sensitivity</p></li>
<li><p>Koniocellular pathway (K cells): Blue-yellow color, other functions</p></li>
</ul>
</li>
</ul>
<p><strong>Primary Visual Cortex (V1)</strong>:</p>
<ul class="simple">
<li><p>Located in the occipital lobe</p></li>
<li><p>First stage of cortical visual processing</p></li>
<li><p>Organized in retinotopic map of visual field</p></li>
<li><p>Divided into six layers with distinct cell types and connections</p></li>
<li><p>Contains orientation-selective cells (simple and complex)</p></li>
<li><p>Processes basic features: orientation, spatial frequency, color, motion</p></li>
</ul>
<p>From V1, information flows to higher visual areas (V2, V3, V4, etc.) that extract increasingly complex features.</p>
</section>
<section id="ventral-vs-dorsal-streams-what-vs-where">
<h3>4.1.2 Ventral vs Dorsal Streams: “What” vs “Where”<a class="headerlink" href="#ventral-vs-dorsal-streams-what-vs-where" title="Link to this heading">#</a></h3>
<p>Visual processing bifurcates into two main pathways beyond V1 and V2:</p>
<p><strong>Ventral Stream (the “What” pathway)</strong>:</p>
<ul class="simple">
<li><p>Flows from V1 → V2 → V4 → Inferior Temporal (IT) cortex</p></li>
<li><p>Specializes in object recognition and form processing</p></li>
<li><p>Receptive fields increase in size and complexity along the pathway</p></li>
<li><p>Culminates in cells that respond to complex objects regardless of position, size, or lighting</p></li>
<li><p>Lesions cause object recognition deficits (visual agnosia)</p></li>
</ul>
<p><strong>Dorsal Stream (the “Where”/”How” pathway)</strong>:</p>
<ul class="simple">
<li><p>Flows from V1 → V2 → V3 → Middle Temporal (MT/V5) → Posterior Parietal cortex</p></li>
<li><p>Specializes in spatial relationships and motion</p></li>
<li><p>Processes location, movement, and action-relevant properties</p></li>
<li><p>Critical for visuomotor coordination and spatial attention</p></li>
<li><p>Lesions cause spatial awareness deficits (e.g., hemispatial neglect)</p></li>
</ul>
<p>This dual-pathway organization has inspired various dual-stream architectures in artificial intelligence, particularly for tasks requiring both recognition and localization.</p>
</section>
<section id="hierarchical-organization-of-visual-cortex">
<h3>4.1.3 Hierarchical Organization of Visual Cortex<a class="headerlink" href="#hierarchical-organization-of-visual-cortex" title="Link to this heading">#</a></h3>
<p>The visual cortex exhibits a clear hierarchical organization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">display_visual_hierarchy</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Display the visual processing hierarchy and key properties.&quot;&quot;&quot;</span>
    <span class="n">visual_areas</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;area&quot;</span><span class="p">:</span> <span class="s2">&quot;V1&quot;</span><span class="p">,</span>
            <span class="s2">&quot;receptive_field_size&quot;</span><span class="p">:</span> <span class="s2">&quot;~1°&quot;</span><span class="p">,</span>
            <span class="s2">&quot;feature_complexity&quot;</span><span class="p">:</span> <span class="s2">&quot;Oriented edges, bars, gratings&quot;</span><span class="p">,</span>
            <span class="s2">&quot;invariance&quot;</span><span class="p">:</span> <span class="s2">&quot;Minimal position invariance&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;area&quot;</span><span class="p">:</span> <span class="s2">&quot;V2&quot;</span><span class="p">,</span>
            <span class="s2">&quot;receptive_field_size&quot;</span><span class="p">:</span> <span class="s2">&quot;~2-4°&quot;</span><span class="p">,</span>
            <span class="s2">&quot;feature_complexity&quot;</span><span class="p">:</span> <span class="s2">&quot;Contours, textures, illusory contours&quot;</span><span class="p">,</span>
            <span class="s2">&quot;invariance&quot;</span><span class="p">:</span> <span class="s2">&quot;Limited position invariance&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;area&quot;</span><span class="p">:</span> <span class="s2">&quot;V4&quot;</span><span class="p">,</span>
            <span class="s2">&quot;receptive_field_size&quot;</span><span class="p">:</span> <span class="s2">&quot;~4-8°&quot;</span><span class="p">,</span>
            <span class="s2">&quot;feature_complexity&quot;</span><span class="p">:</span> <span class="s2">&quot;Shape fragments, curvature, color&quot;</span><span class="p">,</span>
            <span class="s2">&quot;invariance&quot;</span><span class="p">:</span> <span class="s2">&quot;Moderate position and size invariance&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;area&quot;</span><span class="p">:</span> <span class="s2">&quot;IT (PIT/CIT/AIT)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;receptive_field_size&quot;</span><span class="p">:</span> <span class="s2">&quot;~10-30°&quot;</span><span class="p">,</span>
            <span class="s2">&quot;feature_complexity&quot;</span><span class="p">:</span> <span class="s2">&quot;Object parts, faces, complex shapes&quot;</span><span class="p">,</span>
            <span class="s2">&quot;invariance&quot;</span><span class="p">:</span> <span class="s2">&quot;Strong invariance to position, size, and viewpoint&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Visual Processing Hierarchy - Ventral Stream&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Area&#39;</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;RF Size&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Feature Complexity&#39;</span><span class="si">:</span><span class="s2">&lt;35</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Invariance&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">area</span> <span class="ow">in</span> <span class="n">visual_areas</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">area</span><span class="p">[</span><span class="s1">&#39;area&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">area</span><span class="p">[</span><span class="s1">&#39;receptive_field_size&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">area</span><span class="p">[</span><span class="s1">&#39;feature_complexity&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;35</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">area</span><span class="p">[</span><span class="s1">&#39;invariance&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Key principles of this hierarchical organization include:</p>
<ol class="arabic simple">
<li><p><strong>Progressive Abstraction</strong>: Higher areas represent more complex features and objects</p></li>
<li><p><strong>Increasing Receptive Field Size</strong>: Receptive fields become larger at higher levels</p></li>
<li><p><strong>Growing Invariance</strong>: Higher areas show greater invariance to transformations (position, size, etc.)</p></li>
<li><p><strong>Declining Retinotopy</strong>: Spatial organization becomes less precise in higher areas</p></li>
<li><p><strong>Increasing Response Latency</strong>: Processing time increases along the hierarchy</p></li>
</ol>
</section>
<section id="feedback-connections-and-top-down-processing">
<h3>4.1.4 Feedback Connections and Top-Down Processing<a class="headerlink" href="#feedback-connections-and-top-down-processing" title="Link to this heading">#</a></h3>
<p>While the visual system is often explained as a feedforward hierarchy, feedback connections are equally important:</p>
<ul class="simple">
<li><p>Feedback projections outnumber feedforward connections in the visual cortex</p></li>
<li><p>Higher areas send predictions to lower areas</p></li>
<li><p>Feedback modulates response properties and selectivity</p></li>
<li><p>Facilitates attention, expectation, and context effects</p></li>
<li><p>Enables perceptual completion and object disambiguation</p></li>
</ul>
<p>These recurrent connections enable the visual system to integrate prior knowledge and context with incoming sensory signals, forming the basis for predictive processing frameworks.</p>
</section>
</section>
<section id="receptive-fields">
<h2>4.2 Receptive Fields<a class="headerlink" href="#receptive-fields" title="Link to this heading">#</a></h2>
<p>Receptive fields—the regions of visual space that influence a neuron’s response—are fundamental to understanding visual processing in both biological and artificial systems.</p>
<p><img alt="Receptive Field Types" src="../_images/receptive_fields.svg" />
<em>Figure 4.2: Different types of receptive fields in the visual pathway, from center-surround to oriented bars to complex pattern detectors.</em></p>
<section id="simple-complex-and-hypercomplex-cells">
<h3>4.2.1 Simple, Complex, and Hypercomplex Cells<a class="headerlink" href="#simple-complex-and-hypercomplex-cells" title="Link to this heading">#</a></h3>
<p>Hubel and Wiesel’s groundbreaking work in the 1960s identified three main classes of cells in the visual cortex:</p>
<p><strong>Simple Cells</strong>:</p>
<ul class="simple">
<li><p>Linear summation of inputs within precisely defined excitatory and inhibitory regions</p></li>
<li><p>Selective for orientation, position, and size of stimuli</p></li>
<li><p>Predictable responses based on exact stimulus location</p></li>
<li><p>Example: cells that respond to bars or edges with specific orientations at precise locations</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">simple_cell_model</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">orientation_deg</span><span class="p">,</span> <span class="n">sigma_x</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sigma_y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">center_x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">center_y</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simulate a simple cell in V1 using a Gabor filter.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        image: Input image (grayscale)</span>
<span class="sd">        orientation_deg: Preferred orientation in degrees</span>
<span class="sd">        sigma_x, sigma_y: Gaussian envelope widths</span>
<span class="sd">        center_x, center_y: Center coordinates</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Simple cell response to the input image</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Convert orientation to radians</span>
    <span class="n">orientation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">deg2rad</span><span class="p">(</span><span class="n">orientation_deg</span><span class="p">)</span>
    
    <span class="c1"># Create a grid of coordinates</span>
    <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">cols</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">rows</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">cols</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="n">center_x</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">rows</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="n">center_y</span>
    
    <span class="c1"># Rotate coordinates</span>
    <span class="n">x_theta</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">orientation</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">orientation</span><span class="p">)</span>
    <span class="n">y_theta</span> <span class="o">=</span> <span class="o">-</span><span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">orientation</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">orientation</span><span class="p">)</span>
    
    <span class="c1"># Gabor filter components</span>
    <span class="n">gaussian</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_theta</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_theta</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_y</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
    <span class="n">sinusoid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_theta</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">sigma_x</span><span class="p">))</span>
    
    <span class="c1"># Gabor filter (receptive field)</span>
    <span class="n">gabor</span> <span class="o">=</span> <span class="n">gaussian</span> <span class="o">*</span> <span class="n">sinusoid</span>
    
    <span class="c1"># Apply the filter (convolve with the image)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">ndimage</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="n">gabor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">response</span><span class="p">,</span> <span class="n">gabor</span>

<span class="c1"># Example usage (with a synthetic edge image)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_edge_image</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">angle_deg</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">edge_position</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">edge_width</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a simple edge image for demonstration.&quot;&quot;&quot;</span>
    <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">deg2rad</span><span class="p">(</span><span class="n">angle_deg</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span>
    
    <span class="c1"># Rotate coordinates</span>
    <span class="n">x_rot</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span>
    
    <span class="c1"># Create edge</span>
    <span class="n">edge</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_rot</span> <span class="o">&gt;</span> <span class="n">edge_position</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
    
    <span class="c1"># Smooth the edge slightly</span>
    <span class="n">edge</span> <span class="o">=</span> <span class="n">ndimage</span><span class="o">.</span><span class="n">gaussian_filter</span><span class="p">(</span><span class="n">edge</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">edge_width</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">edge</span>
</pre></div>
</div>
<p><strong>Complex Cells</strong>:</p>
<ul class="simple">
<li><p>Respond to oriented edges and bars regardless of exact position</p></li>
<li><p>Maintain orientation selectivity but exhibit spatial invariance</p></li>
<li><p>Often modeled as combining outputs from multiple simple cells</p></li>
<li><p>Example: cells that respond to a vertical edge anywhere within their receptive field</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">complex_cell_model</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">orientation_deg</span><span class="p">,</span> <span class="n">sigma_x</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sigma_y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">positions</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">spacing</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simulate a complex cell by combining responses from multiple simple cells.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        image: Input image</span>
<span class="sd">        orientation_deg: Preferred orientation</span>
<span class="sd">        sigma_x, sigma_y: Filter parameters</span>
<span class="sd">        positions: Number of simple cells to combine</span>
<span class="sd">        spacing: Spacing between simple cell centers</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Complex cell response</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Generate responses from multiple simple cells at different positions</span>
    <span class="n">responses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">positions</span><span class="p">):</span>
        <span class="c1"># Calculate position offset</span>
        <span class="n">offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">positions</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">spacing</span>
        
        <span class="c1"># Get simple cell response</span>
        <span class="n">simple_response</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">simple_cell_model</span><span class="p">(</span>
            <span class="n">image</span><span class="p">,</span> <span class="n">orientation_deg</span><span class="p">,</span> <span class="n">sigma_x</span><span class="p">,</span> <span class="n">sigma_y</span><span class="p">,</span> <span class="n">center_x</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">center_y</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">responses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">simple_response</span><span class="p">)</span>
    
    <span class="c1"># Complex cell combines simple cell responses (maximum response at each point)</span>
    <span class="n">complex_response</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">responses</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">complex_response</span>
</pre></div>
</div>
<p><strong>Hypercomplex (End-Stopped) Cells</strong>:</p>
<ul class="simple">
<li><p>Selective for specific stimulus properties beyond orientation</p></li>
<li><p>Respond to line endings, curvature, or corners</p></li>
<li><p>Can be direction-selective or sensitive to specific motion patterns</p></li>
<li><p>Example: cells that respond to a line segment of specific length but not to a longer line</p></li>
</ul>
<p>These cell types implement a hierarchical feature detection system, where complex cells build invariance to position, and hypercomplex cells detect more specialized features needed for object recognition.</p>
</section>
<section id="gabor-filters-and-orientation-selectivity">
<h3>4.2.2 Gabor Filters and Orientation Selectivity<a class="headerlink" href="#gabor-filters-and-orientation-selectivity" title="Link to this heading">#</a></h3>
<p>The orientation selectivity of V1 cells is well-modeled by Gabor filters—sinusoidal gratings modulated by a Gaussian envelope:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">visualize_gabor_filters</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize Gabor filters at different orientations and frequencies.&quot;&quot;&quot;</span>
    <span class="n">orientations</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">135</span><span class="p">]</span>
    <span class="n">frequencies</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">frequencies</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">orientations</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">frequencies</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">orient</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">orientations</span><span class="p">):</span>
            <span class="c1"># Create Gabor filter</span>
            <span class="n">sigma_x</span> <span class="o">=</span> <span class="mi">10</span>
            <span class="n">sigma_y</span> <span class="o">=</span> <span class="mi">10</span>
            
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">))</span>
            
            <span class="c1"># Rotate coordinates</span>
            <span class="n">orient_rad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">deg2rad</span><span class="p">(</span><span class="n">orient</span><span class="p">)</span>
            <span class="n">x_theta</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">orient_rad</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">orient_rad</span><span class="p">)</span>
            <span class="n">y_theta</span> <span class="o">=</span> <span class="o">-</span><span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">orient_rad</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">orient_rad</span><span class="p">)</span>
            
            <span class="c1"># Create Gabor filter</span>
            <span class="n">gaussian</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_theta</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y_theta</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma_x</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
            <span class="n">sinusoid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">freq</span> <span class="o">*</span> <span class="n">x_theta</span><span class="p">)</span>
            <span class="n">gabor</span> <span class="o">=</span> <span class="n">gaussian</span> <span class="o">*</span> <span class="n">sinusoid</span>
            
            <span class="c1"># Display the filter</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">gabor</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu_r&#39;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">orient</span><span class="si">}</span><span class="s2">°, f=</span><span class="si">{</span><span class="n">freq</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="c1"># plt.show()  # Uncomment to display</span>
</pre></div>
</div>
<p>Key properties of Gabor filters that match V1 neurophysiology:</p>
<ol class="arabic simple">
<li><p><strong>Orientation Selectivity</strong>: Responds strongly to edges/gratings at a preferred orientation</p></li>
<li><p><strong>Spatial Frequency Selectivity</strong>: Tuned to specific scales or frequencies</p></li>
<li><p><strong>Localization</strong>: Well-defined in both spatial and frequency domains</p></li>
<li><p><strong>Phase Sensitivity</strong>: Different cells prefer different phases of gratings</p></li>
<li><p><strong>Bandwidth</strong>: Cells have specific tuning widths for orientation and frequency</p></li>
</ol>
<p>The orientation preference of neurons in V1 is organized in a systematic way:</p>
<ul class="simple">
<li><p>Cells with similar orientation preferences cluster in “orientation columns”</p></li>
<li><p>Adjacent columns represent gradually changing orientations, forming “pinwheels”</p></li>
<li><p>This organization efficiently covers the space of all possible orientations</p></li>
</ul>
</section>
<section id="center-surround-organization">
<h3>4.2.3 Center-Surround Organization<a class="headerlink" href="#center-surround-organization" title="Link to this heading">#</a></h3>
<p>Center-surround receptive fields are foundational to early visual processing:</p>
<p><strong>Retinal Ganglion Cells</strong>:</p>
<ul class="simple">
<li><p>ON-center cells: Excited by light in center, inhibited by light in surround</p></li>
<li><p>OFF-center cells: Inhibited by light in center, excited by light in surround</p></li>
<li><p>Function as contrast detectors and edge enhancers</p></li>
<li><p>Perform efficient coding by reducing redundancy in natural images</p></li>
</ul>
<p><strong>LGN Cells</strong>:</p>
<ul class="simple">
<li><p>Maintain the center-surround organization from retinal inputs</p></li>
<li><p>P cells: Small receptive fields, color-opponent (red-green)</p></li>
<li><p>M cells: Larger receptive fields, higher contrast sensitivity</p></li>
<li><p>K cells: Blue-yellow color opponency</p></li>
</ul>
<p>This center-surround organization implements a form of local normalization and contrast enhancement that is also found in early layers of CNNs.</p>
</section>
<section id="feature-hierarchy-from-v1-to-it">
<h3>4.2.4 Feature Hierarchy from V1 to IT<a class="headerlink" href="#feature-hierarchy-from-v1-to-it" title="Link to this heading">#</a></h3>
<p>As we move from V1 through the ventral stream to IT cortex, cells respond to increasingly complex features:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">feature_hierarchy_visual</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualization of the feature hierarchy in the ventral visual stream.&quot;&quot;&quot;</span>
    <span class="n">areas</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;V1&quot;</span><span class="p">,</span> <span class="s2">&quot;V2&quot;</span><span class="p">,</span> <span class="s2">&quot;V4&quot;</span><span class="p">,</span> <span class="s2">&quot;IT&quot;</span><span class="p">]</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;Oriented edges, bars&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Contours, textures&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Shape fragments, curves&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Objects, faces, scenes&quot;</span>
    <span class="p">]</span>
    
    <span class="c1"># This would generate a visualization in a real notebook</span>
    <span class="n">feature_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">areas</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">features</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">areas</span><span class="p">))}</span>
    <span class="k">return</span> <span class="n">feature_map</span>
</pre></div>
</div>
<p><strong>V1</strong>: Orientation, spatial frequency, edges, local features</p>
<p><strong>V2</strong>: Combinations of orientations, contours, textures</p>
<ul class="simple">
<li><p>Responds to illusory contours</p></li>
<li><p>Sensitive to border ownership</p></li>
<li><p>Begins processing figure-ground relationships</p></li>
</ul>
<p><strong>V4</strong>: Shape fragments, curvature, color</p>
<ul class="simple">
<li><p>Object-part selective responses</p></li>
<li><p>Moderate invariance to size and position</p></li>
<li><p>Attention strongly modulates responses</p></li>
</ul>
<p><strong>IT Cortex</strong>: Objects, faces, complex patterns</p>
<ul class="simple">
<li><p>Cells selective for specific object categories</p></li>
<li><p>“Grandmother cells” that respond to particular face identities</p></li>
<li><p>High invariance to transformations (position, size, viewpoint)</p></li>
<li><p>Organized in specialized modules (e.g., fusiform face area)</p></li>
</ul>
<p>This hierarchical feature organization is a key principle that directly inspired convolutional neural networks.</p>
</section>
</section>
<section id="cnn-parallels">
<h2>4.3 CNN Parallels<a class="headerlink" href="#cnn-parallels" title="Link to this heading">#</a></h2>
<p>Convolutional Neural Networks (CNNs) incorporate many principles from the visual system, representing a striking example of neuroscience-inspired AI.</p>
<p><img alt="CNN and Visual Cortex" src="../_images/cnn_visual_cortex.svg" />
<em>Figure 4.3: Parallels between the visual cortex hierarchy and layers of a convolutional neural network.</em></p>
<section id="convolutional-layers-as-receptive-fields">
<h3>4.3.1 Convolutional Layers as Receptive Fields<a class="headerlink" href="#convolutional-layers-as-receptive-fields" title="Link to this heading">#</a></h3>
<p>Convolutional layers in CNNs implement the same basic principle as receptive fields in the visual cortex:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SimpleConvNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleConvNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># First convolutional layer - similar to V1 simple cells</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Second convolutional layer - similar to V2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        
        <span class="c1"># Fully connected layers - similar to higher visual areas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># 10 output classes</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Conv + ReLU + Max Pooling: detection + spatial invariance</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Flatten to vector for fully connected layers</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
        
        <span class="c1"># Fully connected layers with ReLU</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Create a simple CNN</span>
<span class="n">simple_cnn</span> <span class="o">=</span> <span class="n">SimpleConvNet</span><span class="p">()</span>

<span class="c1"># Initialize with random image (1 channel, 28x28 pixels)</span>
<span class="n">sample_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>

<span class="c1"># Visualize the first layer filters after training (in a real notebook)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize_filters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize the filters learned in the first convolutional layer.&quot;&quot;&quot;</span>
    <span class="c1"># Extract filters from the first layer</span>
    <span class="n">filters</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="c1"># For demonstration purpose, just return the filter shapes</span>
    <span class="k">return</span> <span class="n">filters</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<p>Key parallels between convolutional layers and visual receptive fields:</p>
<ol class="arabic simple">
<li><p><strong>Local Connectivity</strong>: Convolutional filters only connect to a small region of the input, just like receptive fields</p></li>
<li><p><strong>Weight Sharing</strong>: The same filter is applied across the entire input, similar to how V1 cells with the same orientation preference process different parts of the visual field</p></li>
<li><p><strong>Hierarchical Processing</strong>: Deeper layers process the outputs of earlier layers to detect more complex patterns</p></li>
<li><p><strong>Feature Detectors</strong>: First-layer filters often learn Gabor-like patterns similar to V1 cells</p></li>
<li><p><strong>Parameter Efficiency</strong>: Both systems reduce parameters through spatial weight sharing</p></li>
</ol>
</section>
<section id="pooling-operations-and-invariance">
<h3>4.3.2 Pooling Operations and Invariance<a class="headerlink" href="#pooling-operations-and-invariance" title="Link to this heading">#</a></h3>
<p>Pooling layers in CNNs create invariance to small transformations, analogous to complex cells in V1:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_pooling</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate how pooling creates invariance to small translations.&quot;&quot;&quot;</span>
    <span class="c1"># Create a small translated version of the image</span>
    <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">translation</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Translate by 1 pixel</span>
    
    <span class="n">translated_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">translated_image</span><span class="p">[</span><span class="n">translation</span><span class="p">:,</span> <span class="n">translation</span><span class="p">:]</span> <span class="o">=</span> <span class="n">image</span><span class="p">[:</span><span class="o">-</span><span class="n">translation</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="n">translation</span><span class="p">]</span>
    
    <span class="c1"># Apply max pooling to both images</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">max_pool</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">pool_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">stride</span><span class="p">,</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="n">stride</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">pool_size</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">pool_size</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
                <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="n">stride</span><span class="p">,</span> <span class="n">j</span><span class="o">//</span><span class="n">stride</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">pool_size</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">pool_size</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span>
    
    <span class="n">pooled_orig</span> <span class="o">=</span> <span class="n">max_pool</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">pool_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="n">pooled_trans</span> <span class="o">=</span> <span class="n">max_pool</span><span class="p">(</span><span class="n">translated_image</span><span class="p">,</span> <span class="n">pool_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    
    <span class="c1"># Compute similarity between pooled representations</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">pooled_orig</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">pooled_trans</span><span class="o">.</span><span class="n">flatten</span><span class="p">())[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># Return results (would visualize in a real notebook)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;original_shape&quot;</span><span class="p">:</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="s2">&quot;pooled_shape&quot;</span><span class="p">:</span> <span class="n">pooled_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="s2">&quot;similarity_after_pooling&quot;</span><span class="p">:</span> <span class="n">similarity</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Parallels between pooling and biological visual processing:</p>
<ol class="arabic simple">
<li><p><strong>Translation Invariance</strong>: Max pooling creates robustness to exact positioning, similar to complex cells</p></li>
<li><p><strong>Dimensionality Reduction</strong>: Pooling reduces spatial dimensions, similar to increasing receptive field sizes in higher visual areas</p></li>
<li><p><strong>Hierarchical Invariance</strong>: Stacking convolutional and pooling layers creates increasing invariance, mirroring the ventral stream</p></li>
<li><p><strong>Feature Integration</strong>: Pooling helps integrate features over local regions</p></li>
</ol>
<p>The increasing invariance to transformations is a key feature of the ventral stream that CNNs replicate through their architecture.</p>
</section>
<section id="feature-hierarchies-in-deep-networks">
<h3>4.3.3 Feature Hierarchies in Deep Networks<a class="headerlink" href="#feature-hierarchies-in-deep-networks" title="Link to this heading">#</a></h3>
<p>Deep CNNs develop feature hierarchies strikingly similar to the visual cortex:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">visualize_cnn_hierarchy</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate the hierarchy of features learned in different CNN layers.&quot;&quot;&quot;</span>
    
    <span class="n">cnn_hierarchy</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;layer1&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;feature_type&quot;</span><span class="p">:</span> <span class="s2">&quot;Edges, textures, colors&quot;</span><span class="p">,</span>
            <span class="s2">&quot;biological_parallel&quot;</span><span class="p">:</span> <span class="s2">&quot;V1 simple cells (oriented edge detectors)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;receptive_field&quot;</span><span class="p">:</span> <span class="s2">&quot;Small (e.g., 5x5 pixels)&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;layer2&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;feature_type&quot;</span><span class="p">:</span> <span class="s2">&quot;Corners, contours, simple textures&quot;</span><span class="p">,</span>
            <span class="s2">&quot;biological_parallel&quot;</span><span class="p">:</span> <span class="s2">&quot;V2 cells (contour integration)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;receptive_field&quot;</span><span class="p">:</span> <span class="s2">&quot;Medium (effective size grows)&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;layer3&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;feature_type&quot;</span><span class="p">:</span> <span class="s2">&quot;Object parts, complex textures&quot;</span><span class="p">,</span>
            <span class="s2">&quot;biological_parallel&quot;</span><span class="p">:</span> <span class="s2">&quot;V4 cells (shape fragments)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;receptive_field&quot;</span><span class="p">:</span> <span class="s2">&quot;Large (encompasses significant portion of image)&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;layer4&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;feature_type&quot;</span><span class="p">:</span> <span class="s2">&quot;Object detectors, category-specific features&quot;</span><span class="p">,</span>
            <span class="s2">&quot;biological_parallel&quot;</span><span class="p">:</span> <span class="s2">&quot;IT cells (object and category selective)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;receptive_field&quot;</span><span class="p">:</span> <span class="s2">&quot;Very large (most of the image)&quot;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">cnn_hierarchy</span>
</pre></div>
</div>
<p>Zeiler and Fergus (2014) demonstrated this hierarchy by visualizing features learned at different layers of a CNN:</p>
<ol class="arabic simple">
<li><p><strong>First Layer</strong>: Oriented edges, color blobs (like V1)</p></li>
<li><p><strong>Middle Layers</strong>: Textures, patterns, object parts (like V2/V4)</p></li>
<li><p><strong>Deep Layers</strong>: Object detectors, category-specific features (like IT)</p></li>
</ol>
<p>This emergent hierarchy develops through training, without being explicitly programmed—a case of convergent evolution between biological and artificial systems.</p>
</section>
<section id="visualization-of-cnn-features">
<h3>4.3.4 Visualization of CNN Features<a class="headerlink" href="#visualization-of-cnn-features" title="Link to this heading">#</a></h3>
<p>Techniques to visualize CNN features have revealed striking similarities to biological vision:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">feature_visualization_techniques</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Summarize techniques for visualizing CNN features.&quot;&quot;&quot;</span>
    
    <span class="n">techniques</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Activation Maximization&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Optimize input to maximize activation of specific neurons&quot;</span><span class="p">,</span>
            <span class="s2">&quot;insight&quot;</span><span class="p">:</span> <span class="s2">&quot;Reveals preferred stimuli of units, similar to neurophysiology experiments&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Deconvolution &amp; Guided Backprop&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Back-project activations to input space to visualize what activated units&quot;</span><span class="p">,</span>
            <span class="s2">&quot;insight&quot;</span><span class="p">:</span> <span class="s2">&quot;Shows what patterns in input space drive specific features&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Feature Inversion&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Reconstruct image from feature representations&quot;</span><span class="p">,</span>
            <span class="s2">&quot;insight&quot;</span><span class="p">:</span> <span class="s2">&quot;Reveals what information is preserved at each layer&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;GradCAM&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;approach&quot;</span><span class="p">:</span> <span class="s2">&quot;Use gradients to identify important regions for classification&quot;</span><span class="p">,</span>
            <span class="s2">&quot;insight&quot;</span><span class="p">:</span> <span class="s2">&quot;Shows attention-like focus on diagnostic image regions&quot;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">techniques</span>
</pre></div>
</div>
<p>These visualization techniques have revealed that:</p>
<ol class="arabic simple">
<li><p>Early CNN layers develop Gabor-like filters similar to V1 neurons</p></li>
<li><p>Intermediate layers detect combinations of edges and textures</p></li>
<li><p>Deep layers contain units selective for specific objects or parts</p></li>
<li><p>CNNs develop specialized detectors for faces, text, and other categories</p></li>
</ol>
<p>The similarity between CNN feature visualization and neurophysiological recordings is one of the most compelling connections between artificial and biological vision.</p>
</section>
</section>
<section id="from-biology-to-deep-learning">
<h2>4.4 From Biology to Deep Learning<a class="headerlink" href="#from-biology-to-deep-learning" title="Link to this heading">#</a></h2>
<p>The development of CNNs represents a clear example of neuroscience directly inspiring AI advances.</p>
<section id="hubel-wiesel-s-discoveries-neocognitron-lenet-alexnet">
<h3>4.4.1 Hubel &amp; Wiesel’s Discoveries → Neocognitron → LeNet → AlexNet<a class="headerlink" href="#hubel-wiesel-s-discoveries-neocognitron-lenet-alexnet" title="Link to this heading">#</a></h3>
<p>The lineage from visual neuroscience to modern deep learning is direct and well-documented:</p>
<p><strong>Hubel &amp; Wiesel (1960s)</strong>:</p>
<ul class="simple">
<li><p>Discovered simple and complex cells in cat visual cortex</p></li>
<li><p>Identified hierarchical processing and receptive fields</p></li>
<li><p>Won Nobel Prize in 1981 for this groundbreaking work</p></li>
</ul>
<p><strong>Neocognitron (Fukushima, 1980)</strong>:</p>
<ul class="simple">
<li><p>First computational model directly inspired by Hubel &amp; Wiesel</p></li>
<li><p>Introduced alternating feature detection and pooling layers</p></li>
<li><p>Pioneered the basic CNN architecture but lacked effective training</p></li>
</ul>
<p><strong>LeNet (LeCun et al., 1989-1998)</strong>:</p>
<ul class="simple">
<li><p>Added backpropagation to train the network end-to-end</p></li>
<li><p>Developed for handwritten digit recognition (MNIST)</p></li>
<li><p>Demonstrated the effectiveness of convolutional architecture</p></li>
<li><p>Limited by computational constraints of the era</p></li>
</ul>
<p><strong>AlexNet (Krizhevsky et al., 2012)</strong>:</p>
<ul class="simple">
<li><p>Scaled up CNN approach with GPU implementation</p></li>
<li><p>Won ImageNet competition by large margin</p></li>
<li><p>Triggered the deep learning revolution</p></li>
<li><p>Maintained the same biological inspiration from earlier work</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cnn_evolution_timeline</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a timeline of CNN evolution from biological inspiration to modern architectures.&quot;&quot;&quot;</span>
    
    <span class="n">timeline</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;year&quot;</span><span class="p">:</span> <span class="s2">&quot;1959-1968&quot;</span><span class="p">,</span> <span class="s2">&quot;development&quot;</span><span class="p">:</span> <span class="s2">&quot;Hubel &amp; Wiesel discover simple and complex cells in visual cortex&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;year&quot;</span><span class="p">:</span> <span class="s2">&quot;1980&quot;</span><span class="p">,</span> <span class="s2">&quot;development&quot;</span><span class="p">:</span> <span class="s2">&quot;Fukushima&#39;s Neocognitron: First hierarchical convolutional architecture&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;year&quot;</span><span class="p">:</span> <span class="s2">&quot;1989&quot;</span><span class="p">,</span> <span class="s2">&quot;development&quot;</span><span class="p">:</span> <span class="s2">&quot;LeCun introduces backpropagation for training CNNs&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;year&quot;</span><span class="p">:</span> <span class="s2">&quot;1998&quot;</span><span class="p">,</span> <span class="s2">&quot;development&quot;</span><span class="p">:</span> <span class="s2">&quot;LeNet-5 demonstrates effective digit recognition&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;year&quot;</span><span class="p">:</span> <span class="s2">&quot;2012&quot;</span><span class="p">,</span> <span class="s2">&quot;development&quot;</span><span class="p">:</span> <span class="s2">&quot;AlexNet wins ImageNet, starts deep learning revolution&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;year&quot;</span><span class="p">:</span> <span class="s2">&quot;2014&quot;</span><span class="p">,</span> <span class="s2">&quot;development&quot;</span><span class="p">:</span> <span class="s2">&quot;VGGNet and GoogLeNet deepen and refine architectures&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;year&quot;</span><span class="p">:</span> <span class="s2">&quot;2015&quot;</span><span class="p">,</span> <span class="s2">&quot;development&quot;</span><span class="p">:</span> <span class="s2">&quot;ResNet introduces skip connections, enabling very deep networks&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;year&quot;</span><span class="p">:</span> <span class="s2">&quot;2017+&quot;</span><span class="p">,</span> <span class="s2">&quot;development&quot;</span><span class="p">:</span> <span class="s2">&quot;Advanced architectures (DenseNet, EfficientNet) optimize CNN design&quot;</span><span class="p">}</span>
    <span class="p">]</span>
    
    <span class="k">return</span> <span class="n">timeline</span>
</pre></div>
</div>
<p>This direct lineage demonstrates how neuroscience insights laid the foundation for transformative AI advances decades later.</p>
</section>
<section id="biological-constraints-vs-engineering-solutions">
<h3>4.4.2 Biological Constraints vs Engineering Solutions<a class="headerlink" href="#biological-constraints-vs-engineering-solutions" title="Link to this heading">#</a></h3>
<p>While CNNs draw inspiration from biology, they also diverge in significant ways:</p>
<p><strong>Biological Features Maintained</strong>:</p>
<ul class="simple">
<li><p>Local receptive fields and hierarchical processing</p></li>
<li><p>Feature detection followed by pooling for invariance</p></li>
<li><p>Increasingly complex feature representations</p></li>
<li><p>Parallel processing of different visual attributes</p></li>
</ul>
<p><strong>Engineering Departures</strong>:</p>
<ul class="simple">
<li><p>Backpropagation (biologically implausible in its standard form)</p></li>
<li><p>Separate forward and backward passes</p></li>
<li><p>Weight sharing across entire feature maps</p></li>
<li><p>Supervised training with labeled examples</p></li>
<li><p>Batch normalization and other engineering optimizations</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compare_bio_vs_engineering</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare biological visual systems with CNN engineering approaches.&quot;&quot;&quot;</span>
    
    <span class="n">comparison</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Learning&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Biology&quot;</span><span class="p">:</span> <span class="s2">&quot;Hebbian learning, STDP, reinforcement, unsupervised&quot;</span><span class="p">,</span>
            <span class="s2">&quot;CNNs&quot;</span><span class="p">:</span> <span class="s2">&quot;Backpropagation, supervised learning with labels&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Architecture&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Biology&quot;</span><span class="p">:</span> <span class="s2">&quot;Recurrent connections, feedback pathways, modulatory inputs&quot;</span><span class="p">,</span>
            <span class="s2">&quot;CNNs&quot;</span><span class="p">:</span> <span class="s2">&quot;Primarily feedforward, limited recurrence in some designs&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Activation&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Biology&quot;</span><span class="p">:</span> <span class="s2">&quot;Spiking neurons with temporal dynamics&quot;</span><span class="p">,</span>
            <span class="s2">&quot;CNNs&quot;</span><span class="p">:</span> <span class="s2">&quot;Continuous-valued activations (ReLU, etc.)&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Efficiency&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Biology&quot;</span><span class="p">:</span> <span class="s2">&quot;Extremely energy efficient, sparse activity&quot;</span><span class="p">,</span>
            <span class="s2">&quot;CNNs&quot;</span><span class="p">:</span> <span class="s2">&quot;Computationally intensive, dense computation&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Learning Speed&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Biology&quot;</span><span class="p">:</span> <span class="s2">&quot;Fast one-shot learning for some tasks&quot;</span><span class="p">,</span>
            <span class="s2">&quot;CNNs&quot;</span><span class="p">:</span> <span class="s2">&quot;Requires many examples, but improving with few-shot methods&quot;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">comparison</span>
</pre></div>
</div>
<p>These differences represent engineering pragmatism: CNNs adopt biologically-inspired principles where helpful but diverge where engineering solutions are more effective.</p>
</section>
<section id="normalization-and-gain-control-mechanisms">
<h3>4.4.3 Normalization and Gain Control Mechanisms<a class="headerlink" href="#normalization-and-gain-control-mechanisms" title="Link to this heading">#</a></h3>
<p>Both biological and artificial visual systems implement normalization:</p>
<p><strong>Biological Normalization</strong>:</p>
<ul class="simple">
<li><p>Contrast normalization in retina and LGN</p></li>
<li><p>Cross-orientation inhibition in V1</p></li>
<li><p>Surround suppression effects</p></li>
<li><p>Divisive normalization across neural populations</p></li>
<li><p>Adaptation to prevailing input statistics</p></li>
</ul>
<p><strong>CNN Normalization</strong>:</p>
<ul class="simple">
<li><p>Batch normalization</p></li>
<li><p>Layer normalization</p></li>
<li><p>Instance normalization</p></li>
<li><p>Group normalization</p></li>
<li><p>Local response normalization (used in AlexNet)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">normalization_example</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate the effect of normalization in neural processing.&quot;&quot;&quot;</span>
    
    <span class="c1"># Create sample activations</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>  <span class="c1"># Skewed distribution</span>
    
    <span class="c1"># Apply divisive normalization (simplified model)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">divisive_normalization</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">neighborhood_size</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
        <span class="n">normalized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">act</span><span class="p">)):</span>
            <span class="c1"># Define local neighborhood</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">neighborhood_size</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">act</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="n">neighborhood_size</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">neighborhood</span> <span class="o">=</span> <span class="n">act</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>
            
            <span class="c1"># Compute normalization factor</span>
            <span class="n">norm_factor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">neighborhood</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
            
            <span class="c1"># Normalize</span>
            <span class="n">normalized</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">act</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">norm_factor</span>
        
        <span class="k">return</span> <span class="n">normalized</span>
    
    <span class="n">normalized_activations</span> <span class="o">=</span> <span class="n">divisive_normalization</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span>
    
    <span class="c1"># Compare statistics</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;original_mean&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">activations</span><span class="p">),</span>
        <span class="s2">&quot;original_std&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">activations</span><span class="p">),</span>
        <span class="s2">&quot;original_max&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">activations</span><span class="p">),</span>
        <span class="s2">&quot;normalized_mean&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">normalized_activations</span><span class="p">),</span>
        <span class="s2">&quot;normalized_std&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">normalized_activations</span><span class="p">),</span>
        <span class="s2">&quot;normalized_max&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">normalized_activations</span><span class="p">)</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">stats</span>
</pre></div>
</div>
<p>Normalization serves several key functions:</p>
<ol class="arabic simple">
<li><p>Stabilizes training by controlling activation distributions</p></li>
<li><p>Enables efficient coding by adapting to input statistics</p></li>
<li><p>Enhances discriminability by emphasizing differences</p></li>
<li><p>Manages the dynamic range of neural responses</p></li>
</ol>
<p>The parallel implementation of normalization in both systems highlights its computational importance.</p>
</section>
<section id="attention-mechanisms">
<h3>4.4.4 Attention Mechanisms<a class="headerlink" href="#attention-mechanisms" title="Link to this heading">#</a></h3>
<p>Attention mechanisms represent another area of convergence:</p>
<p><strong>Visual Attention in the Brain</strong>:</p>
<ul class="simple">
<li><p>Selective enhancement of relevant visual regions</p></li>
<li><p>Spotlight or zoom lens models of spatial attention</p></li>
<li><p>Feature-based attention that enhances specific attributes</p></li>
<li><p>Modulation by frontal and parietal control networks</p></li>
<li><p>Enhanced processing and perceptual quality in attended regions</p></li>
</ul>
<p><strong>Attention in Deep Learning</strong>:</p>
<ul class="simple">
<li><p>Spatial attention mechanisms in CNNs</p></li>
<li><p>Channel attention (e.g., Squeeze-and-Excitation Networks)</p></li>
<li><p>Self-attention in Transformers (now integrated into vision models)</p></li>
<li><p>Cross-attention between modalities</p></li>
<li><p>Hard attention (selecting specific regions) vs. soft attention (weighting)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">attention_mechanisms</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare biological and artificial attention mechanisms.&quot;&quot;&quot;</span>
    
    <span class="n">attention_comparison</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Spatial Attention&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Biology&quot;</span><span class="p">:</span> <span class="s2">&quot;Enhanced processing at attended locations, controlled by FEF/IPS&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AI&quot;</span><span class="p">:</span> <span class="s2">&quot;Spatial attention maps, region proposal networks, visual transformers&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Feature Attention&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Biology&quot;</span><span class="p">:</span> <span class="s2">&quot;Enhanced processing of attended features across visual field&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AI&quot;</span><span class="p">:</span> <span class="s2">&quot;Channel attention mechanisms, feature emphasizing&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Effects&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Biology&quot;</span><span class="p">:</span> <span class="s2">&quot;Increased firing rates, improved discriminability, reduced latency&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AI&quot;</span><span class="p">:</span> <span class="s2">&quot;Weighted feature maps, selected processing pathways&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;Control&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;Biology&quot;</span><span class="p">:</span> <span class="s2">&quot;Top-down control from prefrontal and parietal cortex&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AI&quot;</span><span class="p">:</span> <span class="s2">&quot;Learned attention weights, explicit guidance mechanisms&quot;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">attention_comparison</span>
</pre></div>
</div>
<p>Recent CNN architectures increasingly incorporate attention mechanisms, moving beyond pure feedforward processing toward more brain-like dynamic routing of information.</p>
</section>
</section>
<section id="beyond-feedforward-processing">
<h2>4.5 Beyond Feedforward Processing<a class="headerlink" href="#beyond-feedforward-processing" title="Link to this heading">#</a></h2>
<p>While early CNNs focused on feedforward processing, both biological vision and cutting-edge AI incorporate more complex processing dynamics.</p>
<section id="recurrence-in-visual-processing">
<h3>4.5.1 Recurrence in Visual Processing<a class="headerlink" href="#recurrence-in-visual-processing" title="Link to this heading">#</a></h3>
<p>The visual cortex contains extensive recurrent connections:</p>
<p><strong>Biological Recurrence</strong>:</p>
<ul class="simple">
<li><p>Local recurrent circuits within cortical areas</p></li>
<li><p>Feedback connections from higher to lower areas</p></li>
<li><p>Horizontal connections linking neurons within layers</p></li>
<li><p>Temporal dynamics and sustained activity patterns</p></li>
</ul>
<p><strong>AI Recurrence</strong>:</p>
<ul class="simple">
<li><p>Recurrent convolutional networks</p></li>
<li><p>Convolutional LSTMs</p></li>
<li><p>CORnet and similar biologically-inspired models</p></li>
<li><p>Transformer-based architectures with self-attention</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">RecurrentConvLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simple implementation of a recurrent convolutional layer.&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RecurrentConvLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
        
        <span class="c1"># Forward connection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_forward</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        
        <span class="c1"># Recurrent connection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_recurrent</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">output_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
        
        <span class="c1"># Activation function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">prev_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Forward pass</span>
        <span class="n">h_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Add recurrent connection if previous state exists</span>
        <span class="k">if</span> <span class="n">prev_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">h_new</span> <span class="o">=</span> <span class="n">h_new</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_recurrent</span><span class="p">(</span><span class="n">prev_state</span><span class="p">)</span>
        
        <span class="c1"># Apply non-linearity</span>
        <span class="n">h_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h_new</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">h_new</span>
</pre></div>
</div>
<p>Recurrent processing enables:</p>
<ol class="arabic simple">
<li><p>Integration of information over time</p></li>
<li><p>Refinement of initial representations</p></li>
<li><p>Context-sensitive processing</p></li>
<li><p>Figure-ground segmentation</p></li>
<li><p>Perceptual completion and illusion</p></li>
</ol>
<p>Incorporating recurrence brings AI systems closer to biological vision and can improve performance on challenging tasks.</p>
</section>
<section id="predictive-coding-and-generative-models">
<h3>4.5.2 Predictive Coding and Generative Models<a class="headerlink" href="#predictive-coding-and-generative-models" title="Link to this heading">#</a></h3>
<p>The brain appears to implement predictive processing, which has inspired generative models in AI:</p>
<p><strong>Predictive Coding in the Brain</strong>:</p>
<ul class="simple">
<li><p>Higher levels predict lower-level activity</p></li>
<li><p>Prediction errors drive learning and updating</p></li>
<li><p>Balances bottom-up evidence with top-down predictions</p></li>
<li><p>Explains effects like end-stopping and non-classical receptive fields</p></li>
</ul>
<p><strong>Corresponding AI Approaches</strong>:</p>
<ul class="simple">
<li><p>Variational autoencoders (VAEs)</p></li>
<li><p>Generative adversarial networks (GANs)</p></li>
<li><p>Diffusion models</p></li>
<li><p>Hierarchical predictive coding models</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">predictive_coding_model</span><span class="p">(</span><span class="n">input_image</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simplified implementation of predictive coding update.&quot;&quot;&quot;</span>
    
    <span class="c1"># Compute prediction error</span>
    <span class="n">prediction_error</span> <span class="o">=</span> <span class="n">input_image</span> <span class="o">-</span> <span class="n">prediction</span>
    
    <span class="c1"># Update predictions to minimize error</span>
    <span class="n">new_prediction</span> <span class="o">=</span> <span class="n">prediction</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">prediction_error</span>
    
    <span class="c1"># Return updated prediction and error</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;prediction&quot;</span><span class="p">:</span> <span class="n">new_prediction</span><span class="p">,</span>
        <span class="s2">&quot;prediction_error&quot;</span><span class="p">:</span> <span class="n">prediction_error</span><span class="p">,</span>
        <span class="s2">&quot;error_magnitude&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">prediction_error</span><span class="p">))</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Predictive coding offers a unified framework for understanding perception, learning, and attention in both biological and artificial systems.</p>
</section>
<section id="integration-of-context-and-prior-knowledge">
<h3>4.5.3 Integration of Context and Prior Knowledge<a class="headerlink" href="#integration-of-context-and-prior-knowledge" title="Link to this heading">#</a></h3>
<p>Both biological and advanced artificial vision systems integrate contextual information:</p>
<p><strong>Contextual Effects in Biological Vision</strong>:</p>
<ul class="simple">
<li><p>Scene context influences object recognition</p></li>
<li><p>Prior knowledge shapes perception of ambiguous stimuli</p></li>
<li><p>Contour integration and grouping phenomena</p></li>
<li><p>Long-range connections create context-dependent responses</p></li>
</ul>
<p><strong>AI Approaches to Context</strong>:</p>
<ul class="simple">
<li><p>Scene graph models</p></li>
<li><p>Contextual attention mechanisms</p></li>
<li><p>Memory networks that maintain context</p></li>
<li><p>Multimodal systems integrating vision with language</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">contextual_influence_demo</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate how context influences perception.&quot;&quot;&quot;</span>
    
    <span class="c1"># Example of ambiguous stimuli with different contexts</span>
    <span class="n">contexts</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;context_A&quot;</span><span class="p">:</span> <span class="s2">&quot;The chef carefully placed the ___ on the dining table.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;context_B&quot;</span><span class="p">:</span> <span class="s2">&quot;The courier delivered the ___ to the customer.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ambiguous_stimulus&quot;</span><span class="p">:</span> <span class="s2">&quot;plate&quot;</span>  <span class="c1"># Could be dinner plate or tectonic plate</span>
    <span class="p">}</span>
    
    <span class="c1"># Simulated perception based on context (would be model outputs in practice)</span>
    <span class="n">perceptions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;context_A&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;interpretation&quot;</span><span class="p">:</span> <span class="s2">&quot;dinner plate&quot;</span><span class="p">,</span>
            <span class="s2">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
            <span class="s2">&quot;associated_features&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;round&quot;</span><span class="p">,</span> <span class="s2">&quot;ceramic&quot;</span><span class="p">,</span> <span class="s2">&quot;food-related&quot;</span><span class="p">]</span>
        <span class="p">},</span>
        <span class="s2">&quot;context_B&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;interpretation&quot;</span><span class="p">:</span> <span class="s2">&quot;shipment package&quot;</span><span class="p">,</span>
            <span class="s2">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.82</span><span class="p">,</span>
            <span class="s2">&quot;associated_features&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cardboard&quot;</span><span class="p">,</span> <span class="s2">&quot;shipping label&quot;</span><span class="p">,</span> <span class="s2">&quot;delivery&quot;</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;contexts&quot;</span><span class="p">:</span> <span class="n">contexts</span><span class="p">,</span> <span class="s2">&quot;perceptions&quot;</span><span class="p">:</span> <span class="n">perceptions</span><span class="p">}</span>
</pre></div>
</div>
<p>Integrating context and prior knowledge brings artificial systems closer to human-like perception with its robust, context-sensitive interpretation.</p>
</section>
<section id="the-free-energy-principle">
<h3>4.5.4 The Free Energy Principle<a class="headerlink" href="#the-free-energy-principle" title="Link to this heading">#</a></h3>
<p>The free energy principle provides a unifying theoretical framework that may explain many aspects of brain function:</p>
<p><strong>Key Concepts</strong>:</p>
<ul class="simple">
<li><p>The brain minimizes prediction error (free energy)</p></li>
<li><p>Perception is inference about the causes of sensory data</p></li>
<li><p>Learning improves the generative model to better predict inputs</p></li>
<li><p>Attention allocates resources to minimize expected free energy</p></li>
</ul>
<p><strong>AI Implementations</strong>:</p>
<ul class="simple">
<li><p>Variational inference methods</p></li>
<li><p>Active inference models</p></li>
<li><p>Energy-based models</p></li>
<li><p>Self-supervised predictive learning approaches</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">free_energy_calculation</span><span class="p">(</span><span class="n">sensory_data</span><span class="p">,</span> <span class="n">predicted_data</span><span class="p">,</span> <span class="n">model_complexity</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simplified calculation of free energy.&quot;&quot;&quot;</span>
    
    <span class="c1"># Prediction error component (accuracy)</span>
    <span class="n">prediction_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">sensory_data</span> <span class="o">-</span> <span class="n">predicted_data</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># Complexity penalty (regularization)</span>
    <span class="n">complexity_cost</span> <span class="o">=</span> <span class="n">model_complexity</span>
    
    <span class="c1"># Free energy combines both terms</span>
    <span class="n">free_energy</span> <span class="o">=</span> <span class="n">prediction_error</span> <span class="o">+</span> <span class="n">complexity_cost</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;free_energy&quot;</span><span class="p">:</span> <span class="n">free_energy</span><span class="p">,</span>
        <span class="s2">&quot;prediction_error&quot;</span><span class="p">:</span> <span class="n">prediction_error</span><span class="p">,</span>
        <span class="s2">&quot;complexity_cost&quot;</span><span class="p">:</span> <span class="n">complexity_cost</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>The free energy principle provides a theoretical bridge between biological and artificial intelligence, suggesting fundamental computational principles that may underlie both.</p>
</section>
</section>
<section id="code-lab">
<h2>4.6 Code Lab<a class="headerlink" href="#code-lab" title="Link to this heading">#</a></h2>
<p>Let’s explore practical implementations of visual processing concepts from both biological and artificial systems.</p>
<section id="implementing-simple-gabor-filters">
<h3>4.6.1 Implementing Simple Gabor Filters<a class="headerlink" href="#implementing-simple-gabor-filters" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">ndimage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">skimage</span><span class="w"> </span><span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">color</span>

<span class="k">def</span><span class="w"> </span><span class="nf">create_gabor_filter</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">psi</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a Gabor filter kernel.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        size: Size of the kernel</span>
<span class="sd">        sigma: Standard deviation of the Gaussian envelope</span>
<span class="sd">        theta: Orientation (in radians)</span>
<span class="sd">        lambda_: Wavelength of sinusoidal component</span>
<span class="sd">        gamma: Spatial aspect ratio</span>
<span class="sd">        psi: Phase offset</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Gabor kernel</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Generate grid</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="c1"># Rotation</span>
    <span class="n">x_theta</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">y_theta</span> <span class="o">=</span> <span class="o">-</span><span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    
    <span class="c1"># Gaussian envelope</span>
    <span class="n">gb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_theta</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">y_theta</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    
    <span class="c1"># Sinusoidal carrier</span>
    <span class="n">gb</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_theta</span> <span class="o">/</span> <span class="n">lambda_</span> <span class="o">+</span> <span class="n">psi</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">gb</span>

<span class="k">def</span><span class="w"> </span><span class="nf">apply_gabor_bank</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">num_orientations</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply a bank of Gabor filters to an image.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Convert to grayscale if color</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">color</span><span class="o">.</span><span class="n">rgb2gray</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    
    <span class="c1"># Create filters at different orientations</span>
    <span class="n">orientations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">num_orientations</span><span class="p">)</span>
    <span class="n">responses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">filters</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">orientations</span><span class="p">:</span>
        <span class="c1"># Create filter</span>
        <span class="n">gabor_kernel</span> <span class="o">=</span> <span class="n">create_gabor_filter</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">filters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gabor_kernel</span><span class="p">)</span>
        
        <span class="c1"># Apply filter</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">ndimage</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">gabor_kernel</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
        <span class="n">responses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">responses</span><span class="p">,</span> <span class="n">filters</span>

<span class="c1"># Example usage</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gabor_demo</span><span class="p">():</span>
    <span class="c1"># Load sample image</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">camera</span><span class="p">()</span>
    
    <span class="c1"># Apply Gabor filters</span>
    <span class="n">responses</span><span class="p">,</span> <span class="n">filters</span> <span class="o">=</span> <span class="n">apply_gabor_bank</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    
    <span class="c1"># Visualization code (would generate plots in real notebook)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;image_shape&quot;</span><span class="p">:</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="s2">&quot;num_filters&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">filters</span><span class="p">),</span>
        <span class="s2">&quot;filter_shape&quot;</span><span class="p">:</span> <span class="n">filters</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="s2">&quot;response_shape&quot;</span><span class="p">:</span> <span class="n">responses</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
    <span class="p">}</span>

<span class="c1"># Run the demo</span>
<span class="n">gabor_result</span> <span class="o">=</span> <span class="n">gabor_demo</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Applied </span><span class="si">{</span><span class="n">gabor_result</span><span class="p">[</span><span class="s1">&#39;num_filters&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> Gabor filters to image of shape </span><span class="si">{</span><span class="n">gabor_result</span><span class="p">[</span><span class="s1">&#39;image_shape&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="building-a-basic-convolutional-layer">
<h3>4.6.2 Building a Basic Convolutional Layer<a class="headerlink" href="#building-a-basic-convolutional-layer" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">transforms</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SimpleV1Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simple CNN model inspired by V1 processing.&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_orientations</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_scales</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleV1Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">num_orientations</span> <span class="o">=</span> <span class="n">num_orientations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_scales</span> <span class="o">=</span> <span class="n">num_scales</span>
        
        <span class="c1"># Simple cells (orientation-selective filters)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">simple_cells</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_orientations</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="o">+</span><span class="n">i</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_scales</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="c1"># Complex cells (pooling for position invariance)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">complex_cells</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_scales</span><span class="p">)</span>
        <span class="p">])</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Apply simple and complex cells at each scale</span>
        <span class="n">responses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">simple</span><span class="p">,</span> <span class="n">complex_pool</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">simple_cells</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">complex_cells</span><span class="p">):</span>
            <span class="c1"># Simple cell response</span>
            <span class="n">simple_response</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">simple</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            
            <span class="c1"># Complex cell response (pooling)</span>
            <span class="n">complex_response</span> <span class="o">=</span> <span class="n">complex_pool</span><span class="p">(</span><span class="n">simple_response</span><span class="p">)</span>
            
            <span class="c1"># Flatten and add to responses</span>
            <span class="n">responses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">complex_response</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">complex_response</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># Concatenate all responses</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">combined</span>

<span class="c1"># Example usage</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_v1_model</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train a V1-inspired model on MNIST.&quot;&quot;&quot;</span>
    <span class="c1"># Set device</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    
    <span class="c1"># Load MNIST dataset</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,))])</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Create model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleV1Model</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># For demonstration only - return model architecture</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;model_type&quot;</span><span class="p">:</span> <span class="s2">&quot;V1-inspired CNN&quot;</span><span class="p">,</span>
        <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span>
        <span class="s2">&quot;architecture&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="p">}</span>

<span class="c1"># Show model architecture</span>
<span class="n">v1_model_info</span> <span class="o">=</span> <span class="n">train_v1_model</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model architecture: </span><span class="si">{</span><span class="n">v1_model_info</span><span class="p">[</span><span class="s1">&#39;architecture&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="feature-visualization-techniques">
<h3>4.6.3 Feature Visualization Techniques<a class="headerlink" href="#feature-visualization-techniques" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">feature_vis_demo</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">feature_map_idx</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement activation maximization to visualize what activates a specific feature.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        model: Pretrained CNN</span>
<span class="sd">        layer_idx: Index of layer to visualize</span>
<span class="sd">        feature_map_idx: Index of feature map to visualize</span>
<span class="sd">        iterations: Number of optimization iterations</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Synthesized image that maximally activates the feature</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Note: This is a simplified outline of the technique</span>
    
    <span class="c1"># Create a random image</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>
    
    <span class="c1"># Convert to tensor</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Optimization loop</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="c1"># Forward pass</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Here we would:</span>
        <span class="c1"># 1. Forward pass through the network up to target layer</span>
        <span class="c1"># 2. Extract activation of target feature map</span>
        <span class="c1"># 3. Compute loss (negative activation to maximize)</span>
        <span class="c1"># 4. Backward pass to get gradients on input</span>
        <span class="c1"># 5. Update input image with gradients</span>
        <span class="c1"># 6. Apply constraints (e.g., regularization)</span>
        
        <span class="c1"># For demonstration, return placeholder</span>
        <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: [simulated activation]&quot;</span><span class="p">)</span>
    
    <span class="c1"># For this code template, return a description</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;technique&quot;</span><span class="p">:</span> <span class="s2">&quot;Activation Maximization&quot;</span><span class="p">,</span>
        <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="n">layer_idx</span><span class="si">}</span><span class="s2">, Feature Map </span><span class="si">{</span><span class="n">feature_map_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="s2">&quot;iterations&quot;</span><span class="p">:</span> <span class="n">iterations</span><span class="p">,</span>
        <span class="s2">&quot;result&quot;</span><span class="p">:</span> <span class="s2">&quot;In a real implementation, returns the synthesized image&quot;</span>
    <span class="p">}</span>
</pre></div>
</div>
</section>
<section id="transfer-learning-with-pre-trained-cnns">
<h3>4.6.4 Transfer Learning with Pre-trained CNNs<a class="headerlink" href="#transfer-learning-with-pre-trained-cnns" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">models</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adam</span>

<span class="k">def</span><span class="w"> </span><span class="nf">transfer_learning_example</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate transfer learning with a pre-trained CNN.&quot;&quot;&quot;</span>
    
    <span class="c1"># Load pre-trained model</span>
    <span class="n">resnet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Freeze early layers</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">resnet</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    
    <span class="c1"># Replace final fully connected layer</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="n">resnet</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># e.g., for a custom dataset</span>
    <span class="n">resnet</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    
    <span class="c1"># Create optimizer for only the new layer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">resnet</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    
    <span class="c1"># In a real implementation, we would:</span>
    <span class="c1"># 1. Load custom dataset</span>
    <span class="c1"># 2. Create data loaders</span>
    <span class="c1"># 3. Train the modified model</span>
    <span class="c1"># 4. Evaluate performance</span>
    
    <span class="c1"># For this template, return the model architecture</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;ResNet-18 with transfer learning&quot;</span><span class="p">,</span>
        <span class="s2">&quot;frozen_layers&quot;</span><span class="p">:</span> <span class="s2">&quot;All except final FC layer&quot;</span><span class="p">,</span>
        <span class="s2">&quot;trainable_parameters&quot;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">resnet</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span>
        <span class="s2">&quot;total_parameters&quot;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">resnet</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="p">}</span>

<span class="c1"># Get transfer learning details</span>
<span class="n">transfer_details</span> <span class="o">=</span> <span class="n">transfer_learning_example</span><span class="p">()</span>
<span class="n">efficiency</span> <span class="o">=</span> <span class="p">(</span><span class="n">transfer_details</span><span class="p">[</span><span class="s2">&quot;trainable_parameters&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">transfer_details</span><span class="p">[</span><span class="s2">&quot;total_parameters&quot;</span><span class="p">])</span> <span class="o">*</span> <span class="mi">100</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transfer learning trains </span><span class="si">{</span><span class="n">efficiency</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% of total parameters&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="take-aways">
<h2>4.7 Take-aways<a class="headerlink" href="#take-aways" title="Link to this heading">#</a></h2>
<p>The relationship between biological vision and CNNs demonstrates the power of neuroscience-inspired AI:</p>
<ol class="arabic simple">
<li><p><strong>Hierarchical Feature Extraction is Universal</strong>: Both biological vision and CNNs build complex representations from simple features through hierarchical processing.</p></li>
<li><p><strong>Local to Global Processing Works</strong>: Processing visual information with local receptive fields that grow in size and complexity is a powerful and efficient approach.</p></li>
<li><p><strong>Feature Detection Plus Invariance</strong>: The separation of feature detection (simple cells/convolutional layers) and invariance operations (complex cells/pooling layers) is fundamental to both systems.</p></li>
<li><p><strong>Beyond Feedforward</strong>: While CNNs began as purely feedforward, both biological vision and cutting-edge AI increasingly incorporate recurrence, feedback, and context.</p></li>
<li><p><strong>Convergent Evolution</strong>: The strong similarities between features learned by CNNs and those observed in the visual cortex suggest fundamental principles of efficient visual processing.</p></li>
<li><p><strong>Bidirectional Insights</strong>: Neuroscience continues to inspire AI innovations, while AI implementations help test and refine theories about biological vision.</p></li>
</ol>
<p>This bidirectional flow of insights exemplifies the value of interdisciplinary research between neuroscience and artificial intelligence.</p>
</section>
<section id="further-reading-media">
<h2>4.8 Further Reading &amp; Media<a class="headerlink" href="#further-reading-media" title="Link to this heading">#</a></h2>
<section id="foundational-papers">
<h3>Foundational Papers<a class="headerlink" href="#foundational-papers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Hubel, D. H., &amp; Wiesel, T. N. (1962). Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex. <em>The Journal of physiology, 160</em>(1), 106-154.</p></li>
<li><p>Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. <em>Biological cybernetics, 36</em>(4), 193-202.</p></li>
<li><p>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. <em>Proceedings of the IEEE, 86</em>(11), 2278-2324.</p></li>
<li><p>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. <em>Advances in neural information processing systems, 25</em>.</p></li>
</ul>
</section>
<section id="biological-vision">
<h3>Biological Vision<a class="headerlink" href="#biological-vision" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Felleman, D. J., &amp; Van Essen, D. C. (1991). Distributed hierarchical processing in the primate cerebral cortex. <em>Cerebral cortex, 1</em>(1), 1-47.</p></li>
<li><p>Carandini, M., Demb, J. B., Mante, V., Tolhurst, D. J., Dan, Y., Olshausen, B. A., … &amp; Rust, N. C. (2005). Do we know what the early visual system does?. <em>Journal of Neuroscience, 25</em>(46), 10577-10597.</p></li>
<li><p>DiCarlo, J. J., Zoccolan, D., &amp; Rust, N. C. (2012). How does the brain solve visual object recognition?. <em>Neuron, 73</em>(3), 415-434.</p></li>
</ul>
</section>
<section id="cnn-visualization-analysis">
<h3>CNN Visualization &amp; Analysis<a class="headerlink" href="#cnn-visualization-analysis" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Zeiler, M. D., &amp; Fergus, R. (2014). Visualizing and understanding convolutional networks. <em>European conference on computer vision</em> (pp. 818-833).</p></li>
<li><p>Yamins, D. L., Hong, H., Cadieu, C. F., Solomon, E. A., Seibert, D., &amp; DiCarlo, J. J. (2014). Performance-optimized hierarchical models predict neural responses in higher visual cortex. <em>Proceedings of the National Academy of Sciences, 111</em>(23), 8619-8624.</p></li>
<li><p>Olah, C., Mordvintsev, A., &amp; Schubert, L. (2017). Feature visualization. <em>Distill, 2</em>(11), e7.</p></li>
</ul>
</section>
<section id="advanced-topics">
<h3>Advanced Topics<a class="headerlink" href="#advanced-topics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Kriegeskorte, N. (2015). Deep neural networks: a new framework for modeling biological vision and brain information processing. <em>Annual review of vision science, 1</em>, 417-446.</p></li>
<li><p>Kietzmann, T. C., McClure, P., &amp; Kriegeskorte, N. (2019). Deep neural networks in computational neuroscience. <em>Oxford Research Encyclopedia of Neuroscience</em>.</p></li>
<li><p>Kubilius, J., Schrimpf, M., Nayebi, A., Bear, D., Yamins, D. L., &amp; DiCarlo, J. J. (2018). CORnet: Modeling the neural mechanisms of core object recognition. <em>BioRxiv</em>, 408385.</p></li>
</ul>
</section>
<section id="books">
<h3>Books<a class="headerlink" href="#books" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Wandell, B. A. (1995). <em>Foundations of vision</em>. Sinauer Associates.</p></li>
<li><p>Palmer, S. E. (1999). <em>Vision science: Photons to phenomenology</em>. MIT press.</p></li>
<li><p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep learning</em>. MIT press.</p></li>
<li><p>DiCarlo, Zoccolan &amp; Rust (2012) - “How Does the Brain Solve Visual Object Recognition?”</p></li>
<li><p>Kriegeskorte (2015) - “Deep Neural Networks: A New Framework for Modeling Biological Vision”</p></li>
<li><p>Yamins &amp; DiCarlo (2016) - “Using goal-driven deep learning models to understand sensory cortex”</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="in-depth-content">
<h2>In-Depth Content<a class="headerlink" href="#in-depth-content" title="Link to this heading">#</a></h2>
<p><img alt="Visual Cortex vs CNN" src="../_images/cortical_column_vs_ann.svg" /></p>
<section id="the-human-visual-system">
<h3>The Human Visual System<a class="headerlink" href="#the-human-visual-system" title="Link to this heading">#</a></h3>
<p>The visual processing pipeline begins with the retina, where photoreceptors convert light into electrical signals. These signals pass through retinal ganglion cells, which perform the first stage of processing - edge detection and contrast enhancement through center-surround receptive fields. The signals then travel via the optic nerve to the lateral geniculate nucleus (LGN) of the thalamus, which acts as a relay station, before reaching the primary visual cortex (V1) in the occipital lobe.</p>
<p>In V1, neurons have small receptive fields that respond to oriented edges in specific parts of the visual field. As information flows to higher areas (V2, V4, IT cortex), neurons have progressively larger receptive fields and respond to more complex patterns - from corners and simple shapes to specific objects or faces. This hierarchical organization forms the basis for the layered architecture in artificial neural networks designed for visual processing.</p>
</section>
<section id="receptive-fields-and-hierarchies">
<h3>Receptive Fields and Hierarchies<a class="headerlink" href="#receptive-fields-and-hierarchies" title="Link to this heading">#</a></h3>
<p>A key discovery by Hubel and Wiesel in the 1960s was that neurons in the visual cortex act as feature detectors. Simple cells in V1 respond to oriented edges at specific locations, complex cells respond to oriented edges regardless of exact position, and hypercomplex cells (end-stopped cells) respond to edges of specific lengths or corners.</p>
<p>The receptive field of a visual neuron is the region of visual space that, when stimulated, affects the firing of that neuron. As visual information progresses through the cortical hierarchy, receptive fields become larger and encode more complex features:</p>
<ol class="arabic simple">
<li><p>V1 neurons: Oriented edges, spatial frequency, color</p></li>
<li><p>V2 neurons: Contours, figure-ground separation</p></li>
<li><p>V4 neurons: Shape features, curvature</p></li>
<li><p>IT neurons: Complex objects, faces</p></li>
</ol>
<p>This progressive abstraction of visual features - from simple to complex - is the key insight that informed the development of convolutional neural networks.</p>
</section>
<section id="convolutional-neural-networks">
<h3>Convolutional Neural Networks<a class="headerlink" href="#convolutional-neural-networks" title="Link to this heading">#</a></h3>
<p>CNNs mirror the architecture of the visual cortex in several key ways:</p>
<ol class="arabic simple">
<li><p><strong>Convolutional layers</strong>: Like visual cortex neurons, CNN filters scan across the image and respond to specific patterns within their receptive field. Early layers detect simple features (edges, textures) while deeper layers combine these to detect complex objects.</p></li>
<li><p><strong>Local connectivity</strong>: Rather than connecting to every input pixel (as in fully-connected networks), CNN neurons connect only to a small region, similar to the receptive fields in the visual system.</p></li>
<li><p><strong>Hierarchical processing</strong>: CNNs stack multiple layers, progressively building more complex representations - from edges to textures to object parts to whole objects.</p></li>
<li><p><strong>Pooling operations</strong>: These provide translation invariance by summarizing features across small spatial regions, similar to how complex cells in V1 respond to oriented edges regardless of their exact position.</p></li>
</ol>
<p>Individual CNN neurons, like cortical neurons, respond to inputs in a restricted region (their receptive field) and collectively form a retinotopic map of visual space.</p>
</section>
<section id="examples-of-biological-inspiration">
<h3>Examples of Biological Inspiration<a class="headerlink" href="#examples-of-biological-inspiration" title="Link to this heading">#</a></h3>
<p>Hubel and Wiesel’s discovery of edge-detecting cells in the cat visual cortex foreshadowed the edge-detecting filters learned by the first layer of CNNs. When visualized, the filters learned by the first convolutional layer of networks like AlexNet show striking similarity to the Gabor-like filters found in V1 neurons.</p>
</section>
<section id="python-pytorch-code-examples">
<h3>Python/PyTorch Code Examples<a class="headerlink" href="#python-pytorch-code-examples" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">models</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># Implementing Gabor filters - similar to V1 simple cell receptive fields</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gabor_filter</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">lambda_val</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">psi</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a Gabor filter (similar to V1 simple cell receptive fields)</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    size (int): Size of the filter</span>
<span class="sd">    lambda_val (float): Wavelength</span>
<span class="sd">    theta (float): Orientation in radians</span>
<span class="sd">    sigma (float): Standard deviation of the Gaussian envelope</span>
<span class="sd">    gamma (float): Spatial aspect ratio</span>
<span class="sd">    psi (float): Phase offset</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    np.array: Gabor filter kernel</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="c1"># Rotation</span>
    <span class="n">x_theta</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">y_theta</span> <span class="o">=</span> <span class="o">-</span><span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    
    <span class="c1"># Gabor function</span>
    <span class="n">gb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_theta</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">y_theta</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_theta</span> <span class="o">/</span> <span class="n">lambda_val</span> <span class="o">+</span> <span class="n">psi</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">gb</span>

<span class="c1"># Create and visualize a bank of Gabor filters with different orientations</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize_gabor_bank</span><span class="p">():</span>
    <span class="c1"># Parameters</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">31</span>
    <span class="n">lambda_val</span> <span class="o">=</span> <span class="mf">10.0</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="mf">5.0</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">psi</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="c1"># Create filters at different orientations</span>
    <span class="n">orientations</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">filters</span> <span class="o">=</span> <span class="p">[</span><span class="n">gabor_filter</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">lambda_val</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">psi</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">orientations</span><span class="p">]</span>
    
    <span class="c1"># Visualize</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">filt</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">orientations</span><span class="p">)):</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">filt</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Orientation: </span><span class="si">{</span><span class="n">theta</span><span class="o">*</span><span class="mi">180</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">°&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">filters</span>

<span class="c1"># A simple CNN to demonstrate visual processing hierarchy</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SimpleCNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleCNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># First conv layer - like V1 simple cells (edge detectors)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># Second conv layer - like V2 (combining edges into shapes)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># Third conv layer - like V4 (more complex shapes)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Final layer - like IT (object recognition)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># For MNIST digits (10 classes)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Hierarchical feature extraction</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># Conv + pooling (like complex cells)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># Greater invariance</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># Higher-level features</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Flatten</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Classification</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Function to visualize the activations at different layers - like recording from different visual areas</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize_layer_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">image_path</span><span class="p">):</span>
    <span class="c1"># Load and preprocess image</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">&#39;L&#39;</span><span class="p">)</span>  <span class="c1"># Convert to grayscale</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="p">])</span>
    <span class="n">img_tensor</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Add batch dimension</span>
    
    <span class="c1"># Hook to capture activations</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">hook_fn</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
            <span class="n">activations</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">hook</span>
    
    <span class="c1"># Register hooks</span>
    <span class="n">hooks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">hook_fn</span><span class="p">(</span><span class="s1">&#39;conv1&#39;</span><span class="p">)))</span>
    <span class="n">hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">hook_fn</span><span class="p">(</span><span class="s1">&#39;conv2&#39;</span><span class="p">)))</span>
    <span class="n">hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv3</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">hook_fn</span><span class="p">(</span><span class="s1">&#39;conv3&#39;</span><span class="p">)))</span>
    
    <span class="c1"># Forward pass</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>
    
    <span class="c1"># Remove hooks</span>
    <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="n">hooks</span><span class="p">:</span>
        <span class="n">hook</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
    
    <span class="c1"># Visualize activations from different layers</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    
    <span class="c1"># First layer (V1-like)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">activations</span><span class="p">[</span><span class="s1">&#39;conv1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="s1">&#39;conv1&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;V1-like Filter </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Second layer (V2-like)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">activations</span><span class="p">[</span><span class="s1">&#39;conv2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="s1">&#39;conv2&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;V2-like Filter </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># Third layer (V4-like)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">activations</span><span class="p">[</span><span class="s1">&#39;conv3&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="s1">&#39;conv3&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;V4-like Filter </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Using a pre-trained model to visualize hierarchical features</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize_pretrained_features</span><span class="p">():</span>
    <span class="c1"># Load pre-trained VGG16 model</span>
    <span class="n">vgg</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vgg16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Print the model architecture to see the hierarchical organization</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">vgg</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    
    <span class="c1"># Plot filters from the first convolutional layer (edge detectors like V1)</span>
    <span class="n">filters</span> <span class="o">=</span> <span class="n">vgg</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="c1"># Normalize filter values to 0-1 range for display</span>
    <span class="n">filters</span> <span class="o">=</span> <span class="p">(</span><span class="n">filters</span> <span class="o">-</span> <span class="n">filters</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">filters</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">filters</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
    
    <span class="c1"># Plot first 16 filters</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">):</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">filters</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Filter </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Example usage:</span>
<span class="c1"># gabor_bank = visualize_gabor_bank()</span>
<span class="c1"># model = SimpleCNN()</span>
<span class="c1"># visualize_layer_activations(model, &#39;path_to_image.jpg&#39;)</span>
<span class="c1"># visualize_pretrained_features()</span>
</pre></div>
</div>
</section>
<section id="suggested-readings">
<h3>Suggested Readings<a class="headerlink" href="#suggested-readings" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Hubel &amp; Wiesel (1962), “Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex,” Journal of Physiology – Classic paper detailing edge-detecting neurons (historical context for CNN inspiration).</p></li>
<li><p>Fukushima (1980), “Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,” Biol. Cybernetics – Early CNN-like model influenced by neuroscience.</p></li>
<li><p>Yamins &amp; DiCarlo (2016), “Using goal-driven deep learning models to understand sensory cortex,” Nature Neuroscience – Demonstrates that deep CNNs not only draw inspiration from the brain but can predict neural responses in the primate visual system, validating CNNs as models of vision.</p></li>
</ul>
</section>
<section id="supplementary-videos-lectures">
<h3>Supplementary Videos/Lectures<a class="headerlink" href="#supplementary-videos-lectures" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>TED-Ed: “How Your Eyes Make Sense of the World” – Visual explanation of the human visual processing pathway.</p></li>
<li><p>Stanford CS231n (Convolutional Neural Networks for Visual Recognition) – Lecture 1 – Introduction that connects biological vision to CNN design (includes historical notes on Hubel &amp; Wiesel).</p></li>
<li><p>“Do convolutional neural networks mimic the human visual system?” (MIT Quest for Intelligence seminar) – Discusses the parallels and differences between CNNs and actual brain networks for vision.</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "executablebooks/jupyter-book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./part1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ch03_spatial_navigation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 3: Spatial Navigation – Place &amp; Grid Cells</p>
      </div>
    </a>
    <a class="right-next"
       href="../part2/ch05_brain_networks.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 5: Default-Mode vs Executive Control Networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-goals">4.0 Chapter Goals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-system-architecture">4.1 Visual System Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-retina-to-lgn-to-primary-visual-cortex">4.1.1 From Retina to LGN to Primary Visual Cortex</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ventral-vs-dorsal-streams-what-vs-where">4.1.2 Ventral vs Dorsal Streams: “What” vs “Where”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical-organization-of-visual-cortex">4.1.3 Hierarchical Organization of Visual Cortex</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feedback-connections-and-top-down-processing">4.1.4 Feedback Connections and Top-Down Processing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#receptive-fields">4.2 Receptive Fields</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-complex-and-hypercomplex-cells">4.2.1 Simple, Complex, and Hypercomplex Cells</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gabor-filters-and-orientation-selectivity">4.2.2 Gabor Filters and Orientation Selectivity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#center-surround-organization">4.2.3 Center-Surround Organization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-hierarchy-from-v1-to-it">4.2.4 Feature Hierarchy from V1 to IT</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn-parallels">4.3 CNN Parallels</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-layers-as-receptive-fields">4.3.1 Convolutional Layers as Receptive Fields</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-operations-and-invariance">4.3.2 Pooling Operations and Invariance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-hierarchies-in-deep-networks">4.3.3 Feature Hierarchies in Deep Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-of-cnn-features">4.3.4 Visualization of CNN Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-biology-to-deep-learning">4.4 From Biology to Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hubel-wiesel-s-discoveries-neocognitron-lenet-alexnet">4.4.1 Hubel &amp; Wiesel’s Discoveries → Neocognitron → LeNet → AlexNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#biological-constraints-vs-engineering-solutions">4.4.2 Biological Constraints vs Engineering Solutions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-and-gain-control-mechanisms">4.4.3 Normalization and Gain Control Mechanisms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanisms">4.4.4 Attention Mechanisms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-feedforward-processing">4.5 Beyond Feedforward Processing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrence-in-visual-processing">4.5.1 Recurrence in Visual Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-coding-and-generative-models">4.5.2 Predictive Coding and Generative Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-of-context-and-prior-knowledge">4.5.3 Integration of Context and Prior Knowledge</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-free-energy-principle">4.5.4 The Free Energy Principle</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-lab">4.6 Code Lab</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-simple-gabor-filters">4.6.1 Implementing Simple Gabor Filters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-basic-convolutional-layer">4.6.2 Building a Basic Convolutional Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-visualization-techniques">4.6.3 Feature Visualization Techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-with-pre-trained-cnns">4.6.4 Transfer Learning with Pre-trained CNNs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#take-aways">4.7 Take-aways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading-media">4.8 Further Reading &amp; Media</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#foundational-papers">Foundational Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#biological-vision">Biological Vision</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn-visualization-analysis">CNN Visualization &amp; Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-topics">Advanced Topics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#books">Books</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#in-depth-content">In-Depth Content</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-human-visual-system">The Human Visual System</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#receptive-fields-and-hierarchies">Receptive Fields and Hierarchies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks">Convolutional Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-biological-inspiration">Examples of Biological Inspiration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-pytorch-code-examples">Python/PyTorch Code Examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#suggested-readings">Suggested Readings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supplementary-videos-lectures">Supplementary Videos/Lectures</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Richard Young
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>