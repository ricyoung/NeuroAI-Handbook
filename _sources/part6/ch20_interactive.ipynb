{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Examples for Case Studies in NeuroAI\n",
    "\n",
    "This notebook provides interactive examples to complement Chapter 20: Case Studies in NeuroAI. You can run these examples directly in the book or launch a Binder session to experiment with the code.\n",
    "\n",
    "## Interactive Example 1: PredNet Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q matplotlib numpy ipywidgets plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Simplified PredNet simulation function\n",
    "def simulate_prednet(input_noise=0.1, prediction_strength=0.7, learning_rate=0.1, timesteps=100):\n",
    "    # Initialize arrays\n",
    "    inputs = np.zeros(timesteps)\n",
    "    predictions = np.zeros(timesteps)\n",
    "    errors = np.zeros(timesteps)\n",
    "    representation = 0.0\n",
    "    \n",
    "    # Simulate a simple oscillating input with noise\n",
    "    for t in range(timesteps):\n",
    "        # Input signal (sine wave + noise)\n",
    "        inputs[t] = 0.5 * np.sin(0.1 * t) + np.random.normal(0, input_noise)\n",
    "        \n",
    "        # Generate prediction from representation\n",
    "        predictions[t] = prediction_strength * representation\n",
    "        \n",
    "        # Compute prediction error\n",
    "        errors[t] = inputs[t] - predictions[t]\n",
    "        \n",
    "        # Update representation based on error\n",
    "        representation += learning_rate * errors[t]\n",
    "    \n",
    "    return inputs, predictions, errors\n",
    "\n",
    "# Plotting function\n",
    "def plot_prednet_simulation(inputs, predictions, errors):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(inputs, label='Input')\n",
    "    plt.title('Input Signal')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(predictions, label='Prediction', color='orange')\n",
    "    plt.title('Predicted Signal')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(errors, label='Error', color='red')\n",
    "    plt.title('Prediction Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget\n",
    "@widgets.interact(\n",
    "    input_noise=widgets.FloatSlider(min=0.0, max=0.5, step=0.05, value=0.1, description='Input Noise:'),\n",
    "    prediction_strength=widgets.FloatSlider(min=0.0, max=1.0, step=0.1, value=0.7, description='Pred. Strength:'),\n",
    "    learning_rate=widgets.FloatSlider(min=0.01, max=0.5, step=0.01, value=0.1, description='Learning Rate:')\n",
    ")\n",
    "def interactive_prednet(input_noise, prediction_strength, learning_rate):\n",
    "    inputs, predictions, errors = simulate_prednet(input_noise, prediction_strength, learning_rate)\n",
    "    plot_prednet_simulation(inputs, predictions, errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Example 2: Prioritized Experience Replay\n",
    "\n",
    "This interactive example demonstrates how prioritized experience replay can improve learning efficiency in reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import clear_output\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import random\n",
    "\n",
    "# Simple environment\n",
    "class SimpleGridworld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.agent_pos = (0, 0)\n",
    "        self.goal_pos = (size-1, size-1)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.agent_pos[0] * self.size + self.agent_pos[1]\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 0: up, 1: right, 2: down, 3: left\n",
    "        moves = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        x, y = self.agent_pos\n",
    "        dx, dy = moves[action]\n",
    "        \n",
    "        # Move agent\n",
    "        new_x = max(0, min(self.size-1, x + dx))\n",
    "        new_y = max(0, min(self.size-1, y + dy))\n",
    "        self.agent_pos = (new_x, new_y)\n",
    "        \n",
    "        # Calculate reward\n",
    "        done = self.agent_pos == self.goal_pos\n",
    "        reward = 1.0 if done else -0.01\n",
    "        \n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "# Simple replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=1000):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "\n",
    "# Prioritized replay buffer\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity=1000, alpha=0.6):\n",
    "        self.buffer = []\n",
    "        self.priorities = []\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # Priority exponent\n",
    "    \n",
    "    def add(self, experience, priority=None):\n",
    "        if priority is None:\n",
    "            priority = 1.0  # Default priority\n",
    "            \n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "            self.priorities.pop(0)\n",
    "            \n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(priority)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if not self.buffer:\n",
    "            return []\n",
    "            \n",
    "        # Calculate sampling probabilities\n",
    "        priorities = np.array(self.priorities) ** self.alpha\n",
    "        probs = priorities / np.sum(priorities)\n",
    "        \n",
    "        # Sample based on priorities\n",
    "        indices = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            if idx < len(self.priorities):\n",
    "                self.priorities[idx] = priority\n",
    "\n",
    "# Simple Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_dim, action_dim, prioritized=False, alpha=0.6):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.q_table = np.zeros((state_dim, action_dim))\n",
    "        self.prioritized = prioritized\n",
    "        \n",
    "        if prioritized:\n",
    "            self.replay_buffer = PrioritizedReplayBuffer(alpha=alpha)\n",
    "        else:\n",
    "            self.replay_buffer = ReplayBuffer()\n",
    "    \n",
    "    def select_action(self, state, epsilon=0.1):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done, learning_rate=0.1, gamma=0.99):\n",
    "        # Calculate TD error\n",
    "        q_value = self.q_table[state, action]\n",
    "        next_q_value = np.max(self.q_table[next_state]) if not done else 0\n",
    "        td_target = reward + gamma * next_q_value\n",
    "        td_error = td_target - q_value\n",
    "        \n",
    "        # Store experience with priority\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        if self.prioritized:\n",
    "            priority = abs(td_error) + 0.01  # Small constant for stability\n",
    "            self.replay_buffer.add(experience, priority)\n",
    "        else:\n",
    "            self.replay_buffer.add(experience)\n",
    "        \n",
    "        # Update Q-value directly\n",
    "        self.q_table[state, action] += learning_rate * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def replay(self, batch_size=32, learning_rate=0.1, gamma=0.99):\n",
    "        # Learn from experiences in replay buffer\n",
    "        experiences = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        td_errors = []\n",
    "        for state, action, reward, next_state, done in experiences:\n",
    "            q_value = self.q_table[state, action]\n",
    "            next_q_value = np.max(self.q_table[next_state]) if not done else 0\n",
    "            td_target = reward + gamma * next_q_value\n",
    "            td_error = td_target - q_value\n",
    "            \n",
    "            # Update Q-value\n",
    "            self.q_table[state, action] += learning_rate * td_error\n",
    "            td_errors.append(abs(td_error))\n",
    "        \n",
    "        # If using prioritized replay, update priorities\n",
    "        if self.prioritized and experiences:\n",
    "            indices = list(range(len(self.replay_buffer.buffer) - len(experiences), len(self.replay_buffer.buffer)))\n",
    "            self.replay_buffer.update_priorities(indices, td_errors)\n",
    "\n",
    "# Function to run experiment\n",
    "def run_experiment(prioritized=False, alpha=0.6, episodes=100, epsilon=0.1, learning_rate=0.1, gamma=0.99):\n",
    "    env = SimpleGridworld(size=5)\n",
    "    state_dim = env.size * env.size\n",
    "    action_dim = 4\n",
    "    \n",
    "    agent = QLearningAgent(state_dim, action_dim, prioritized, alpha)\n",
    "    \n",
    "    rewards_history = []\n",
    "    steps_history = []\n",
    "    td_errors = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        episode_td_errors = []\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Learn from this experience\n",
    "            td_error = agent.learn(state, action, reward, next_state, done, learning_rate, gamma)\n",
    "            episode_td_errors.append(abs(td_error))\n",
    "            \n",
    "            # Learn from replay buffer\n",
    "            if steps % 5 == 0:  # Replay every 5 steps\n",
    "                agent.replay(batch_size=16, learning_rate=learning_rate, gamma=gamma)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done or steps >= 100:  # Max 100 steps per episode\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        steps_history.append(steps)\n",
    "        td_errors.append(np.mean(episode_td_errors) if episode_td_errors else 0)\n",
    "    \n",
    "    return rewards_history, steps_history, td_errors\n",
    "\n",
    "# Plotting function\n",
    "def plot_experiment_results(standard_results, prioritized_results):\n",
    "    std_rewards, std_steps, std_errors = standard_results\n",
    "    pri_rewards, pri_steps, pri_errors = prioritized_results\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=3, cols=1, \n",
    "                        subplot_titles=('Total Reward per Episode', \n",
    "                                        'Steps per Episode',\n",
    "                                        'Mean TD Error per Episode'))\n",
    "    \n",
    "    # Add traces\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=std_rewards, mode='lines', name='Standard Replay'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=pri_rewards, mode='lines', name='Prioritized Replay'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=std_steps, mode='lines', name='Standard Replay'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=pri_steps, mode='lines', name='Prioritized Replay'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=std_errors, mode='lines', name='Standard Replay'),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=pri_errors, mode='lines', name='Prioritized Replay'),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(height=800, width=800, title_text=\"Standard vs Prioritized Experience Replay\")\n",
    "    \n",
    "    # Update y-axis titles\n",
    "    fig.update_yaxes(title_text=\"Reward\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Steps\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"TD Error\", row=3, col=1)\n",
    "    \n",
    "    # Update x-axis titles\n",
    "    fig.update_xaxes(title_text=\"Episode\", row=3, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Interactive experiment runner\n",
    "@widgets.interact(\n",
    "    alpha=widgets.FloatSlider(min=0.1, max=1.0, step=0.1, value=0.6, description='Alpha:'),\n",
    "    episodes=widgets.IntSlider(min=10, max=200, step=10, value=50, description='Episodes:'),\n",
    "    learning_rate=widgets.FloatSlider(min=0.01, max=0.5, step=0.01, value=0.1, description='Learning Rate:')\n",
    ")\n",
    "def interactive_replay_experiment(alpha, episodes, learning_rate):\n",
    "    print(\"Running experiment with Standard Replay...\")\n",
    "    standard_results = run_experiment(prioritized=False, episodes=episodes, learning_rate=learning_rate)\n",
    "    \n",
    "    print(\"Running experiment with Prioritized Replay...\")\n",
    "    prioritized_results = run_experiment(prioritized=True, alpha=alpha, episodes=episodes, learning_rate=learning_rate)\n",
    "    \n",
    "    print(\"Plotting results...\")\n",
    "    plot_experiment_results(standard_results, prioritized_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Example 3: Vision Transformer Attention Visualization\n",
    "\n",
    "This interactive example shows how attention works in Vision Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "\n",
    "# Simulate attention patterns for Vision Transformer\n",
    "def create_sample_image(size=16, seed=None):\n",
    "    \"\"\"Create a sample binary image with a pattern\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    # Create a blank image\n",
    "    img = np.zeros((size, size))\n",
    "    \n",
    "    # Add a random shape (square, circle, or line)\n",
    "    shape_type = np.random.choice(['square', 'circle', 'line'])\n",
    "    \n",
    "    if shape_type == 'square':\n",
    "        # Add a square\n",
    "        x, y = np.random.randint(0, size//2, 2)\n",
    "        w, h = np.random.randint(3, size//2, 2)\n",
    "        img[x:x+w, y:y+h] = 1\n",
    "    \n",
    "    elif shape_type == 'circle':\n",
    "        # Add a circle\n",
    "        center_x, center_y = np.random.randint(size//4, 3*size//4, 2)\n",
    "        radius = np.random.randint(2, size//4)\n",
    "        \n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                if (i - center_x)**2 + (j - center_y)**2 < radius**2:\n",
    "                    img[i, j] = 1\n",
    "    \n",
    "    else:  # line\n",
    "        # Add a line\n",
    "        start_x, start_y = np.random.randint(0, size, 2)\n",
    "        end_x, end_y = np.random.randint(0, size, 2)\n",
    "        \n",
    "        # Simple line drawing algorithm\n",
    "        length = max(abs(end_x - start_x), abs(end_y - start_y))\n",
    "        for i in range(length):\n",
    "            x = int(start_x + (end_x - start_x) * i / length)\n",
    "            y = int(start_y + (end_y - start_y) * i / length)\n",
    "            if 0 <= x < size and 0 <= y < size:\n",
    "                img[x, y] = 1\n",
    "    \n",
    "    return img\n",
    "\n",
    "def split_image_into_patches(image, patch_size=4):\n",
    "    \"\"\"Split an image into patches\"\"\"\n",
    "    h, w = image.shape\n",
    "    patches = []\n",
    "    patch_positions = []\n",
    "    \n",
    "    for i in range(0, h, patch_size):\n",
    "        for j in range(0, w, patch_size):\n",
    "            if i + patch_size <= h and j + patch_size <= w:\n",
    "                patch = image[i:i+patch_size, j:j+patch_size]\n",
    "                patches.append(patch)\n",
    "                patch_positions.append((i, j))\n",
    "    \n",
    "    return patches, patch_positions\n",
    "\n",
    "def generate_attention_map(patches, focus_patch_idx, attention_strength=0.8, noise=0.2):\n",
    "    \"\"\"Generate a simulated attention map for a specific patch\"\"\"\n",
    "    num_patches = len(patches)\n",
    "    attention_scores = np.random.rand(num_patches) * noise\n",
    "    \n",
    "    # Increase attention for patches that have similar content\n",
    "    focus_patch = patches[focus_patch_idx]\n",
    "    for i, patch in enumerate(patches):\n",
    "        similarity = 1 - np.mean(np.abs(patch - focus_patch))\n",
    "        attention_scores[i] += similarity * attention_strength\n",
    "    \n",
    "    # Normalize\n",
    "    attention_scores = attention_scores / np.sum(attention_scores)\n",
    "    \n",
    "    return attention_scores\n",
    "\n",
    "def visualize_attention(image, patches, patch_positions, patch_size, focus_patch_idx, attention_scores):\n",
    "    \"\"\"Visualize the image and attention map\"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    # Plot the original image with patch grid\n",
    "    axs[0].imshow(image, cmap='gray')\n",
    "    h, w = image.shape\n",
    "    \n",
    "    # Draw patch grid\n",
    "    for i in range(0, h, patch_size):\n",
    "        axs[0].axhline(i, color='blue', alpha=0.3)\n",
    "    for j in range(0, w, patch_size):\n",
    "        axs[0].axvline(j, color='blue', alpha=0.3)\n",
    "    \n",
    "    # Highlight the focus patch\n",
    "    focus_i, focus_j = patch_positions[focus_patch_idx]\n",
    "    rect = plt.Rectangle((focus_j, focus_i), patch_size, patch_size, \n",
    "                         edgecolor='red', facecolor='none', linewidth=2)\n",
    "    axs[0].add_patch(rect)\n",
    "    axs[0].set_title(f\"Original Image (Focus on Patch {focus_patch_idx+1})\")\n",
    "    \n",
    "    # Plot the attention map\n",
    "    attention_map = np.zeros_like(image)\n",
    "    for idx, (i, j) in enumerate(patch_positions):\n",
    "        attention_map[i:i+patch_size, j:j+patch_size] = attention_scores[idx]\n",
    "    \n",
    "    im = axs[1].imshow(attention_map, cmap='hot')\n",
    "    axs[1].set_title(\"Attention Map\")\n",
    "    fig.colorbar(im, ax=axs[1], shrink=0.8, label='Attention Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive visualization\n",
    "@widgets.interact(\n",
    "    seed=widgets.IntSlider(min=1, max=10, step=1, value=1, description='Image Seed:'),\n",
    "    focus_patch=widgets.IntSlider(min=0, max=15, step=1, value=0, description='Focus Patch:'),\n",
    "    attention_strength=widgets.FloatSlider(min=0.1, max=1.0, step=0.1, value=0.8, description='Attention Strength:'),\n",
    "    noise=widgets.FloatSlider(min=0.0, max=0.5, step=0.05, value=0.2, description='Noise Level:')\n",
    ")\n",
    "def interactive_attention_visualization(seed, focus_patch, attention_strength, noise):\n",
    "    # Create a sample image\n",
    "    image_size = 16\n",
    "    patch_size = 4\n",
    "    image = create_sample_image(size=image_size, seed=seed)\n",
    "    \n",
    "    # Split into patches\n",
    "    patches, patch_positions = split_image_into_patches(image, patch_size=patch_size)\n",
    "    \n",
    "    # Ensure focus_patch is valid\n",
    "    num_patches = len(patches)\n",
    "    focus_patch_idx = min(focus_patch, num_patches-1)\n",
    "    \n",
    "    # Generate attention map\n",
    "    attention_scores = generate_attention_map(patches, focus_patch_idx, attention_strength, noise)\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_attention(image, patches, patch_positions, patch_size, focus_patch_idx, attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Example 4: Glossary - Interactive Neural Mechanisms\n",
    "\n",
    "Below is an interactive glossary of key terms from Chapter 20, with popups explaining neural mechanisms and their AI implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "# Create interactive glossary with popups\n",
    "glossary_html = \"\"\"\n",
    "<style>\n",
    ".glossary-term {\n",
    "    color: #0366d6;\n",
    "    cursor: pointer;\n",
    "    font-weight: bold;\n",
    "    text-decoration: underline;\n",
    "    position: relative;\n",
    "    display: inline-block;\n",
    "}\n",
    "\n",
    ".glossary-term .term-definition {\n",
    "    visibility: hidden;\n",
    "    width: 350px;\n",
    "    background-color: #f8f9fa;\n",
    "    color: #333;\n",
    "    text-align: left;\n",
    "    border-radius: 6px;\n",
    "    padding: 10px;\n",
    "    position: absolute;\n",
    "    z-index: 1;\n",
    "    bottom: 125%;\n",
    "    left: 50%;\n",
    "    margin-left: -175px;\n",
    "    box-shadow: 0px 0px 15px rgba(0,0,0,0.2);\n",
    "    transition: opacity 0.3s;\n",
    "    opacity: 0;\n",
    "    font-weight: normal;\n",
    "    text-decoration: none;\n",
    "    font-size: 0.9em;\n",
    "    border: 1px solid #ddd;\n",
    "}\n",
    "\n",
    ".glossary-term:hover .term-definition {\n",
    "    visibility: visible;\n",
    "    opacity: 1;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h3>Interactive Glossary for Case Studies in NeuroAI</h3>\n",
    "\n",
    "<div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-top: 20px;\">\n",
    "    <p>Hover over each term to see its definition and neural-AI connections.</p>\n",
    "    \n",
    "    <div class=\"glossary-term\">Predictive Coding\n",
    "        <span class=\"term-definition\">\n",
    "            <strong>Predictive Coding</strong><br>\n",
    "            <em>Neural Mechanism:</em> The brain continually generates predictions about incoming sensory information and learns from prediction errors.<br>\n",
    "            <em>AI Implementation:</em> PredNet architecture implements hierarchical predictive processing with explicit representation of prediction errors between layers.<br>\n",
    "            <em>Chapter Reference:</em> Chapter 20.2\n",
    "        </span>\n",
    "    </div>\n",
    "    <br><br>\n",
    "    \n",
    "    <div class=\"glossary-term\">PredNet\n",
    "        <span class=\"term-definition\">\n",
    "            <strong>PredNet</strong><br>\n",
    "            <em>Description:</em> A deep learning architecture implementing hierarchical predictive coding, with explicit computation of prediction errors at each layer.<br>\n",
    "            <em>Neural Parallel:</em> Mirrors the predictive processing in visual cortex, with both bottom-up and top-down information flow.<br>\n",
    "            <em>Advantages:</em> Superior performance in video prediction and sample-efficient learning.<br>\n",
    "            <em>Chapter Reference:</em> Chapter 20.2.2\n",
    "        </span>\n",
    "    </div>\n",
    "    <br><br>\n",
    "    \n",
    "    <div class=\"glossary-term\">Prioritized Experience Replay (PER)\n",
    "        <span class=\"term-definition\">\n",
    "            <strong>Prioritized Experience Replay (PER)</strong><br>\n",
    "            <em>Neural Mechanism:</em> The hippocampus selectively consolidates behaviorally relevant experiences during sleep and rest.<br>\n",
    "            <em>AI Implementation:</em> In reinforcement learning, experiences with high TD error (surprising outcomes) are sampled more frequently during training.<br>\n",
    "            <em>Benefits:</em> 50% faster convergence and 20% better final performance in many RL tasks.<br>\n",
    "            <em>Chapter Reference:</em> Chapter 20.3\n",
    "        </span>\n",
    "    </div>\n",
    "    <br><br>\n",
    "    \n",
    "    <div class=\"glossary-term\">Vision Transformer (ViT)\n",
    "        <span class=\"term-definition\">\n",
    "            <strong>Vision Transformer (ViT)</strong><br>\n",
    "            <em>Neural Mechanism:</em> Visual attention in humans allows selective processing of relevant information while filtering distractions.<br>\n",
    "            <em>AI Implementation:</em> Divides images into patches, processes them with self-attention mechanisms to capture relationships between distant image regions.<br>\n",
    "            <em>Key Innovation:</em> Demonstrates that attention-based models can outperform CNNs on image classification tasks when pre-trained on sufficient data.<br>\n",
    "            <em>Chapter Reference:</em> Chapter 20.4\n",
    "        </span>\n",
    "    </div>\n",
    "    <br><br>\n",
    "    \n",
    "    <div class=\"glossary-term\">Latent Factor Analysis via Dynamical Systems (LFADS)\n",
    "        <span class=\"term-definition\">\n",
    "            <strong>Latent Factor Analysis via Dynamical Systems (LFADS)</strong><br>\n",
    "            <em>Neural Principle:</em> High-dimensional neural activity often reflects low-dimensional latent dynamics.<br>\n",
    "            <em>AI Technique:</em> Uses variational auto-encoders with recurrent neural networks to model underlying dynamics in neural population recordings.<br>\n",
    "            <em>Applications:</em> Single-trial neural decoding, extracting dynamics from noisy spike recordings, improved BMI control.<br>\n",
    "            <em>Chapter Reference:</em> Chapter 20.5\n",
    "        </span>\n",
    "    </div>\n",
    "    <br><br>\n",
    "    \n",
    "    <div class=\"glossary-term\">Attention Mechanism\n",
    "        <span class=\"term-definition\">\n",
    "            <strong>Attention Mechanism</strong><br>\n",
    "            <em>Neural Basis:</em> The brain's ability to selectively focus on important stimuli while ignoring distractions.<br>\n",
    "            <em>AI Implementation:</em> Computational technique that allows models to weight the importance of different input elements.<br>\n",
    "            <em>Example:</em> Self-attention in Vision Transformers allows each image patch to attend to all other patches.<br>\n",
    "            <em>Chapter Reference:</em> Chapters 11 and 20.4\n",
    "        </span>\n",
    "    </div>\n",
    "    <br><br>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(glossary_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Binder\n",
    "\n",
    "You can run this notebook interactively on Binder without any local installation.\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/yourusername/NeuroAI-Handbook/main?filepath=book/part6/ch20_interactive.ipynb)\n",
    "\n",
    "## Summary\n",
    "\n",
    "These interactive examples demonstrate key concepts from Chapter 20: Case Studies in NeuroAI:\n",
    "\n",
    "1. **PredNet Visualization**: Explore how predictive coding works by adjusting parameters and seeing how prediction errors change.\n",
    "2. **Prioritized Experience Replay**: Compare standard and prioritized replay methods in reinforcement learning.\n",
    "3. **Vision Transformer Attention**: Visualize how attention mechanisms in ViT focus on different parts of an image.\n",
    "4. **Interactive Glossary**: Hover over key terms to see detailed explanations and neural-AI connections.\n",
    "\n",
    "These examples help bridge theoretical concepts with practical implementations, making the case studies more concrete and accessible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}